* 
* ==> Audit <==
* |--------------|--------------------------------|----------|---------------|---------|-------------------------------|-------------------------------|
|   Command    |              Args              | Profile  |     User      | Version |          Start Time           |           End Time            |
|--------------|--------------------------------|----------|---------------|---------|-------------------------------|-------------------------------|
| start        |                                | minikube | raymondtorres | v1.24.0 | Thu, 09 Dec 2021 23:01:24 EST | Thu, 09 Dec 2021 23:02:56 EST |
| service      | list                           | minikube | raymondtorres | v1.24.0 | Fri, 10 Dec 2021 02:10:36 EST | Fri, 10 Dec 2021 02:10:38 EST |
| service      | list                           | minikube | raymondtorres | v1.24.0 | Fri, 10 Dec 2021 02:11:35 EST | Fri, 10 Dec 2021 02:11:37 EST |
| delete       |                                | minikube | raymondtorres | v1.24.0 | Fri, 10 Dec 2021 09:02:29 EST | Fri, 10 Dec 2021 09:02:34 EST |
| start        |                                | minikube | raymondtorres | v1.24.0 | Fri, 10 Dec 2021 09:02:53 EST | Fri, 10 Dec 2021 09:03:30 EST |
| docker-env   |                                | minikube | raymondtorres | v1.24.0 | Fri, 10 Dec 2021 09:25:39 EST | Fri, 10 Dec 2021 09:25:40 EST |
| service      | ordering                       | minikube | raymondtorres | v1.24.0 | Fri, 10 Dec 2021 16:24:44 EST | Fri, 10 Dec 2021 16:26:37 EST |
| update-check |                                | minikube | raymondtorres | v1.24.0 | Fri, 10 Dec 2021 17:54:41 EST | Fri, 10 Dec 2021 17:54:42 EST |
| ip           |                                | minikube | raymondtorres | v1.24.0 | Fri, 10 Dec 2021 18:19:25 EST | Fri, 10 Dec 2021 18:19:25 EST |
| ip           |                                | minikube | raymondtorres | v1.24.0 | Fri, 10 Dec 2021 18:47:09 EST | Fri, 10 Dec 2021 18:47:09 EST |
| update-check |                                | minikube | raymondtorres | v1.24.0 | Fri, 10 Dec 2021 22:31:33 EST | Fri, 10 Dec 2021 22:31:34 EST |
| start        |                                | minikube | raymondtorres | v1.24.0 | Fri, 10 Dec 2021 23:48:14 EST | Fri, 10 Dec 2021 23:48:44 EST |
| start        |                                | minikube | raymondtorres | v1.24.0 | Sun, 12 Dec 2021 23:17:28 EST | Sun, 12 Dec 2021 23:17:55 EST |
| update-check |                                | minikube | raymondtorres | v1.24.0 | Mon, 13 Dec 2021 10:36:50 EST | Mon, 13 Dec 2021 10:36:50 EST |
| docker-env   | --shell none -p minikube       | minikube | skaffold      | v1.24.0 | Mon, 13 Dec 2021 14:39:02 EST | Mon, 13 Dec 2021 14:39:03 EST |
|              | --user=skaffold                |          |               |         |                               |                               |
| docker-env   | --shell none -p minikube       | minikube | skaffold      | v1.24.0 | Mon, 13 Dec 2021 14:40:55 EST | Mon, 13 Dec 2021 14:40:56 EST |
|              | --user=skaffold                |          |               |         |                               |                               |
| docker-env   | --shell none -p minikube       | minikube | skaffold      | v1.24.0 | Mon, 13 Dec 2021 14:45:18 EST | Mon, 13 Dec 2021 14:45:19 EST |
|              | --user=skaffold                |          |               |         |                               |                               |
| docker-env   | --shell none -p minikube       | minikube | skaffold      | v1.24.0 | Mon, 13 Dec 2021 14:46:44 EST | Mon, 13 Dec 2021 14:46:45 EST |
|              | --user=skaffold                |          |               |         |                               |                               |
| docker-env   | --shell none -p minikube       | minikube | skaffold      | v1.24.0 | Mon, 13 Dec 2021 14:49:12 EST | Mon, 13 Dec 2021 14:49:14 EST |
|              | --user=skaffold                |          |               |         |                               |                               |
| docker-env   | --shell none -p minikube       | minikube | skaffold      | v1.24.0 | Mon, 13 Dec 2021 14:50:59 EST | Mon, 13 Dec 2021 14:51:01 EST |
|              | --user=skaffold                |          |               |         |                               |                               |
| docker-env   | --shell none -p minikube       | minikube | skaffold      | v1.24.0 | Mon, 13 Dec 2021 14:51:41 EST | Mon, 13 Dec 2021 14:51:42 EST |
|              | --user=skaffold                |          |               |         |                               |                               |
| docker-env   | --shell none -p minikube       | minikube | skaffold      | v1.24.0 | Mon, 13 Dec 2021 14:52:02 EST | Mon, 13 Dec 2021 14:52:03 EST |
|              | --user=skaffold                |          |               |         |                               |                               |
| --help       |                                | minikube | raymondtorres | v1.24.0 | Mon, 13 Dec 2021 15:03:39 EST | Mon, 13 Dec 2021 15:03:39 EST |
| docker-env   | --shell none -p minikube       | minikube | skaffold      | v1.24.0 | Mon, 13 Dec 2021 15:37:51 EST | Mon, 13 Dec 2021 15:37:52 EST |
|              | --user=skaffold                |          |               |         |                               |                               |
| docker-env   | --shell none -p minikube       | minikube | skaffold      | v1.24.0 | Mon, 13 Dec 2021 15:38:27 EST | Mon, 13 Dec 2021 15:38:28 EST |
|              | --user=skaffold                |          |               |         |                               |                               |
| service      | list                           | minikube | raymondtorres | v1.24.0 | Mon, 13 Dec 2021 15:39:40 EST | Mon, 13 Dec 2021 15:39:42 EST |
| service      | ordering-app-service           | minikube | raymondtorres | v1.24.0 | Mon, 13 Dec 2021 15:47:06 EST | Mon, 13 Dec 2021 15:47:11 EST |
| docker-env   | --shell none -p minikube       | minikube | skaffold      | v1.24.0 | Mon, 13 Dec 2021 15:47:42 EST | Mon, 13 Dec 2021 15:47:43 EST |
|              | --user=skaffold                |          |               |         |                               |                               |
|--------------|--------------------------------|----------|---------------|---------|-------------------------------|-------------------------------|

* 
* ==> Last Start <==
* Log file created at: 2021/12/12 23:17:28
Running on machine: raymondtorres
Binary: Built with gc go1.17.2 for darwin/amd64
Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg
I1212 23:17:28.724222    3392 out.go:297] Setting OutFile to fd 1 ...
I1212 23:17:28.724366    3392 out.go:349] isatty.IsTerminal(1) = true
I1212 23:17:28.724368    3392 out.go:310] Setting ErrFile to fd 2...
I1212 23:17:28.724372    3392 out.go:349] isatty.IsTerminal(2) = true
I1212 23:17:28.724453    3392 root.go:313] Updating PATH: /Users/raymondtorres/.minikube/bin
I1212 23:17:28.724734    3392 out.go:304] Setting JSON to false
I1212 23:17:28.766918    3392 start.go:112] hostinfo: {"hostname":"raymondtorres.verizon.net","uptime":116381,"bootTime":1639252667,"procs":512,"os":"darwin","platform":"darwin","platformFamily":"Standalone Workstation","platformVersion":"11.2.3","kernelVersion":"20.3.0","kernelArch":"x86_64","virtualizationSystem":"","virtualizationRole":"","hostId":"bdfb2cd3-d436-51e4-8ce2-4706d6ae17ef"}
W1212 23:17:28.767034    3392 start.go:120] gopshost.Virtualization returned error: not implemented yet
I1212 23:17:28.792578    3392 out.go:176] 😄  minikube v1.24.0 on Darwin 11.2.3
I1212 23:17:28.792736    3392 notify.go:174] Checking for updates...
I1212 23:17:28.793167    3392 config.go:176] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.22.3
I1212 23:17:28.793207    3392 driver.go:343] Setting default libvirt URI to qemu:///system
I1212 23:17:28.926505    3392 docker.go:132] docker version: linux-20.10.10
I1212 23:17:28.926638    3392 cli_runner.go:115] Run: docker system info --format "{{json .}}"
I1212 23:17:29.530188    3392 info.go:263] docker info: {ID:XIO7:A32H:7OYL:GZGF:KJW3:K7GA:J6QA:FJO7:FMFM:E6L2:SAXL:26HW Containers:7 ContainersRunning:1 ContainersPaused:0 ContainersStopped:6 Images:14 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:true KernelMemoryTCP:true CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:43 OomKillDisable:true NGoroutines:46 SystemTime:2021-12-13 04:17:29.076439748 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:4 KernelVersion:5.10.47-linuxkit OperatingSystem:Docker Desktop OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:6 MemTotal:2081755136 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy: Name:docker-desktop Labels:[] ExperimentalBuild:false ServerVersion:20.10.10 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:5b46e404f6b9f661a205e28d59c982d3634148f8 Expected:5b46e404f6b9f661a205e28d59c982d3634148f8} RuncCommit:{ID:v1.0.2-0-g52b36a2 Expected:v1.0.2-0-g52b36a2} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=default] ProductLicense: Warnings:<nil> ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:/usr/local/lib/docker/cli-plugins/docker-buildx SchemaVersion:0.1.0 ShortDescription:Build with BuildKit Vendor:Docker Inc. Version:v0.6.3] map[Name:compose Path:/usr/local/lib/docker/cli-plugins/docker-compose SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.1.1] map[Name:scan Path:/usr/local/lib/docker/cli-plugins/docker-scan SchemaVersion:0.1.0 ShortDescription:Docker Scan Vendor:Docker Inc. Version:0.9.0]] Warnings:<nil>}}
I1212 23:17:29.574823    3392 out.go:176] ✨  Using the docker driver based on existing profile
I1212 23:17:29.574882    3392 start.go:280] selected driver: docker
I1212 23:17:29.574887    3392 start.go:762] validating driver "docker" against &{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.28@sha256:4780f1897569d2bf77aafb3d133a08d42b4fe61127f06fcfc90c2c5d902d893c Memory:1985 CPUs:2 DiskSize:20000 VMDriver: Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.22.3 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin: FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.22.3 ControlPlane:true Worker:true}] Addons:map[ambassador:false auto-pause:false csi-hostpath-driver:false dashboard:true default-storageclass:true efk:false freshpod:false gcp-auth:false gvisor:false helm-tiller:false ingress:false ingress-dns:false istio:false istio-provisioner:false kubevirt:false logviewer:false metallb:false metrics-server:false nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:true storage-provisioner-gluster:false volumesnapshots:false] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/Users:/minikube-host}
I1212 23:17:29.574954    3392 start.go:773] status for docker: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc:}
I1212 23:17:29.575205    3392 cli_runner.go:115] Run: docker system info --format "{{json .}}"
I1212 23:17:29.802511    3392 info.go:263] docker info: {ID:XIO7:A32H:7OYL:GZGF:KJW3:K7GA:J6QA:FJO7:FMFM:E6L2:SAXL:26HW Containers:7 ContainersRunning:1 ContainersPaused:0 ContainersStopped:6 Images:14 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:true KernelMemoryTCP:true CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:43 OomKillDisable:true NGoroutines:46 SystemTime:2021-12-13 04:17:29.732122309 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:4 KernelVersion:5.10.47-linuxkit OperatingSystem:Docker Desktop OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:6 MemTotal:2081755136 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy: Name:docker-desktop Labels:[] ExperimentalBuild:false ServerVersion:20.10.10 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:5b46e404f6b9f661a205e28d59c982d3634148f8 Expected:5b46e404f6b9f661a205e28d59c982d3634148f8} RuncCommit:{ID:v1.0.2-0-g52b36a2 Expected:v1.0.2-0-g52b36a2} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=default] ProductLicense: Warnings:<nil> ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:/usr/local/lib/docker/cli-plugins/docker-buildx SchemaVersion:0.1.0 ShortDescription:Build with BuildKit Vendor:Docker Inc. Version:v0.6.3] map[Name:compose Path:/usr/local/lib/docker/cli-plugins/docker-compose SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.1.1] map[Name:scan Path:/usr/local/lib/docker/cli-plugins/docker-scan SchemaVersion:0.1.0 ShortDescription:Docker Scan Vendor:Docker Inc. Version:0.9.0]] Warnings:<nil>}}
I1212 23:17:29.802791    3392 cni.go:93] Creating CNI manager for ""
I1212 23:17:29.802800    3392 cni.go:167] CNI unnecessary in this configuration, recommending no CNI
I1212 23:17:29.802811    3392 start_flags.go:282] config:
{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.28@sha256:4780f1897569d2bf77aafb3d133a08d42b4fe61127f06fcfc90c2c5d902d893c Memory:1985 CPUs:2 DiskSize:20000 VMDriver: Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.22.3 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin: FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.22.3 ControlPlane:true Worker:true}] Addons:map[ambassador:false auto-pause:false csi-hostpath-driver:false dashboard:true default-storageclass:true efk:false freshpod:false gcp-auth:false gvisor:false helm-tiller:false ingress:false ingress-dns:false istio:false istio-provisioner:false kubevirt:false logviewer:false metallb:false metrics-server:false nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:true storage-provisioner-gluster:false volumesnapshots:false] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/Users:/minikube-host}
I1212 23:17:29.826824    3392 out.go:176] 👍  Starting control plane node minikube in cluster minikube
I1212 23:17:29.827228    3392 cache.go:118] Beginning downloading kic base image for docker with docker
I1212 23:17:29.873748    3392 out.go:176] 🚜  Pulling base image ...
I1212 23:17:29.873797    3392 preload.go:132] Checking if preload exists for k8s version v1.22.3 and runtime docker
I1212 23:17:29.873847    3392 image.go:75] Checking for gcr.io/k8s-minikube/kicbase:v0.0.28@sha256:4780f1897569d2bf77aafb3d133a08d42b4fe61127f06fcfc90c2c5d902d893c in local docker daemon
I1212 23:17:29.873910    3392 preload.go:148] Found local preload: /Users/raymondtorres/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v13-v1.22.3-docker-overlay2-amd64.tar.lz4
I1212 23:17:29.873922    3392 cache.go:57] Caching tarball of preloaded images
I1212 23:17:29.874977    3392 preload.go:174] Found /Users/raymondtorres/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v13-v1.22.3-docker-overlay2-amd64.tar.lz4 in cache, skipping download
I1212 23:17:29.875025    3392 cache.go:60] Finished verifying existence of preloaded tar for  v1.22.3 on docker
I1212 23:17:29.875857    3392 profile.go:147] Saving config to /Users/raymondtorres/.minikube/profiles/minikube/config.json ...
I1212 23:17:30.022607    3392 image.go:79] Found gcr.io/k8s-minikube/kicbase:v0.0.28@sha256:4780f1897569d2bf77aafb3d133a08d42b4fe61127f06fcfc90c2c5d902d893c in local docker daemon, skipping pull
I1212 23:17:30.022615    3392 cache.go:140] gcr.io/k8s-minikube/kicbase:v0.0.28@sha256:4780f1897569d2bf77aafb3d133a08d42b4fe61127f06fcfc90c2c5d902d893c exists in daemon, skipping load
I1212 23:17:30.022628    3392 cache.go:206] Successfully downloaded all kic artifacts
I1212 23:17:30.022657    3392 start.go:313] acquiring machines lock for minikube: {Name:mk9add393920ba5992e557aa122a39fe4845fcde Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I1212 23:17:30.022803    3392 start.go:317] acquired machines lock for "minikube" in 128.906µs
I1212 23:17:30.022823    3392 start.go:93] Skipping create...Using existing machine configuration
I1212 23:17:30.022829    3392 fix.go:55] fixHost starting: 
I1212 23:17:30.023477    3392 cli_runner.go:115] Run: docker container inspect minikube --format={{.State.Status}}
I1212 23:17:30.177901    3392 fix.go:108] recreateIfNeeded on minikube: state=Stopped err=<nil>
W1212 23:17:30.177938    3392 fix.go:134] unexpected machine state, will restart: <nil>
I1212 23:17:30.222400    3392 out.go:176] 🔄  Restarting existing docker container for "minikube" ...
I1212 23:17:30.222520    3392 cli_runner.go:115] Run: docker start minikube
I1212 23:17:30.785717    3392 cli_runner.go:115] Run: docker container inspect minikube --format={{.State.Status}}
I1212 23:17:30.941651    3392 kic.go:420] container "minikube" state is running.
I1212 23:17:30.942846    3392 cli_runner.go:115] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I1212 23:17:31.096239    3392 profile.go:147] Saving config to /Users/raymondtorres/.minikube/profiles/minikube/config.json ...
I1212 23:17:31.096629    3392 machine.go:88] provisioning docker machine ...
I1212 23:17:31.096652    3392 ubuntu.go:169] provisioning hostname "minikube"
I1212 23:17:31.096729    3392 cli_runner.go:115] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1212 23:17:31.258668    3392 main.go:130] libmachine: Using SSH client type: native
I1212 23:17:31.259448    3392 main.go:130] libmachine: &{{{<nil> 0 [] [] []} docker [0x43a0ca0] 0x43a3d80 <nil>  [] 0s} 127.0.0.1 49885 <nil> <nil>}
I1212 23:17:31.259458    3392 main.go:130] libmachine: About to run SSH command:
sudo hostname minikube && echo "minikube" | sudo tee /etc/hostname
I1212 23:17:31.260749    3392 main.go:130] libmachine: Error dialing TCP: ssh: handshake failed: EOF
I1212 23:17:34.414291    3392 main.go:130] libmachine: SSH cmd err, output: <nil>: minikube

I1212 23:17:34.414427    3392 cli_runner.go:115] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1212 23:17:34.564590    3392 main.go:130] libmachine: Using SSH client type: native
I1212 23:17:34.564757    3392 main.go:130] libmachine: &{{{<nil> 0 [] [] []} docker [0x43a0ca0] 0x43a3d80 <nil>  [] 0s} 127.0.0.1 49885 <nil> <nil>}
I1212 23:17:34.564766    3392 main.go:130] libmachine: About to run SSH command:

		if ! grep -xq '.*\sminikube' /etc/hosts; then
			if grep -xq '127.0.1.1\s.*' /etc/hosts; then
				sudo sed -i 's/^127.0.1.1\s.*/127.0.1.1 minikube/g' /etc/hosts;
			else 
				echo '127.0.1.1 minikube' | sudo tee -a /etc/hosts; 
			fi
		fi
I1212 23:17:34.679924    3392 main.go:130] libmachine: SSH cmd err, output: <nil>: 
I1212 23:17:34.679946    3392 ubuntu.go:175] set auth options {CertDir:/Users/raymondtorres/.minikube CaCertPath:/Users/raymondtorres/.minikube/certs/ca.pem CaPrivateKeyPath:/Users/raymondtorres/.minikube/certs/ca-key.pem CaCertRemotePath:/etc/docker/ca.pem ServerCertPath:/Users/raymondtorres/.minikube/machines/server.pem ServerKeyPath:/Users/raymondtorres/.minikube/machines/server-key.pem ClientKeyPath:/Users/raymondtorres/.minikube/certs/key.pem ServerCertRemotePath:/etc/docker/server.pem ServerKeyRemotePath:/etc/docker/server-key.pem ClientCertPath:/Users/raymondtorres/.minikube/certs/cert.pem ServerCertSANs:[] StorePath:/Users/raymondtorres/.minikube}
I1212 23:17:34.679966    3392 ubuntu.go:177] setting up certificates
I1212 23:17:34.679976    3392 provision.go:83] configureAuth start
I1212 23:17:34.680081    3392 cli_runner.go:115] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I1212 23:17:34.833423    3392 provision.go:138] copyHostCerts
I1212 23:17:34.834551    3392 exec_runner.go:144] found /Users/raymondtorres/.minikube/cert.pem, removing ...
I1212 23:17:34.834559    3392 exec_runner.go:207] rm: /Users/raymondtorres/.minikube/cert.pem
I1212 23:17:34.834718    3392 exec_runner.go:151] cp: /Users/raymondtorres/.minikube/certs/cert.pem --> /Users/raymondtorres/.minikube/cert.pem (1139 bytes)
I1212 23:17:34.835320    3392 exec_runner.go:144] found /Users/raymondtorres/.minikube/key.pem, removing ...
I1212 23:17:34.835323    3392 exec_runner.go:207] rm: /Users/raymondtorres/.minikube/key.pem
I1212 23:17:34.835393    3392 exec_runner.go:151] cp: /Users/raymondtorres/.minikube/certs/key.pem --> /Users/raymondtorres/.minikube/key.pem (1679 bytes)
I1212 23:17:34.835991    3392 exec_runner.go:144] found /Users/raymondtorres/.minikube/ca.pem, removing ...
I1212 23:17:34.835995    3392 exec_runner.go:207] rm: /Users/raymondtorres/.minikube/ca.pem
I1212 23:17:34.836067    3392 exec_runner.go:151] cp: /Users/raymondtorres/.minikube/certs/ca.pem --> /Users/raymondtorres/.minikube/ca.pem (1099 bytes)
I1212 23:17:34.836397    3392 provision.go:112] generating server cert: /Users/raymondtorres/.minikube/machines/server.pem ca-key=/Users/raymondtorres/.minikube/certs/ca.pem private-key=/Users/raymondtorres/.minikube/certs/ca-key.pem org=raymondtorres.minikube san=[192.168.49.2 127.0.0.1 localhost 127.0.0.1 minikube minikube]
I1212 23:17:35.134665    3392 provision.go:172] copyRemoteCerts
I1212 23:17:35.136563    3392 ssh_runner.go:152] Run: sudo mkdir -p /etc/docker /etc/docker /etc/docker
I1212 23:17:35.136624    3392 cli_runner.go:115] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1212 23:17:35.296446    3392 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:49885 SSHKeyPath:/Users/raymondtorres/.minikube/machines/minikube/id_rsa Username:docker}
I1212 23:17:35.381985    3392 ssh_runner.go:319] scp /Users/raymondtorres/.minikube/certs/ca.pem --> /etc/docker/ca.pem (1099 bytes)
I1212 23:17:35.406219    3392 ssh_runner.go:319] scp /Users/raymondtorres/.minikube/machines/server.pem --> /etc/docker/server.pem (1220 bytes)
I1212 23:17:35.423949    3392 ssh_runner.go:319] scp /Users/raymondtorres/.minikube/machines/server-key.pem --> /etc/docker/server-key.pem (1679 bytes)
I1212 23:17:35.443180    3392 provision.go:86] duration metric: configureAuth took 763.187289ms
I1212 23:17:35.443197    3392 ubuntu.go:193] setting minikube options for container-runtime
I1212 23:17:35.443360    3392 config.go:176] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.22.3
I1212 23:17:35.443427    3392 cli_runner.go:115] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1212 23:17:35.591812    3392 main.go:130] libmachine: Using SSH client type: native
I1212 23:17:35.591993    3392 main.go:130] libmachine: &{{{<nil> 0 [] [] []} docker [0x43a0ca0] 0x43a3d80 <nil>  [] 0s} 127.0.0.1 49885 <nil> <nil>}
I1212 23:17:35.592001    3392 main.go:130] libmachine: About to run SSH command:
df --output=fstype / | tail -n 1
I1212 23:17:35.706734    3392 main.go:130] libmachine: SSH cmd err, output: <nil>: overlay

I1212 23:17:35.706742    3392 ubuntu.go:71] root file system type: overlay
I1212 23:17:35.707310    3392 provision.go:309] Updating docker unit: /lib/systemd/system/docker.service ...
I1212 23:17:35.707419    3392 cli_runner.go:115] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1212 23:17:35.862789    3392 main.go:130] libmachine: Using SSH client type: native
I1212 23:17:35.862955    3392 main.go:130] libmachine: &{{{<nil> 0 [] [] []} docker [0x43a0ca0] 0x43a3d80 <nil>  [] 0s} 127.0.0.1 49885 <nil> <nil>}
I1212 23:17:35.863009    3392 main.go:130] libmachine: About to run SSH command:
sudo mkdir -p /lib/systemd/system && printf %!s(MISSING) "[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP \$MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target
" | sudo tee /lib/systemd/system/docker.service.new
I1212 23:17:35.989098    3392 main.go:130] libmachine: SSH cmd err, output: <nil>: [Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP $MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target

I1212 23:17:35.989187    3392 cli_runner.go:115] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1212 23:17:36.139562    3392 main.go:130] libmachine: Using SSH client type: native
I1212 23:17:36.139701    3392 main.go:130] libmachine: &{{{<nil> 0 [] [] []} docker [0x43a0ca0] 0x43a3d80 <nil>  [] 0s} 127.0.0.1 49885 <nil> <nil>}
I1212 23:17:36.139715    3392 main.go:130] libmachine: About to run SSH command:
sudo diff -u /lib/systemd/system/docker.service /lib/systemd/system/docker.service.new || { sudo mv /lib/systemd/system/docker.service.new /lib/systemd/system/docker.service; sudo systemctl -f daemon-reload && sudo systemctl -f enable docker && sudo systemctl -f restart docker; }
I1212 23:17:36.262217    3392 main.go:130] libmachine: SSH cmd err, output: <nil>: 
I1212 23:17:36.262236    3392 machine.go:91] provisioned docker machine in 5.16556899s
I1212 23:17:36.262254    3392 start.go:267] post-start starting for "minikube" (driver="docker")
I1212 23:17:36.262259    3392 start.go:277] creating required directories: [/etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs]
I1212 23:17:36.262438    3392 ssh_runner.go:152] Run: sudo mkdir -p /etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs
I1212 23:17:36.262493    3392 cli_runner.go:115] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1212 23:17:36.414674    3392 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:49885 SSHKeyPath:/Users/raymondtorres/.minikube/machines/minikube/id_rsa Username:docker}
I1212 23:17:36.499738    3392 ssh_runner.go:152] Run: cat /etc/os-release
I1212 23:17:36.504018    3392 main.go:130] libmachine: Couldn't set key PRIVACY_POLICY_URL, no corresponding struct field found
I1212 23:17:36.504032    3392 main.go:130] libmachine: Couldn't set key VERSION_CODENAME, no corresponding struct field found
I1212 23:17:36.504042    3392 main.go:130] libmachine: Couldn't set key UBUNTU_CODENAME, no corresponding struct field found
I1212 23:17:36.504046    3392 info.go:137] Remote host: Ubuntu 20.04.2 LTS
I1212 23:17:36.504053    3392 filesync.go:126] Scanning /Users/raymondtorres/.minikube/addons for local assets ...
I1212 23:17:36.504159    3392 filesync.go:126] Scanning /Users/raymondtorres/.minikube/files for local assets ...
I1212 23:17:36.504201    3392 start.go:270] post-start completed in 241.9417ms
I1212 23:17:36.504414    3392 ssh_runner.go:152] Run: sh -c "df -h /var | awk 'NR==2{print $5}'"
I1212 23:17:36.504460    3392 cli_runner.go:115] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1212 23:17:36.661160    3392 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:49885 SSHKeyPath:/Users/raymondtorres/.minikube/machines/minikube/id_rsa Username:docker}
I1212 23:17:36.744356    3392 fix.go:57] fixHost completed within 6.721480841s
I1212 23:17:36.744377    3392 start.go:80] releasing machines lock for "minikube", held for 6.721522046s
I1212 23:17:36.744523    3392 cli_runner.go:115] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I1212 23:17:36.907983    3392 ssh_runner.go:152] Run: curl -sS -m 2 https://k8s.gcr.io/
I1212 23:17:36.908009    3392 ssh_runner.go:152] Run: systemctl --version
I1212 23:17:36.908073    3392 cli_runner.go:115] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1212 23:17:36.908078    3392 cli_runner.go:115] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1212 23:17:37.078002    3392 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:49885 SSHKeyPath:/Users/raymondtorres/.minikube/machines/minikube/id_rsa Username:docker}
I1212 23:17:37.097263    3392 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:49885 SSHKeyPath:/Users/raymondtorres/.minikube/machines/minikube/id_rsa Username:docker}
I1212 23:17:37.335774    3392 ssh_runner.go:152] Run: sudo systemctl is-active --quiet service containerd
I1212 23:17:37.347843    3392 ssh_runner.go:152] Run: sudo systemctl cat docker.service
I1212 23:17:37.358945    3392 cruntime.go:255] skipping containerd shutdown because we are bound to it
I1212 23:17:37.359535    3392 ssh_runner.go:152] Run: sudo systemctl is-active --quiet service crio
I1212 23:17:37.372476    3392 ssh_runner.go:152] Run: /bin/bash -c "sudo mkdir -p /etc && printf %!s(MISSING) "runtime-endpoint: unix:///var/run/dockershim.sock
image-endpoint: unix:///var/run/dockershim.sock
" | sudo tee /etc/crictl.yaml"
I1212 23:17:37.390155    3392 ssh_runner.go:152] Run: sudo systemctl unmask docker.service
I1212 23:17:37.464262    3392 ssh_runner.go:152] Run: sudo systemctl enable docker.socket
I1212 23:17:37.534076    3392 ssh_runner.go:152] Run: sudo systemctl cat docker.service
I1212 23:17:37.545073    3392 ssh_runner.go:152] Run: sudo systemctl daemon-reload
I1212 23:17:37.611406    3392 ssh_runner.go:152] Run: sudo systemctl start docker
I1212 23:17:37.623076    3392 ssh_runner.go:152] Run: docker version --format {{.Server.Version}}
I1212 23:17:37.811182    3392 ssh_runner.go:152] Run: docker version --format {{.Server.Version}}
I1212 23:17:37.896473    3392 out.go:203] 🐳  Preparing Kubernetes v1.22.3 on Docker 20.10.8 ...
I1212 23:17:37.896622    3392 cli_runner.go:115] Run: docker exec -t minikube dig +short host.docker.internal
I1212 23:17:38.203526    3392 network.go:96] got host ip for mount in container by digging dns: 192.168.65.2
I1212 23:17:38.203809    3392 ssh_runner.go:152] Run: grep 192.168.65.2	host.minikube.internal$ /etc/hosts
I1212 23:17:38.208523    3392 ssh_runner.go:152] Run: /bin/bash -c "{ grep -v $'\thost.minikube.internal$' "/etc/hosts"; echo "192.168.65.2	host.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I1212 23:17:38.219880    3392 cli_runner.go:115] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I1212 23:17:38.383406    3392 preload.go:132] Checking if preload exists for k8s version v1.22.3 and runtime docker
I1212 23:17:38.383495    3392 ssh_runner.go:152] Run: docker images --format {{.Repository}}:{{.Tag}}
I1212 23:17:38.419403    3392 docker.go:558] Got preloaded images: -- stdout --
ordering:latest
<none>:<none>
mcr.microsoft.com/dotnet/sdk:5.0
mcr.microsoft.com/dotnet/aspnet:5.0
k8s.gcr.io/kube-apiserver:v1.22.3
k8s.gcr.io/kube-scheduler:v1.22.3
k8s.gcr.io/kube-controller-manager:v1.22.3
k8s.gcr.io/kube-proxy:v1.22.3
kubernetesui/dashboard:v2.3.1
k8s.gcr.io/etcd:3.5.0-0
kubernetesui/metrics-scraper:v1.0.7
k8s.gcr.io/coredns/coredns:v1.8.4
gcr.io/k8s-minikube/storage-provisioner:v5
k8s.gcr.io/pause:3.5

-- /stdout --
I1212 23:17:38.419417    3392 docker.go:489] Images already preloaded, skipping extraction
I1212 23:17:38.419514    3392 ssh_runner.go:152] Run: docker images --format {{.Repository}}:{{.Tag}}
I1212 23:17:38.458515    3392 docker.go:558] Got preloaded images: -- stdout --
ordering:latest
<none>:<none>
mcr.microsoft.com/dotnet/sdk:5.0
mcr.microsoft.com/dotnet/aspnet:5.0
k8s.gcr.io/kube-apiserver:v1.22.3
k8s.gcr.io/kube-scheduler:v1.22.3
k8s.gcr.io/kube-controller-manager:v1.22.3
k8s.gcr.io/kube-proxy:v1.22.3
kubernetesui/dashboard:v2.3.1
k8s.gcr.io/etcd:3.5.0-0
kubernetesui/metrics-scraper:v1.0.7
k8s.gcr.io/coredns/coredns:v1.8.4
gcr.io/k8s-minikube/storage-provisioner:v5
k8s.gcr.io/pause:3.5

-- /stdout --
I1212 23:17:38.458541    3392 cache_images.go:79] Images are preloaded, skipping loading
I1212 23:17:38.458666    3392 ssh_runner.go:152] Run: docker info --format {{.CgroupDriver}}
I1212 23:17:38.790549    3392 cni.go:93] Creating CNI manager for ""
I1212 23:17:38.790557    3392 cni.go:167] CNI unnecessary in this configuration, recommending no CNI
I1212 23:17:38.790571    3392 kubeadm.go:87] Using pod CIDR: 10.244.0.0/16
I1212 23:17:38.790586    3392 kubeadm.go:153] kubeadm options: {CertDir:/var/lib/minikube/certs ServiceCIDR:10.96.0.0/12 PodSubnet:10.244.0.0/16 AdvertiseAddress:192.168.49.2 APIServerPort:8443 KubernetesVersion:v1.22.3 EtcdDataDir:/var/lib/minikube/etcd EtcdExtraArgs:map[] ClusterName:minikube NodeName:minikube DNSDomain:cluster.local CRISocket:/var/run/dockershim.sock ImageRepository: ComponentOptions:[{Component:apiServer ExtraArgs:map[enable-admission-plugins:NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota] Pairs:map[certSANs:["127.0.0.1", "localhost", "192.168.49.2"]]} {Component:controllerManager ExtraArgs:map[allocate-node-cidrs:true leader-elect:false] Pairs:map[]} {Component:scheduler ExtraArgs:map[leader-elect:false] Pairs:map[]}] FeatureArgs:map[] NoTaintMaster:true NodeIP:192.168.49.2 CgroupDriver:cgroupfs ClientCAFile:/var/lib/minikube/certs/ca.crt StaticPodPath:/etc/kubernetes/manifests ControlPlaneAddress:control-plane.minikube.internal KubeProxyOptions:map[]}
I1212 23:17:38.790698    3392 kubeadm.go:157] kubeadm config:
apiVersion: kubeadm.k8s.io/v1beta2
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 192.168.49.2
  bindPort: 8443
bootstrapTokens:
  - groups:
      - system:bootstrappers:kubeadm:default-node-token
    ttl: 24h0m0s
    usages:
      - signing
      - authentication
nodeRegistration:
  criSocket: /var/run/dockershim.sock
  name: "minikube"
  kubeletExtraArgs:
    node-ip: 192.168.49.2
  taints: []
---
apiVersion: kubeadm.k8s.io/v1beta2
kind: ClusterConfiguration
apiServer:
  certSANs: ["127.0.0.1", "localhost", "192.168.49.2"]
  extraArgs:
    enable-admission-plugins: "NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota"
controllerManager:
  extraArgs:
    allocate-node-cidrs: "true"
    leader-elect: "false"
scheduler:
  extraArgs:
    leader-elect: "false"
certificatesDir: /var/lib/minikube/certs
clusterName: mk
controlPlaneEndpoint: control-plane.minikube.internal:8443
dns:
  type: CoreDNS
etcd:
  local:
    dataDir: /var/lib/minikube/etcd
    extraArgs:
      proxy-refresh-interval: "70000"
kubernetesVersion: v1.22.3
networking:
  dnsDomain: cluster.local
  podSubnet: "10.244.0.0/16"
  serviceSubnet: 10.96.0.0/12
---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
authentication:
  x509:
    clientCAFile: /var/lib/minikube/certs/ca.crt
cgroupDriver: cgroupfs
clusterDomain: "cluster.local"
# disable disk resource management by default
imageGCHighThresholdPercent: 100
evictionHard:
  nodefs.available: "0%!"(MISSING)
  nodefs.inodesFree: "0%!"(MISSING)
  imagefs.available: "0%!"(MISSING)
failSwapOn: false
staticPodPath: /etc/kubernetes/manifests
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
clusterCIDR: "10.244.0.0/16"
metricsBindAddress: 0.0.0.0:10249
conntrack:
  maxPerCore: 0
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_established"
  tcpEstablishedTimeout: 0s
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_close"
  tcpCloseWaitTimeout: 0s

I1212 23:17:38.790778    3392 kubeadm.go:909] kubelet [Unit]
Wants=docker.socket

[Service]
ExecStart=
ExecStart=/var/lib/minikube/binaries/v1.22.3/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --config=/var/lib/kubelet/config.yaml --container-runtime=docker --hostname-override=minikube --kubeconfig=/etc/kubernetes/kubelet.conf --node-ip=192.168.49.2

[Install]
 config:
{KubernetesVersion:v1.22.3 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin: FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:}
I1212 23:17:38.790961    3392 ssh_runner.go:152] Run: sudo ls /var/lib/minikube/binaries/v1.22.3
I1212 23:17:38.800166    3392 binaries.go:44] Found k8s binaries, skipping transfer
I1212 23:17:38.800333    3392 ssh_runner.go:152] Run: sudo mkdir -p /etc/systemd/system/kubelet.service.d /lib/systemd/system /var/tmp/minikube
I1212 23:17:38.807718    3392 ssh_runner.go:319] scp memory --> /etc/systemd/system/kubelet.service.d/10-kubeadm.conf (334 bytes)
I1212 23:17:38.821298    3392 ssh_runner.go:319] scp memory --> /lib/systemd/system/kubelet.service (352 bytes)
I1212 23:17:38.836983    3392 ssh_runner.go:319] scp memory --> /var/tmp/minikube/kubeadm.yaml.new (2051 bytes)
I1212 23:17:38.853638    3392 ssh_runner.go:152] Run: grep 192.168.49.2	control-plane.minikube.internal$ /etc/hosts
I1212 23:17:38.858291    3392 ssh_runner.go:152] Run: /bin/bash -c "{ grep -v $'\tcontrol-plane.minikube.internal$' "/etc/hosts"; echo "192.168.49.2	control-plane.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I1212 23:17:38.869493    3392 certs.go:54] Setting up /Users/raymondtorres/.minikube/profiles/minikube for IP: 192.168.49.2
I1212 23:17:38.869686    3392 certs.go:182] skipping minikubeCA CA generation: /Users/raymondtorres/.minikube/ca.key
I1212 23:17:38.870258    3392 certs.go:182] skipping proxyClientCA CA generation: /Users/raymondtorres/.minikube/proxy-client-ca.key
I1212 23:17:38.870479    3392 certs.go:298] skipping minikube-user signed cert generation: /Users/raymondtorres/.minikube/profiles/minikube/client.key
I1212 23:17:38.870958    3392 certs.go:298] skipping minikube signed cert generation: /Users/raymondtorres/.minikube/profiles/minikube/apiserver.key.dd3b5fb2
I1212 23:17:38.871396    3392 certs.go:298] skipping aggregator signed cert generation: /Users/raymondtorres/.minikube/profiles/minikube/proxy-client.key
I1212 23:17:38.871749    3392 certs.go:388] found cert: /Users/raymondtorres/.minikube/certs/Users/raymondtorres/.minikube/certs/ca-key.pem (1675 bytes)
I1212 23:17:38.871796    3392 certs.go:388] found cert: /Users/raymondtorres/.minikube/certs/Users/raymondtorres/.minikube/certs/ca.pem (1099 bytes)
I1212 23:17:38.871830    3392 certs.go:388] found cert: /Users/raymondtorres/.minikube/certs/Users/raymondtorres/.minikube/certs/cert.pem (1139 bytes)
I1212 23:17:38.871886    3392 certs.go:388] found cert: /Users/raymondtorres/.minikube/certs/Users/raymondtorres/.minikube/certs/key.pem (1679 bytes)
I1212 23:17:38.875942    3392 ssh_runner.go:319] scp /Users/raymondtorres/.minikube/profiles/minikube/apiserver.crt --> /var/lib/minikube/certs/apiserver.crt (1399 bytes)
I1212 23:17:38.899675    3392 ssh_runner.go:319] scp /Users/raymondtorres/.minikube/profiles/minikube/apiserver.key --> /var/lib/minikube/certs/apiserver.key (1679 bytes)
I1212 23:17:38.919208    3392 ssh_runner.go:319] scp /Users/raymondtorres/.minikube/profiles/minikube/proxy-client.crt --> /var/lib/minikube/certs/proxy-client.crt (1147 bytes)
I1212 23:17:38.939223    3392 ssh_runner.go:319] scp /Users/raymondtorres/.minikube/profiles/minikube/proxy-client.key --> /var/lib/minikube/certs/proxy-client.key (1679 bytes)
I1212 23:17:38.958796    3392 ssh_runner.go:319] scp /Users/raymondtorres/.minikube/ca.crt --> /var/lib/minikube/certs/ca.crt (1111 bytes)
I1212 23:17:38.978696    3392 ssh_runner.go:319] scp /Users/raymondtorres/.minikube/ca.key --> /var/lib/minikube/certs/ca.key (1675 bytes)
I1212 23:17:38.999212    3392 ssh_runner.go:319] scp /Users/raymondtorres/.minikube/proxy-client-ca.crt --> /var/lib/minikube/certs/proxy-client-ca.crt (1119 bytes)
I1212 23:17:39.018728    3392 ssh_runner.go:319] scp /Users/raymondtorres/.minikube/proxy-client-ca.key --> /var/lib/minikube/certs/proxy-client-ca.key (1679 bytes)
I1212 23:17:39.041708    3392 ssh_runner.go:319] scp /Users/raymondtorres/.minikube/ca.crt --> /usr/share/ca-certificates/minikubeCA.pem (1111 bytes)
I1212 23:17:39.061729    3392 ssh_runner.go:319] scp memory --> /var/lib/minikube/kubeconfig (738 bytes)
I1212 23:17:39.076554    3392 ssh_runner.go:152] Run: openssl version
I1212 23:17:39.085076    3392 ssh_runner.go:152] Run: sudo /bin/bash -c "test -s /usr/share/ca-certificates/minikubeCA.pem && ln -fs /usr/share/ca-certificates/minikubeCA.pem /etc/ssl/certs/minikubeCA.pem"
I1212 23:17:39.094571    3392 ssh_runner.go:152] Run: ls -la /usr/share/ca-certificates/minikubeCA.pem
I1212 23:17:39.098944    3392 certs.go:431] hashing: -rw-r--r-- 1 root root 1111 Dec 10 04:02 /usr/share/ca-certificates/minikubeCA.pem
I1212 23:17:39.099134    3392 ssh_runner.go:152] Run: openssl x509 -hash -noout -in /usr/share/ca-certificates/minikubeCA.pem
I1212 23:17:39.105209    3392 ssh_runner.go:152] Run: sudo /bin/bash -c "test -L /etc/ssl/certs/b5213941.0 || ln -fs /etc/ssl/certs/minikubeCA.pem /etc/ssl/certs/b5213941.0"
I1212 23:17:39.113118    3392 kubeadm.go:390] StartCluster: {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.28@sha256:4780f1897569d2bf77aafb3d133a08d42b4fe61127f06fcfc90c2c5d902d893c Memory:1985 CPUs:2 DiskSize:20000 VMDriver: Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.22.3 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin: FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.22.3 ControlPlane:true Worker:true}] Addons:map[ambassador:false auto-pause:false csi-hostpath-driver:false dashboard:true default-storageclass:true efk:false freshpod:false gcp-auth:false gvisor:false helm-tiller:false ingress:false ingress-dns:false istio:false istio-provisioner:false kubevirt:false logviewer:false metallb:false metrics-server:false nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:true storage-provisioner-gluster:false volumesnapshots:false] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/Users:/minikube-host}
I1212 23:17:39.113275    3392 ssh_runner.go:152] Run: docker ps --filter status=paused --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I1212 23:17:39.145916    3392 ssh_runner.go:152] Run: sudo ls /var/lib/kubelet/kubeadm-flags.env /var/lib/kubelet/config.yaml /var/lib/minikube/etcd
I1212 23:17:39.153950    3392 kubeadm.go:401] found existing configuration files, will attempt cluster restart
I1212 23:17:39.153967    3392 kubeadm.go:600] restartCluster start
I1212 23:17:39.154130    3392 ssh_runner.go:152] Run: sudo test -d /data/minikube
I1212 23:17:39.161188    3392 kubeadm.go:126] /data/minikube skipping compat symlinks: sudo test -d /data/minikube: Process exited with status 1
stdout:

stderr:
I1212 23:17:39.161269    3392 cli_runner.go:115] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I1212 23:17:39.323765    3392 kubeconfig.go:92] found "minikube" server: "https://127.0.0.1:50228"
I1212 23:17:39.323782    3392 kubeconfig.go:116] verify returned: got: 127.0.0.1:50228, want: 127.0.0.1:49884
I1212 23:17:39.324266    3392 lock.go:35] WriteFile acquiring /Users/raymondtorres/.kube/config: {Name:mkcd624ad95df23bda1e691390a5cc3fc83a72d0 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1212 23:17:39.338901    3392 ssh_runner.go:152] Run: sudo diff -u /var/tmp/minikube/kubeadm.yaml /var/tmp/minikube/kubeadm.yaml.new
I1212 23:17:39.349963    3392 api_server.go:165] Checking apiserver status ...
I1212 23:17:39.350267    3392 ssh_runner.go:152] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1212 23:17:39.367873    3392 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1212 23:17:39.567942    3392 api_server.go:165] Checking apiserver status ...
I1212 23:17:39.568119    3392 ssh_runner.go:152] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1212 23:17:39.585386    3392 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1212 23:17:39.768689    3392 api_server.go:165] Checking apiserver status ...
I1212 23:17:39.768940    3392 ssh_runner.go:152] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1212 23:17:39.784618    3392 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1212 23:17:39.971756    3392 api_server.go:165] Checking apiserver status ...
I1212 23:17:39.972007    3392 ssh_runner.go:152] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1212 23:17:39.986335    3392 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1212 23:17:40.171718    3392 api_server.go:165] Checking apiserver status ...
I1212 23:17:40.171903    3392 ssh_runner.go:152] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1212 23:17:40.185366    3392 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1212 23:17:40.368037    3392 api_server.go:165] Checking apiserver status ...
I1212 23:17:40.368232    3392 ssh_runner.go:152] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1212 23:17:40.382503    3392 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1212 23:17:40.568695    3392 api_server.go:165] Checking apiserver status ...
I1212 23:17:40.568882    3392 ssh_runner.go:152] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1212 23:17:40.584082    3392 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1212 23:17:40.769830    3392 api_server.go:165] Checking apiserver status ...
I1212 23:17:40.770018    3392 ssh_runner.go:152] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1212 23:17:40.786123    3392 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1212 23:17:40.970109    3392 api_server.go:165] Checking apiserver status ...
I1212 23:17:40.970302    3392 ssh_runner.go:152] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1212 23:17:40.985771    3392 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1212 23:17:41.168015    3392 api_server.go:165] Checking apiserver status ...
I1212 23:17:41.168259    3392 ssh_runner.go:152] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1212 23:17:41.184470    3392 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1212 23:17:41.372123    3392 api_server.go:165] Checking apiserver status ...
I1212 23:17:41.372483    3392 ssh_runner.go:152] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1212 23:17:41.393470    3392 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1212 23:17:41.570405    3392 api_server.go:165] Checking apiserver status ...
I1212 23:17:41.570582    3392 ssh_runner.go:152] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1212 23:17:41.585142    3392 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1212 23:17:41.768228    3392 api_server.go:165] Checking apiserver status ...
I1212 23:17:41.768415    3392 ssh_runner.go:152] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1212 23:17:41.781494    3392 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1212 23:17:41.971725    3392 api_server.go:165] Checking apiserver status ...
I1212 23:17:41.971887    3392 ssh_runner.go:152] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1212 23:17:41.986792    3392 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1212 23:17:42.171657    3392 api_server.go:165] Checking apiserver status ...
I1212 23:17:42.171821    3392 ssh_runner.go:152] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1212 23:17:42.185961    3392 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1212 23:17:42.369552    3392 api_server.go:165] Checking apiserver status ...
I1212 23:17:42.369704    3392 ssh_runner.go:152] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1212 23:17:42.383839    3392 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1212 23:17:42.383847    3392 api_server.go:165] Checking apiserver status ...
I1212 23:17:42.384006    3392 ssh_runner.go:152] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1212 23:17:42.397015    3392 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1212 23:17:42.397022    3392 kubeadm.go:575] needs reconfigure: apiserver error: timed out waiting for the condition
I1212 23:17:42.397031    3392 kubeadm.go:1032] stopping kube-system containers ...
I1212 23:17:42.397120    3392 ssh_runner.go:152] Run: docker ps -a --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I1212 23:17:42.433601    3392 docker.go:390] Stopping containers: [fbd01b507b5b 685c256f4a4c 1176a62c6a59 3eeed8b3e00c 46db5611adc8 eecc58a5c19b 5dea5ce103b9 d3ae499e7f5f 65af892bd55f c3d76ba5da76 1f35b8c9e660 c259e0e0b11e 60164a78875e dd15426cd615 ff1f2d67d21e 99c2fcaf5e13 6c6c36cdd586 c30998839ae4 7f211b93ba90 4430bc57f046 fcc1ce1fc237 eea539281162 3214c391e5ad 42c53967bc04 1d569f527eb5 b0e590928203 e88dae10e13a]
I1212 23:17:42.433697    3392 ssh_runner.go:152] Run: docker stop fbd01b507b5b 685c256f4a4c 1176a62c6a59 3eeed8b3e00c 46db5611adc8 eecc58a5c19b 5dea5ce103b9 d3ae499e7f5f 65af892bd55f c3d76ba5da76 1f35b8c9e660 c259e0e0b11e 60164a78875e dd15426cd615 ff1f2d67d21e 99c2fcaf5e13 6c6c36cdd586 c30998839ae4 7f211b93ba90 4430bc57f046 fcc1ce1fc237 eea539281162 3214c391e5ad 42c53967bc04 1d569f527eb5 b0e590928203 e88dae10e13a
I1212 23:17:42.471895    3392 ssh_runner.go:152] Run: sudo systemctl stop kubelet
I1212 23:17:42.486385    3392 ssh_runner.go:152] Run: sudo ls -la /etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf
I1212 23:17:42.495487    3392 kubeadm.go:154] found existing configuration files:
-rw------- 1 root root 5643 Dec 10 14:03 /etc/kubernetes/admin.conf
-rw------- 1 root root 5656 Dec 11 04:48 /etc/kubernetes/controller-manager.conf
-rw------- 1 root root 1971 Dec 10 14:03 /etc/kubernetes/kubelet.conf
-rw------- 1 root root 5600 Dec 11 04:48 /etc/kubernetes/scheduler.conf

I1212 23:17:42.495656    3392 ssh_runner.go:152] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/admin.conf
I1212 23:17:42.504927    3392 ssh_runner.go:152] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/kubelet.conf
I1212 23:17:42.514258    3392 ssh_runner.go:152] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/controller-manager.conf
I1212 23:17:42.522738    3392 kubeadm.go:165] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/controller-manager.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/controller-manager.conf: Process exited with status 1
stdout:

stderr:
I1212 23:17:42.522895    3392 ssh_runner.go:152] Run: sudo rm -f /etc/kubernetes/controller-manager.conf
I1212 23:17:42.531037    3392 ssh_runner.go:152] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/scheduler.conf
I1212 23:17:42.539712    3392 kubeadm.go:165] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/scheduler.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/scheduler.conf: Process exited with status 1
stdout:

stderr:
I1212 23:17:42.539897    3392 ssh_runner.go:152] Run: sudo rm -f /etc/kubernetes/scheduler.conf
I1212 23:17:42.547828    3392 ssh_runner.go:152] Run: sudo cp /var/tmp/minikube/kubeadm.yaml.new /var/tmp/minikube/kubeadm.yaml
I1212 23:17:42.556270    3392 kubeadm.go:676] reconfiguring cluster from /var/tmp/minikube/kubeadm.yaml
I1212 23:17:42.556277    3392 ssh_runner.go:152] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.22.3:$PATH" kubeadm init phase certs all --config /var/tmp/minikube/kubeadm.yaml"
I1212 23:17:42.737926    3392 ssh_runner.go:152] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.22.3:$PATH" kubeadm init phase kubeconfig all --config /var/tmp/minikube/kubeadm.yaml"
I1212 23:17:43.396458    3392 ssh_runner.go:152] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.22.3:$PATH" kubeadm init phase kubelet-start --config /var/tmp/minikube/kubeadm.yaml"
I1212 23:17:43.534658    3392 ssh_runner.go:152] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.22.3:$PATH" kubeadm init phase control-plane all --config /var/tmp/minikube/kubeadm.yaml"
I1212 23:17:43.598929    3392 ssh_runner.go:152] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.22.3:$PATH" kubeadm init phase etcd local --config /var/tmp/minikube/kubeadm.yaml"
I1212 23:17:43.657162    3392 api_server.go:51] waiting for apiserver process to appear ...
I1212 23:17:43.657338    3392 ssh_runner.go:152] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1212 23:17:44.174184    3392 ssh_runner.go:152] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1212 23:17:44.674538    3392 ssh_runner.go:152] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1212 23:17:45.176349    3392 ssh_runner.go:152] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1212 23:17:45.675637    3392 ssh_runner.go:152] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1212 23:17:45.704128    3392 api_server.go:71] duration metric: took 2.046952038s to wait for apiserver process to appear ...
I1212 23:17:45.704139    3392 api_server.go:87] waiting for apiserver healthz status ...
I1212 23:17:45.704151    3392 api_server.go:240] Checking apiserver healthz at https://127.0.0.1:49884/healthz ...
I1212 23:17:45.705566    3392 api_server.go:256] stopped: https://127.0.0.1:49884/healthz: Get "https://127.0.0.1:49884/healthz": EOF
I1212 23:17:46.209449    3392 api_server.go:240] Checking apiserver healthz at https://127.0.0.1:49884/healthz ...
I1212 23:17:46.210914    3392 api_server.go:256] stopped: https://127.0.0.1:49884/healthz: Get "https://127.0.0.1:49884/healthz": EOF
I1212 23:17:46.709894    3392 api_server.go:240] Checking apiserver healthz at https://127.0.0.1:49884/healthz ...
I1212 23:17:50.745983    3392 api_server.go:266] https://127.0.0.1:49884/healthz returned 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
W1212 23:17:50.745993    3392 api_server.go:102] status: https://127.0.0.1:49884/healthz returned error 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
I1212 23:17:51.207982    3392 api_server.go:240] Checking apiserver healthz at https://127.0.0.1:49884/healthz ...
I1212 23:17:51.217216    3392 api_server.go:266] https://127.0.0.1:49884/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/bootstrap-controller ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
healthz check failed
W1212 23:17:51.217227    3392 api_server.go:102] status: https://127.0.0.1:49884/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/bootstrap-controller ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
healthz check failed
I1212 23:17:51.707873    3392 api_server.go:240] Checking apiserver healthz at https://127.0.0.1:49884/healthz ...
I1212 23:17:51.717778    3392 api_server.go:266] https://127.0.0.1:49884/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/bootstrap-controller ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
healthz check failed
W1212 23:17:51.717791    3392 api_server.go:102] status: https://127.0.0.1:49884/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/bootstrap-controller ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
healthz check failed
I1212 23:17:52.208369    3392 api_server.go:240] Checking apiserver healthz at https://127.0.0.1:49884/healthz ...
I1212 23:17:52.217740    3392 api_server.go:266] https://127.0.0.1:49884/healthz returned 200:
ok
I1212 23:17:52.231608    3392 api_server.go:140] control plane version: v1.22.3
I1212 23:17:52.231621    3392 api_server.go:130] duration metric: took 6.527436538s to wait for apiserver health ...
I1212 23:17:52.231627    3392 cni.go:93] Creating CNI manager for ""
I1212 23:17:52.231634    3392 cni.go:167] CNI unnecessary in this configuration, recommending no CNI
I1212 23:17:52.231944    3392 system_pods.go:43] waiting for kube-system pods to appear ...
I1212 23:17:52.255700    3392 system_pods.go:59] 7 kube-system pods found
I1212 23:17:52.255721    3392 system_pods.go:61] "coredns-78fcd69978-2lsgn" [d5f083d7-6245-4848-a07c-90b8b9379d37] Running
I1212 23:17:52.255730    3392 system_pods.go:61] "etcd-minikube" [7fdc2403-60e9-4958-b4bc-ffd203adeb7f] Running / Ready:ContainersNotReady (containers with unready status: [etcd]) / ContainersReady:ContainersNotReady (containers with unready status: [etcd])
I1212 23:17:52.255735    3392 system_pods.go:61] "kube-apiserver-minikube" [79d55565-e03e-4b95-b491-efbb01d65ac0] Running
I1212 23:17:52.255737    3392 system_pods.go:61] "kube-controller-manager-minikube" [99a54c95-53de-4a05-8592-a23656f9ceb1] Running
I1212 23:17:52.255740    3392 system_pods.go:61] "kube-proxy-n4tdm" [8d8f89ad-c7bb-46f8-aad9-9617f8c47110] Running
I1212 23:17:52.255745    3392 system_pods.go:61] "kube-scheduler-minikube" [d71f3e7c-dfb1-4f12-88d4-de2f3ff6325d] Running
I1212 23:17:52.255748    3392 system_pods.go:61] "storage-provisioner" [f0f4c524-b570-4a94-8a8d-133beec8d090] Running / Ready:ContainersNotReady (containers with unready status: [storage-provisioner]) / ContainersReady:ContainersNotReady (containers with unready status: [storage-provisioner])
I1212 23:17:52.255751    3392 system_pods.go:74] duration metric: took 23.802096ms to wait for pod list to return data ...
I1212 23:17:52.255758    3392 node_conditions.go:102] verifying NodePressure condition ...
I1212 23:17:52.261043    3392 node_conditions.go:122] node storage ephemeral capacity is 61255492Ki
I1212 23:17:52.261061    3392 node_conditions.go:123] node cpu capacity is 6
I1212 23:17:52.261304    3392 node_conditions.go:105] duration metric: took 5.543161ms to run NodePressure ...
I1212 23:17:52.261485    3392 ssh_runner.go:152] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.22.3:$PATH" kubeadm init phase addon all --config /var/tmp/minikube/kubeadm.yaml"
I1212 23:17:52.907850    3392 ssh_runner.go:152] Run: /bin/bash -c "cat /proc/$(pgrep kube-apiserver)/oom_adj"
I1212 23:17:53.013002    3392 ops.go:34] apiserver oom_adj: -16
I1212 23:17:53.013011    3392 kubeadm.go:604] restartCluster took 13.85895073s
I1212 23:17:53.013017    3392 kubeadm.go:392] StartCluster complete in 13.899816811s
I1212 23:17:53.013042    3392 settings.go:142] acquiring lock: {Name:mk344353bd0993d52f1b6f9125b3546ec41b742b Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1212 23:17:53.013367    3392 settings.go:150] Updating kubeconfig:  /Users/raymondtorres/.kube/config
I1212 23:17:53.015838    3392 lock.go:35] WriteFile acquiring /Users/raymondtorres/.kube/config: {Name:mkcd624ad95df23bda1e691390a5cc3fc83a72d0 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1212 23:17:53.087460    3392 kapi.go:244] deployment "coredns" in namespace "kube-system" and context "minikube" rescaled to 1
I1212 23:17:53.088098    3392 start.go:229] Will wait 6m0s for node &{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.22.3 ControlPlane:true Worker:true}
I1212 23:17:53.088142    3392 ssh_runner.go:152] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.22.3/kubectl --kubeconfig=/var/lib/minikube/kubeconfig -n kube-system get configmap coredns -o yaml"
I1212 23:17:53.117021    3392 out.go:176] 🔎  Verifying Kubernetes components...
I1212 23:17:53.088507    3392 config.go:176] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.22.3
I1212 23:17:53.088868    3392 addons.go:415] enableAddons start: toEnable=map[ambassador:false auto-pause:false csi-hostpath-driver:false dashboard:true default-storageclass:true efk:false freshpod:false gcp-auth:false gvisor:false helm-tiller:false ingress:false ingress-dns:false istio:false istio-provisioner:false kubevirt:false logviewer:false metallb:false metrics-server:false nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:true storage-provisioner-gluster:false volumesnapshots:false], additional=[]
I1212 23:17:53.117428    3392 ssh_runner.go:152] Run: sudo systemctl is-active --quiet service kubelet
I1212 23:17:53.117543    3392 addons.go:65] Setting default-storageclass=true in profile "minikube"
I1212 23:17:53.117544    3392 addons.go:65] Setting storage-provisioner=true in profile "minikube"
I1212 23:17:53.117553    3392 addons.go:65] Setting dashboard=true in profile "minikube"
I1212 23:17:53.117574    3392 addons.go:153] Setting addon storage-provisioner=true in "minikube"
I1212 23:17:53.117580    3392 addons.go:153] Setting addon dashboard=true in "minikube"
W1212 23:17:53.117580    3392 addons.go:165] addon storage-provisioner should already be in state true
W1212 23:17:53.117597    3392 addons.go:165] addon dashboard should already be in state true
I1212 23:17:53.117844    3392 host.go:66] Checking if "minikube" exists ...
I1212 23:17:53.117845    3392 host.go:66] Checking if "minikube" exists ...
I1212 23:17:53.118081    3392 addons_storage_classes.go:33] enableOrDisableStorageClasses default-storageclass=true on "minikube"
I1212 23:17:53.119051    3392 cli_runner.go:115] Run: docker container inspect minikube --format={{.State.Status}}
I1212 23:17:53.120087    3392 cli_runner.go:115] Run: docker container inspect minikube --format={{.State.Status}}
I1212 23:17:53.120065    3392 cli_runner.go:115] Run: docker container inspect minikube --format={{.State.Status}}
I1212 23:17:53.202319    3392 cli_runner.go:115] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I1212 23:17:53.512471    3392 api_server.go:51] waiting for apiserver process to appear ...
I1212 23:17:53.568865    3392 out.go:176]     ▪ Using image gcr.io/k8s-minikube/storage-provisioner:v5
I1212 23:17:53.535304    3392 addons.go:153] Setting addon default-storageclass=true in "minikube"
I1212 23:17:53.608974    3392 out.go:176]     ▪ Using image kubernetesui/dashboard:v2.3.1
I1212 23:17:53.569065    3392 addons.go:348] installing /etc/kubernetes/addons/storage-provisioner.yaml
W1212 23:17:53.608986    3392 addons.go:165] addon default-storageclass should already be in state true
I1212 23:17:53.609012    3392 ssh_runner.go:319] scp memory --> /etc/kubernetes/addons/storage-provisioner.yaml (2676 bytes)
I1212 23:17:53.569138    3392 ssh_runner.go:152] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1212 23:17:53.609036    3392 host.go:66] Checking if "minikube" exists ...
I1212 23:17:53.609156    3392 cli_runner.go:115] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1212 23:17:53.609756    3392 cli_runner.go:115] Run: docker container inspect minikube --format={{.State.Status}}
I1212 23:17:53.666398    3392 out.go:176]     ▪ Using image kubernetesui/metrics-scraper:v1.0.7
I1212 23:17:53.666527    3392 addons.go:348] installing /etc/kubernetes/addons/dashboard-ns.yaml
I1212 23:17:53.666534    3392 ssh_runner.go:319] scp memory --> /etc/kubernetes/addons/dashboard-ns.yaml (759 bytes)
I1212 23:17:53.667245    3392 cli_runner.go:115] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1212 23:17:53.938540    3392 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:49885 SSHKeyPath:/Users/raymondtorres/.minikube/machines/minikube/id_rsa Username:docker}
I1212 23:17:53.938621    3392 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:49885 SSHKeyPath:/Users/raymondtorres/.minikube/machines/minikube/id_rsa Username:docker}
I1212 23:17:53.938669    3392 addons.go:348] installing /etc/kubernetes/addons/storageclass.yaml
I1212 23:17:53.938676    3392 ssh_runner.go:319] scp memory --> /etc/kubernetes/addons/storageclass.yaml (271 bytes)
I1212 23:17:53.938785    3392 cli_runner.go:115] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1212 23:17:54.138462    3392 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:49885 SSHKeyPath:/Users/raymondtorres/.minikube/machines/minikube/id_rsa Username:docker}
I1212 23:17:54.291133    3392 ssh_runner.go:152] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.22.3/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml
I1212 23:17:54.297052    3392 addons.go:348] installing /etc/kubernetes/addons/dashboard-clusterrole.yaml
I1212 23:17:54.297061    3392 ssh_runner.go:319] scp memory --> /etc/kubernetes/addons/dashboard-clusterrole.yaml (1001 bytes)
I1212 23:17:54.383110    3392 addons.go:348] installing /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml
I1212 23:17:54.383127    3392 ssh_runner.go:319] scp memory --> /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml (1018 bytes)
I1212 23:17:54.483088    3392 ssh_runner.go:152] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.22.3/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml
I1212 23:17:54.505824    3392 addons.go:348] installing /etc/kubernetes/addons/dashboard-configmap.yaml
I1212 23:17:54.505834    3392 ssh_runner.go:319] scp memory --> /etc/kubernetes/addons/dashboard-configmap.yaml (837 bytes)
I1212 23:17:54.616255    3392 addons.go:348] installing /etc/kubernetes/addons/dashboard-dp.yaml
I1212 23:17:54.616267    3392 ssh_runner.go:319] scp memory --> /etc/kubernetes/addons/dashboard-dp.yaml (4278 bytes)
I1212 23:17:54.701635    3392 ssh_runner.go:192] Completed: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.22.3/kubectl --kubeconfig=/var/lib/minikube/kubeconfig -n kube-system get configmap coredns -o yaml": (1.584606965s)
I1212 23:17:54.701687    3392 ssh_runner.go:192] Completed: sudo pgrep -xnf kube-apiserver.*minikube.*: (1.092635716s)
I1212 23:17:54.701698    3392 api_server.go:71] duration metric: took 1.613562761s to wait for apiserver process to appear ...
I1212 23:17:54.701707    3392 api_server.go:87] waiting for apiserver healthz status ...
I1212 23:17:54.701715    3392 api_server.go:240] Checking apiserver healthz at https://127.0.0.1:49884/healthz ...
I1212 23:17:54.701760    3392 start.go:719] CoreDNS already contains "host.minikube.internal" host record, skipping...
I1212 23:17:54.710802    3392 api_server.go:266] https://127.0.0.1:49884/healthz returned 200:
ok
I1212 23:17:54.713350    3392 api_server.go:140] control plane version: v1.22.3
I1212 23:17:54.713370    3392 api_server.go:130] duration metric: took 11.651342ms to wait for apiserver health ...
I1212 23:17:54.713377    3392 system_pods.go:43] waiting for kube-system pods to appear ...
I1212 23:17:54.730560    3392 addons.go:348] installing /etc/kubernetes/addons/dashboard-role.yaml
I1212 23:17:54.730570    3392 ssh_runner.go:319] scp memory --> /etc/kubernetes/addons/dashboard-role.yaml (1724 bytes)
I1212 23:17:54.781019    3392 system_pods.go:59] 7 kube-system pods found
I1212 23:17:54.781068    3392 system_pods.go:61] "coredns-78fcd69978-2lsgn" [d5f083d7-6245-4848-a07c-90b8b9379d37] Running / Ready:ContainersNotReady (containers with unready status: [coredns]) / ContainersReady:ContainersNotReady (containers with unready status: [coredns])
I1212 23:17:54.781080    3392 system_pods.go:61] "etcd-minikube" [7fdc2403-60e9-4958-b4bc-ffd203adeb7f] Running / Ready:ContainersNotReady (containers with unready status: [etcd]) / ContainersReady:ContainersNotReady (containers with unready status: [etcd])
I1212 23:17:54.781091    3392 system_pods.go:61] "kube-apiserver-minikube" [79d55565-e03e-4b95-b491-efbb01d65ac0] Running
I1212 23:17:54.781098    3392 system_pods.go:61] "kube-controller-manager-minikube" [99a54c95-53de-4a05-8592-a23656f9ceb1] Running
I1212 23:17:54.781104    3392 system_pods.go:61] "kube-proxy-n4tdm" [8d8f89ad-c7bb-46f8-aad9-9617f8c47110] Running / Ready:ContainersNotReady (containers with unready status: [kube-proxy]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-proxy])
I1212 23:17:54.781108    3392 system_pods.go:61] "kube-scheduler-minikube" [d71f3e7c-dfb1-4f12-88d4-de2f3ff6325d] Running
I1212 23:17:54.781120    3392 system_pods.go:61] "storage-provisioner" [f0f4c524-b570-4a94-8a8d-133beec8d090] Running / Ready:ContainersNotReady (containers with unready status: [storage-provisioner]) / ContainersReady:ContainersNotReady (containers with unready status: [storage-provisioner])
I1212 23:17:54.781137    3392 system_pods.go:74] duration metric: took 67.754797ms to wait for pod list to return data ...
I1212 23:17:54.781145    3392 kubeadm.go:547] duration metric: took 1.693009815s to wait for : map[apiserver:true system_pods:true] ...
I1212 23:17:54.781158    3392 node_conditions.go:102] verifying NodePressure condition ...
I1212 23:17:54.785767    3392 node_conditions.go:122] node storage ephemeral capacity is 61255492Ki
I1212 23:17:54.785778    3392 node_conditions.go:123] node cpu capacity is 6
I1212 23:17:54.785789    3392 node_conditions.go:105] duration metric: took 4.624498ms to run NodePressure ...
I1212 23:17:54.785797    3392 start.go:234] waiting for startup goroutines ...
I1212 23:17:54.897036    3392 addons.go:348] installing /etc/kubernetes/addons/dashboard-rolebinding.yaml
I1212 23:17:54.897046    3392 ssh_runner.go:319] scp memory --> /etc/kubernetes/addons/dashboard-rolebinding.yaml (1046 bytes)
I1212 23:17:55.005786    3392 addons.go:348] installing /etc/kubernetes/addons/dashboard-sa.yaml
I1212 23:17:55.005804    3392 ssh_runner.go:319] scp memory --> /etc/kubernetes/addons/dashboard-sa.yaml (837 bytes)
I1212 23:17:55.092899    3392 addons.go:348] installing /etc/kubernetes/addons/dashboard-secret.yaml
I1212 23:17:55.092916    3392 ssh_runner.go:319] scp memory --> /etc/kubernetes/addons/dashboard-secret.yaml (1389 bytes)
I1212 23:17:55.194395    3392 addons.go:348] installing /etc/kubernetes/addons/dashboard-svc.yaml
I1212 23:17:55.194404    3392 ssh_runner.go:319] scp memory --> /etc/kubernetes/addons/dashboard-svc.yaml (1294 bytes)
I1212 23:17:55.279080    3392 ssh_runner.go:152] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.22.3/kubectl apply -f /etc/kubernetes/addons/dashboard-ns.yaml -f /etc/kubernetes/addons/dashboard-clusterrole.yaml -f /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml -f /etc/kubernetes/addons/dashboard-configmap.yaml -f /etc/kubernetes/addons/dashboard-dp.yaml -f /etc/kubernetes/addons/dashboard-role.yaml -f /etc/kubernetes/addons/dashboard-rolebinding.yaml -f /etc/kubernetes/addons/dashboard-sa.yaml -f /etc/kubernetes/addons/dashboard-secret.yaml -f /etc/kubernetes/addons/dashboard-svc.yaml
I1212 23:17:55.633447    3392 out.go:176] 🌟  Enabled addons: storage-provisioner, default-storageclass, dashboard
I1212 23:17:55.633497    3392 addons.go:417] enableAddons completed in 2.545368s
I1212 23:17:55.721615    3392 start.go:473] kubectl: 1.21.5, cluster: 1.22.3 (minor skew: 1)
I1212 23:17:55.777917    3392 out.go:176] 🏄  Done! kubectl is now configured to use "minikube" cluster and "default" namespace by default

* 
* ==> Docker <==
* -- Logs begin at Mon 2021-12-13 04:17:31 UTC, end at Mon 2021-12-13 20:49:31 UTC. --
Dec 13 19:59:50 minikube dockerd[215]: time="2021-12-13T19:59:50.074329400Z" level=info msg="ignoring event" container=43b932abc32241d5f0806e81374d38e8e2cedbe14a8614c8ebff1659a525ea7c module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Dec 13 19:59:50 minikube dockerd[215]: time="2021-12-13T19:59:50.355878200Z" level=info msg="ignoring event" container=396fd433868673a2f0ecbf91b572a0de683cb1fdfc08789e4ff17ac50521e962 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Dec 13 19:59:53 minikube dockerd[215]: time="2021-12-13T19:59:53.090570300Z" level=info msg="ignoring event" container=dc86d3f5f23d4cc3c3bedd6160212696b0362d90c3a37f520b39023b138838e9 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Dec 13 19:59:53 minikube dockerd[215]: time="2021-12-13T19:59:53.386529200Z" level=info msg="ignoring event" container=289a53f4188125fa00f15c04f9d9f7fca7f06e111594a26da9f920cdde36a90c module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Dec 13 19:59:55 minikube dockerd[215]: time="2021-12-13T19:59:55.367638700Z" level=info msg="ignoring event" container=063913382a8d77f791c8780cbf33917a76b4251eb6aa15277c97678906559cc3 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Dec 13 19:59:56 minikube dockerd[215]: time="2021-12-13T19:59:56.876821900Z" level=info msg="ignoring event" container=bd126677da93842c954959dc7721832ce66c0ddc52ea42e3ffba9fa6004eb240 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Dec 13 20:00:00 minikube dockerd[215]: time="2021-12-13T20:00:00.757083900Z" level=info msg="ignoring event" container=3bbf9277849316620774b6ca4545b42c008892fb5a2ab1d718c596872d92dfbb module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Dec 13 20:00:09 minikube dockerd[215]: time="2021-12-13T20:00:09.962017600Z" level=info msg="ignoring event" container=8c1655006055f6669b5b67e90bfe5d2fa41fa591e0e17e9bcc7c412d55100d63 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Dec 13 20:00:17 minikube dockerd[215]: time="2021-12-13T20:00:17.001025200Z" level=info msg="ignoring event" container=ef914fcbc235634c70dcaf927da653be3ea859e15df69aa74a6f2c8408978c38 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Dec 13 20:00:21 minikube dockerd[215]: time="2021-12-13T20:00:21.142852000Z" level=info msg="ignoring event" container=c95258fd0a032cbc4e95bcbce87efe62de205f607f5fc9c5cc25b13ab2991d9b module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Dec 13 20:00:21 minikube dockerd[215]: time="2021-12-13T20:00:21.429046100Z" level=info msg="ignoring event" container=8356c8335d12936c134e06857cd4e0739cba6d37dfe9e04ffad1f0ffef6be96c module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Dec 13 20:00:24 minikube dockerd[215]: time="2021-12-13T20:00:24.454816700Z" level=info msg="ignoring event" container=664ab995a3c8f164434c1d4ba32bd6b0285296cf418d6d61f2f3c868b4f2e3dd module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Dec 13 20:00:24 minikube dockerd[215]: time="2021-12-13T20:00:24.841064400Z" level=info msg="ignoring event" container=cd35f0997e048089bb9707d4c5d5c68cca3efea05be564adecdaefd524483ad5 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Dec 13 20:12:48 minikube dockerd[215]: time="2021-12-13T20:12:48.975191400Z" level=info msg="ignoring event" container=d6d2545e03dd8121658e6c95839775cc7a147532e14087b77bb05a470cabe177 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Dec 13 20:12:49 minikube dockerd[215]: time="2021-12-13T20:12:49.094111500Z" level=info msg="ignoring event" container=5a3c0c487accde468a4dee9a2dc6c9cf8854486209163142a05e13acf1bd09c6 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Dec 13 20:12:51 minikube dockerd[215]: time="2021-12-13T20:12:51.094603400Z" level=info msg="ignoring event" container=ca491df1b0b6ede3ee203a2fd1cf29cd1fc0f645f7638033fe53deb47b372d90 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Dec 13 20:12:51 minikube dockerd[215]: time="2021-12-13T20:12:51.279943300Z" level=info msg="ignoring event" container=b7539933c7649daca39bfe31a819ddd1fd306218d6d4732296631f04a847df23 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Dec 13 20:18:45 minikube dockerd[215]: time="2021-12-13T20:18:45.464803500Z" level=info msg="ignoring event" container=74c240a729747a87fce5b2b907cc96b21f7b0e50509be46dbcb64e9401e41c91 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Dec 13 20:18:45 minikube dockerd[215]: time="2021-12-13T20:18:45.780919900Z" level=info msg="ignoring event" container=4e142fecd8c9b40cd21367aeb73256d062e6f72a886af03684bddc51cf879e00 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Dec 13 20:18:47 minikube dockerd[215]: time="2021-12-13T20:18:47.892616600Z" level=info msg="ignoring event" container=d1a7369a6a72d442e4297de000407636b40dc0a97f97244abeb697608eeea5ea module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Dec 13 20:18:48 minikube dockerd[215]: time="2021-12-13T20:18:48.076074900Z" level=info msg="ignoring event" container=646c3c56a0f1942d44e7e2a862270f361f42f5069aaaec0496dbe73aebb3e93c module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Dec 13 20:20:08 minikube dockerd[215]: time="2021-12-13T20:20:08.097849000Z" level=info msg="ignoring event" container=86ec172fcddae9b99635c5280488fdaf58e5b8560341b9387d7884eb4371b53a module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Dec 13 20:20:08 minikube dockerd[215]: time="2021-12-13T20:20:08.591193900Z" level=info msg="ignoring event" container=854d0b43f48fdfcd7fc0cd8a3526943b2e8a2f7b54640fd1222e834a3a42ffd1 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Dec 13 20:20:10 minikube dockerd[215]: time="2021-12-13T20:20:10.422476600Z" level=info msg="ignoring event" container=181ac1cb6cce88ecc18a0ca15a5305b8902a4609345380ca4c77f32554560003 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Dec 13 20:20:16 minikube dockerd[215]: time="2021-12-13T20:20:16.084119000Z" level=info msg="ignoring event" container=6743f2611623c891b1daeb134753e3af2f31336e6d3a559ad73e4f496b5259d9 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Dec 13 20:20:22 minikube dockerd[215]: time="2021-12-13T20:20:22.960101000Z" level=info msg="ignoring event" container=d4f77245a060e22957a3f0d916dd062e5daeb3d9409fd27cc4ef88d9670e3288 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Dec 13 20:20:27 minikube dockerd[215]: time="2021-12-13T20:20:27.397032300Z" level=info msg="ignoring event" container=761f6249260765cf9109cbf9bafc3ea03e66cc4d449b6d596db095d5b98b56fa module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Dec 13 20:20:27 minikube dockerd[215]: time="2021-12-13T20:20:27.600385000Z" level=info msg="ignoring event" container=00dfb67e7f7c63c1b70ff1236bf32b3f0b4efa601baeaf8a125d9a7299e5b142 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Dec 13 20:20:30 minikube dockerd[215]: time="2021-12-13T20:20:30.270108500Z" level=info msg="ignoring event" container=866219da29506cf36060712e7a3cadcc33baab99f99f848c508ef13bfbf0ecd4 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Dec 13 20:20:30 minikube dockerd[215]: time="2021-12-13T20:20:30.504236700Z" level=info msg="ignoring event" container=3f28f8c4f38254901cbfeaceac98fb1721377545519a3bb5becc89577265a69a module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Dec 13 20:21:56 minikube dockerd[215]: time="2021-12-13T20:21:56.674218000Z" level=info msg="ignoring event" container=cb14b6cf71717ef60bd30ffc9920b33995a04059465229ee95782da283685c53 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Dec 13 20:21:57 minikube dockerd[215]: time="2021-12-13T20:21:57.150959700Z" level=info msg="ignoring event" container=7daba194cbefc43981ab023ba2d5f7c8548b1f027031edcd072034a527c0a3de module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Dec 13 20:22:00 minikube dockerd[215]: time="2021-12-13T20:22:00.054418400Z" level=info msg="ignoring event" container=cae2f6f051fc27a1ac6129151fe2ed8bf9077f42e8d02bf3b9d6d18debd858b0 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Dec 13 20:22:05 minikube dockerd[215]: time="2021-12-13T20:22:05.820967600Z" level=info msg="ignoring event" container=28e115c1fd2dcfdcd195f241acec940d2fc88d8ac0b046aad1d5c6dc84964506 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Dec 13 20:22:13 minikube dockerd[215]: time="2021-12-13T20:22:13.054759100Z" level=info msg="ignoring event" container=21e188b6d7bd6c00d8d8a0e88da2950c6436b71f8272e7a5e3d9e375caaad328 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Dec 13 20:22:15 minikube dockerd[215]: time="2021-12-13T20:22:15.868985800Z" level=info msg="ignoring event" container=ab087d7161550507de45827cb62356e7b950dd19bd0f6a01c05f3fd8639f4635 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Dec 13 20:22:16 minikube dockerd[215]: time="2021-12-13T20:22:16.073194100Z" level=info msg="ignoring event" container=d42b9dc1a25089dc8b5ae82b64dec4f9cf413b917c3fa031ce3d00bd37585584 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Dec 13 20:22:18 minikube dockerd[215]: time="2021-12-13T20:22:18.980138300Z" level=info msg="ignoring event" container=f6614b16cefce8beed5026415910a1cb51170ba00752c94a6f03cadab7dfc86f module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Dec 13 20:22:19 minikube dockerd[215]: time="2021-12-13T20:22:19.163769500Z" level=info msg="ignoring event" container=92ec53090764d723374cf55bb6e34d0a164771d8b3ca208089cf350416294641 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Dec 13 20:37:37 minikube dockerd[215]: time="2021-12-13T20:37:37.571318800Z" level=info msg="ignoring event" container=a445c34c9102258d78dd2e262c43330495aaf07303bb279ae9dc1c60c575a829 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Dec 13 20:37:37 minikube dockerd[215]: time="2021-12-13T20:37:37.578685800Z" level=info msg="ignoring event" container=9257dcadce99a7c5ec68032caea43dd27db9ded3d974eb54f22830276bc97acc module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Dec 13 20:37:37 minikube dockerd[215]: time="2021-12-13T20:37:37.694582600Z" level=info msg="ignoring event" container=86d7444d9f5b0a41f4e966884177e9c8e0413d287901e1a01123dc9882443edb module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Dec 13 20:37:37 minikube dockerd[215]: time="2021-12-13T20:37:37.771357800Z" level=info msg="ignoring event" container=b78e342e1515cfe68c15ad19db2b61f455bc0ca07d79e0b3d7beaf9202500e70 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Dec 13 20:37:37 minikube dockerd[215]: time="2021-12-13T20:37:37.885815400Z" level=info msg="ignoring event" container=543cc57d6ed89f96a50f27273a6dc69f9b6452717e051039620f088488b46b66 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Dec 13 20:37:37 minikube dockerd[215]: time="2021-12-13T20:37:37.899033600Z" level=info msg="ignoring event" container=43cdd94cbe4b244e160a3dcf169714694b88cea691fe8a754ca797acfcf8fe87 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Dec 13 20:37:37 minikube dockerd[215]: time="2021-12-13T20:37:37.909034200Z" level=info msg="ignoring event" container=8cfa081e046b13c3157a6c4d59e10bb9695d840b54cab2d2e9612a768639f9ac module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Dec 13 20:37:37 minikube dockerd[215]: time="2021-12-13T20:37:37.919649700Z" level=info msg="ignoring event" container=5045abdc024330c828d1908649fdfec7f830d6f8c8f3998cbbca78911f8ecfed module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Dec 13 20:38:33 minikube dockerd[215]: time="2021-12-13T20:38:33.344108400Z" level=info msg="ignoring event" container=84c998d9ba005b9acd0da3dfa6da6ef3df3935f2d94b1b48807b7ac780591a01 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Dec 13 20:38:33 minikube dockerd[215]: time="2021-12-13T20:38:33.779023300Z" level=info msg="ignoring event" container=2571cd844e682c06afc171a6fb9806833ee19492afc995f8de724b397e2ab308 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Dec 13 20:38:35 minikube dockerd[215]: time="2021-12-13T20:38:35.372120900Z" level=info msg="ignoring event" container=ce94266d273df830502cf576b80c30e782f4bd6c681fd38eaf0c6eac64398ec2 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Dec 13 20:38:40 minikube dockerd[215]: time="2021-12-13T20:38:40.465008900Z" level=info msg="ignoring event" container=d3e964c135636795fccda28b2c8090b9f1599845611619310bd7ce6d14d2cf35 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Dec 13 20:38:47 minikube dockerd[215]: time="2021-12-13T20:38:46.999309400Z" level=info msg="ignoring event" container=672ad41b432cc9ed5c258e3c519ba11a374c231e4291ffc6b51221250b1d893a module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Dec 13 20:47:40 minikube dockerd[215]: time="2021-12-13T20:47:40.217208000Z" level=info msg="ignoring event" container=5a22a05fd17ea7bf568c420cc2a986ac869897fc97e4c192787b1b3989704603 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Dec 13 20:47:40 minikube dockerd[215]: time="2021-12-13T20:47:40.232718000Z" level=info msg="ignoring event" container=3ca021eac49ffda642e7f0f8ae009d9b2c75e0177816e7e83dd54e11bd498e14 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Dec 13 20:47:40 minikube dockerd[215]: time="2021-12-13T20:47:40.235532100Z" level=info msg="ignoring event" container=faf498aac96f5a5507af9d52b3d31402dbc04d820c0b7abaeb2e859ddf5414a8 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Dec 13 20:47:40 minikube dockerd[215]: time="2021-12-13T20:47:40.293357900Z" level=info msg="ignoring event" container=4f38dc38e9dfd3a2b5bef8ed1e97f57fb00cfbed9585247ae3ab3728e90f6a82 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Dec 13 20:47:40 minikube dockerd[215]: time="2021-12-13T20:47:40.416500100Z" level=info msg="ignoring event" container=56370076ef31a55c5a1a511a5141699cc7a0a89a0f7203e4596a48804708dcc9 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Dec 13 20:47:40 minikube dockerd[215]: time="2021-12-13T20:47:40.434194500Z" level=info msg="ignoring event" container=36a334f57fd1c499f78d89d1a3bbbba00f89f7126609c63e919f7656c5299944 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Dec 13 20:47:40 minikube dockerd[215]: time="2021-12-13T20:47:40.497452900Z" level=info msg="ignoring event" container=ade6542813efb5932b5ebe622e0b29e159c3e56fd03888d1bbe2358010ae9e8e module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Dec 13 20:47:40 minikube dockerd[215]: time="2021-12-13T20:47:40.498641200Z" level=info msg="ignoring event" container=5ac4d792efeed31e6418c70bfc8ac09923268b1f252e52b770da9af83d6e44f1 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"

* 
* ==> container status <==
* CONTAINER           IMAGE               CREATED              STATE               NAME                        ATTEMPT             POD ID
3eb65dc308abf       13c0d20291024       About a minute ago   Running             dispatch-api                0                   1536ee5fe38bb
1458bffd89c3b       44d9419ac2d1e       About a minute ago   Running             ordering-image              0                   e2229818f8a4b
c930be5a88bfa       44d9419ac2d1e       About a minute ago   Running             ordering-image              0                   367830a572ea2
78a0efff4d575       13c0d20291024       About a minute ago   Running             dispatch-api                0                   0840fc01c0d5d
9c0a9db6fa10b       b752ffe108c3a       5 hours ago          Running             rabbitmq                    0                   137e4bb50acc1
3eca36efb59f2       b752ffe108c3a       5 hours ago          Exited              setup-container             0                   137e4bb50acc1
8b8e280b404e4       e1482a24335a6       17 hours ago         Running             kubernetes-dashboard        4                   7191839555a55
e36baf943382f       6e38f40d628db       17 hours ago         Running             storage-provisioner         5                   51cbdd3427f97
9c7d45d53132c       7801cfc6d5c07       17 hours ago         Running             dashboard-metrics-scraper   2                   31cd964fa1c6a
f266c250cfba2       6120bd723dced       17 hours ago         Running             kube-proxy                  2                   633d6e32c57d8
f9e0f7c806e15       6e38f40d628db       17 hours ago         Exited              storage-provisioner         4                   51cbdd3427f97
24d75fa2dfb19       8d147537fb7d1       17 hours ago         Running             coredns                     2                   3252824afe7da
45cfd0fc9b57f       05c905cef780c       17 hours ago         Running             kube-controller-manager     2                   dd57c3a2599d6
379ef94b51800       53224b502ea4d       17 hours ago         Running             kube-apiserver              2                   7893fad7450d3
f6976956c4e41       0aa9c7e31d307       17 hours ago         Running             kube-scheduler              2                   8d3846077e610
5ee1036c208af       0048118155842       17 hours ago         Running             etcd                        2                   003dbe8715d50
685c256f4a4c2       6120bd723dced       2 days ago           Exited              kube-proxy                  1                   46db5611adc88
057739b218541       7801cfc6d5c07       2 days ago           Exited              dashboard-metrics-scraper   1                   ae093b72bdead
1176a62c6a599       8d147537fb7d1       2 days ago           Exited              coredns                     1                   5dea5ce103b90
d3ae499e7f5fe       05c905cef780c       2 days ago           Exited              kube-controller-manager     1                   1f35b8c9e6603
65af892bd55f4       0048118155842       2 days ago           Exited              etcd                        1                   60164a78875e2
c3d76ba5da766       0aa9c7e31d307       2 days ago           Exited              kube-scheduler              1                   dd15426cd6151
c259e0e0b11e8       53224b502ea4d       2 days ago           Exited              kube-apiserver              1                   ff1f2d67d21ed

* 
* ==> coredns [1176a62c6a59] <==
* [INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/ready: Still waiting on: "kubernetes"
[WARNING] plugin/kubernetes: starting server with unsynced Kubernetes API
.:53
[INFO] plugin/reload: Running configuration MD5 = c23ed519c17e71ee396ed052e6209e94
CoreDNS-1.8.4
linux/amd64, go1.16.4, 053c4d5
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/ready: Still waiting on: "kubernetes"

* 
* ==> coredns [24d75fa2dfb1] <==
* [INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/ready: Still waiting on: "kubernetes"
[WARNING] plugin/kubernetes: starting server with unsynced Kubernetes API
.:53
[INFO] plugin/reload: Running configuration MD5 = c23ed519c17e71ee396ed052e6209e94
CoreDNS-1.8.4
linux/amd64, go1.16.4, 053c4d5
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/ready: Still waiting on: "kubernetes"
[WARNING] plugin/health: Local health request to "http://:8080/health" failed: Get "http://:8080/health": context deadline exceeded (Client.Timeout exceeded while awaiting headers)

* 
* ==> describe nodes <==
* Name:               minikube
Roles:              control-plane,master
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=amd64
                    kubernetes.io/hostname=minikube
                    kubernetes.io/os=linux
                    minikube.k8s.io/commit=76b94fb3c4e8ac5062daf70d60cf03ddcc0a741b
                    minikube.k8s.io/name=minikube
                    minikube.k8s.io/updated_at=2021_12_10T09_03_28_0700
                    minikube.k8s.io/version=v1.24.0
                    node-role.kubernetes.io/control-plane=
                    node-role.kubernetes.io/master=
                    node.kubernetes.io/exclude-from-external-load-balancers=
Annotations:        kubeadm.alpha.kubernetes.io/cri-socket: /var/run/dockershim.sock
                    node.alpha.kubernetes.io/ttl: 0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Fri, 10 Dec 2021 14:03:24 +0000
Taints:             <none>
Unschedulable:      false
Lease:
  HolderIdentity:  minikube
  AcquireTime:     <unset>
  RenewTime:       Mon, 13 Dec 2021 20:49:26 +0000
Conditions:
  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----             ------  -----------------                 ------------------                ------                       -------
  MemoryPressure   False   Mon, 13 Dec 2021 20:47:47 +0000   Fri, 10 Dec 2021 14:03:19 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure     False   Mon, 13 Dec 2021 20:47:47 +0000   Fri, 10 Dec 2021 14:03:19 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure      False   Mon, 13 Dec 2021 20:47:47 +0000   Fri, 10 Dec 2021 14:03:19 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready            True    Mon, 13 Dec 2021 20:47:47 +0000   Fri, 10 Dec 2021 14:03:39 +0000   KubeletReady                 kubelet is posting ready status
Addresses:
  InternalIP:  192.168.49.2
  Hostname:    minikube
Capacity:
  cpu:                6
  ephemeral-storage:  61255492Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             2032964Ki
  pods:               110
Allocatable:
  cpu:                6
  ephemeral-storage:  61255492Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             2032964Ki
  pods:               110
System Info:
  Machine ID:                 bba0be70c47c400ea3cf7733f1c0b4c1
  System UUID:                d4485857-b7ac-407a-8a9b-cc77c5bc8ad3
  Boot ID:                    2ce21479-a374-4fb2-90fe-37f599ad2bfc
  Kernel Version:             5.10.47-linuxkit
  OS Image:                   Ubuntu 20.04.2 LTS
  Operating System:           linux
  Architecture:               amd64
  Container Runtime Version:  docker://20.10.8
  Kubelet Version:            v1.22.3
  Kube-Proxy Version:         v1.22.3
PodCIDR:                      10.244.0.0/24
PodCIDRs:                     10.244.0.0/24
Non-terminated Pods:          (14 in total)
  Namespace                   Name                                          CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
  ---------                   ----                                          ------------  ----------  ---------------  -------------  ---
  default                     dispatch-app-deployment-7db8d9494d-crznk      0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         108s
  default                     dispatch-app-deployment-7db8d9494d-hr7gj      0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         108s
  default                     hello-world-rabbitmq-server-0                 500m (8%!)(MISSING)     800m (13%!)(MISSING)  500Mi (25%!)(MISSING)      500Mi (25%!)(MISSING)    4h46m
  default                     ordering-app-deployment-8b649d968-chv45       0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         109s
  default                     ordering-app-deployment-8b649d968-cwj29       0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         109s
  kube-system                 coredns-78fcd69978-2lsgn                      100m (1%!)(MISSING)     0 (0%!)(MISSING)      70Mi (3%!)(MISSING)        170Mi (8%!)(MISSING)     3d6h
  kube-system                 etcd-minikube                                 100m (1%!)(MISSING)     0 (0%!)(MISSING)      100Mi (5%!)(MISSING)       0 (0%!)(MISSING)         3d6h
  kube-system                 kube-apiserver-minikube                       250m (4%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         3d6h
  kube-system                 kube-controller-manager-minikube              200m (3%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         3d6h
  kube-system                 kube-proxy-n4tdm                              0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         3d6h
  kube-system                 kube-scheduler-minikube                       100m (1%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         3d6h
  kube-system                 storage-provisioner                           0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         3d6h
  kubernetes-dashboard        dashboard-metrics-scraper-5594458c94-4hhxg    0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         3d6h
  kubernetes-dashboard        kubernetes-dashboard-654cf69797-86s6j         0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         3d6h
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests     Limits
  --------           --------     ------
  cpu                1250m (20%!)(MISSING)  800m (13%!)(MISSING)
  memory             670Mi (33%!)(MISSING)  670Mi (33%!)(MISSING)
  ephemeral-storage  0 (0%!)(MISSING)       0 (0%!)(MISSING)
  hugepages-1Gi      0 (0%!)(MISSING)       0 (0%!)(MISSING)
  hugepages-2Mi      0 (0%!)(MISSING)       0 (0%!)(MISSING)
Events:              <none>

* 
* ==> dmesg <==
* [Dec13 11:53] ERROR: earlyprintk= earlyser already used
[  +0.000000] ERROR: earlyprintk= earlyser already used
[  +0.000000] ACPI BIOS Warning (bug): Incorrect checksum in table [DSDT] - 0x7E, should be 0xDB (20200925/tbprint-173)
[  +0.157900]  #2
[  +0.065074]  #3
[  +0.065982]  #4
[  +0.064041]  #5
[  +1.888698] Hangcheck: starting hangcheck timer 0.9.1 (tick is 180 seconds, margin is 60 seconds).
[  +0.031646] ACPI Error: Could not enable RealTimeClock event (20200925/evxfevnt-182)
[  +0.001780] ACPI Warning: Could not enable fixed event - RealTimeClock (4) (20200925/evxface-618)
[  +4.638081] grpcfuse: loading out-of-tree module taints kernel.
[Dec13 14:12] clocksource: timekeeping watchdog on CPU2: Marking clocksource 'tsc' as unstable because the skew is too large:
[  +0.000267] clocksource:                       'hpet' wd_now: 52897ea5 wd_last: 4f92c941 mask: ffffffff
[  +0.000114] clocksource:                       'tsc' cs_now: 16c911ba7526 cs_last: 16c55ab3cc66 mask: ffffffffffffffff
[  +0.012078] TSC found unstable after boot, most likely due to broken BIOS. Use 'tsc=unstable'.
[Dec13 14:13] hrtimer: interrupt took 2484700 ns

* 
* ==> etcd [5ee1036c208a] <==
* {"level":"info","ts":"2021-12-13T19:50:42.251Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":65781}
{"level":"info","ts":"2021-12-13T19:50:42.360Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":65781,"took":"101.5913ms"}
{"level":"info","ts":"2021-12-13T19:55:41.964Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":66303}
{"level":"info","ts":"2021-12-13T19:55:42.067Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":66303,"took":"89.1016ms"}
{"level":"warn","ts":"2021-12-13T19:59:34.706Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"366.6795ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128009631001376054 username:\"kube-apiserver-etcd-client\" auth_revision:1 > txn:<compare:<target:MOD key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" mod_revision:67215 > success:<request_put:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" value_size:1022 >> failure:<request_range:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" > >>","response":"size:18"}
{"level":"warn","ts":"2021-12-13T19:59:34.712Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"666.4834ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2021-12-13T19:59:34.712Z","caller":"traceutil/trace.go:171","msg":"trace[1553443201] transaction","detail":"{read_only:false; response_revision:67216; number_of_response:1; }","duration":"744.8487ms","start":"2021-12-13T19:59:34.000Z","end":"2021-12-13T19:59:34.711Z","steps":["trace[1553443201] 'store kv pair into bolt db' {req_type:put; key:/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath; req_size:1092; } (duration: 353.5511ms)"],"step_count":1}
{"level":"info","ts":"2021-12-13T19:59:34.712Z","caller":"traceutil/trace.go:171","msg":"trace[667277673] linearizableReadLoop","detail":"{readStateIndex:81446; appliedIndex:81445; }","duration":"665.993ms","start":"2021-12-13T19:59:34.079Z","end":"2021-12-13T19:59:34.711Z","steps":["trace[667277673] 'read index received'  (duration: 234.4001ms)","trace[667277673] 'applied index is now lower than readState.Index'  (duration: 431.5748ms)"],"step_count":2}
{"level":"info","ts":"2021-12-13T19:59:34.713Z","caller":"traceutil/trace.go:171","msg":"trace[79345386] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:67216; }","duration":"666.5865ms","start":"2021-12-13T19:59:34.079Z","end":"2021-12-13T19:59:34.712Z","steps":["trace[79345386] 'agreement among raft nodes before linearized reading'  (duration: 666.3684ms)"],"step_count":1}
{"level":"warn","ts":"2021-12-13T19:59:34.716Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2021-12-13T19:59:34.079Z","time spent":"669.7299ms","remote":"127.0.0.1:39104","response type":"/etcdserverpb.KV/Range","request count":0,"request size":18,"response count":0,"response size":30,"request content":"key:\"/registry/health\" "}
{"level":"warn","ts":"2021-12-13T19:59:34.716Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2021-12-13T19:59:34.000Z","time spent":"748.2226ms","remote":"127.0.0.1:39036","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":1095,"response count":0,"response size":42,"request content":"compare:<target:MOD key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" mod_revision:67215 > success:<request_put:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" value_size:1022 >> failure:<request_range:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" > >"}
{"level":"warn","ts":"2021-12-13T19:59:43.013Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"114.035ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/horizontalpodautoscalers/\" range_end:\"/registry/horizontalpodautoscalers0\" count_only:true ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2021-12-13T19:59:43.013Z","caller":"traceutil/trace.go:171","msg":"trace[1226815591] range","detail":"{range_begin:/registry/horizontalpodautoscalers/; range_end:/registry/horizontalpodautoscalers0; response_count:0; response_revision:67222; }","duration":"114.1564ms","start":"2021-12-13T19:59:42.899Z","end":"2021-12-13T19:59:43.013Z","steps":["trace[1226815591] 'agreement among raft nodes before linearized reading'  (duration: 113.9776ms)"],"step_count":1}
{"level":"info","ts":"2021-12-13T19:59:43.013Z","caller":"traceutil/trace.go:171","msg":"trace[786656142] linearizableReadLoop","detail":"{readStateIndex:81454; appliedIndex:81454; }","duration":"113.7619ms","start":"2021-12-13T19:59:42.899Z","end":"2021-12-13T19:59:43.013Z","steps":["trace[786656142] 'read index received'  (duration: 113.7293ms)","trace[786656142] 'applied index is now lower than readState.Index'  (duration: 13.2µs)"],"step_count":2}
{"level":"warn","ts":"2021-12-13T19:59:59.786Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"105.3309ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/services/endpoints/default/kubernetes\" ","response":"range_response_count:1 size:425"}
{"level":"info","ts":"2021-12-13T19:59:59.895Z","caller":"traceutil/trace.go:171","msg":"trace[1006312586] range","detail":"{range_begin:/registry/services/endpoints/default/kubernetes; range_end:; response_count:1; response_revision:67307; }","duration":"208.8514ms","start":"2021-12-13T19:59:59.659Z","end":"2021-12-13T19:59:59.868Z","steps":["trace[1006312586] 'agreement among raft nodes before linearized reading'  (duration: 94.6231ms)"],"step_count":1}
{"level":"warn","ts":"2021-12-13T20:00:20.629Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"265.7685ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/events/default/\" range_end:\"/registry/events/default0\" ","response":"range_response_count:188 size:138743"}
{"level":"info","ts":"2021-12-13T20:00:20.639Z","caller":"traceutil/trace.go:171","msg":"trace[2033504583] range","detail":"{range_begin:/registry/events/default/; range_end:/registry/events/default0; response_count:188; response_revision:67352; }","duration":"314.3665ms","start":"2021-12-13T20:00:20.324Z","end":"2021-12-13T20:00:20.638Z","steps":["trace[2033504583] 'range keys from bolt db'  (duration: 262.7406ms)"],"step_count":1}
{"level":"warn","ts":"2021-12-13T20:00:20.644Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2021-12-13T20:00:20.324Z","time spent":"317.0874ms","remote":"127.0.0.1:39018","response type":"/etcdserverpb.KV/Range","request count":0,"request size":54,"response count":188,"response size":138767,"request content":"key:\"/registry/events/default/\" range_end:\"/registry/events/default0\" "}
{"level":"warn","ts":"2021-12-13T20:00:23.946Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"184.2958ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128009631001376630 username:\"kube-apiserver-etcd-client\" auth_revision:1 > txn:<compare:<target:MOD key:\"/registry/apiextensions.k8s.io/customresourcedefinitions/rabbitmqclusters.rabbitmq.com\" mod_revision:67207 > success:<request_put:<key:\"/registry/apiextensions.k8s.io/customresourcedefinitions/rabbitmqclusters.rabbitmq.com\" value_size:199628 >> failure:<request_range:<key:\"/registry/apiextensions.k8s.io/customresourcedefinitions/rabbitmqclusters.rabbitmq.com\" > >>","response":"size:18"}
{"level":"warn","ts":"2021-12-13T20:00:23.947Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"196.3861ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/replicasets/default/ordering-app-deployment-7f966c6c86\" ","response":"range_response_count:1 size:2153"}
{"level":"info","ts":"2021-12-13T20:00:23.947Z","caller":"traceutil/trace.go:171","msg":"trace[982332792] range","detail":"{range_begin:/registry/replicasets/default/ordering-app-deployment-7f966c6c86; range_end:; response_count:1; response_revision:67390; }","duration":"196.4828ms","start":"2021-12-13T20:00:23.750Z","end":"2021-12-13T20:00:23.947Z","steps":["trace[982332792] 'agreement among raft nodes before linearized reading'  (duration: 196.2811ms)"],"step_count":1}
{"level":"info","ts":"2021-12-13T20:00:23.947Z","caller":"traceutil/trace.go:171","msg":"trace[46480746] transaction","detail":"{read_only:false; response_revision:67390; number_of_response:1; }","duration":"200.2211ms","start":"2021-12-13T20:00:23.746Z","end":"2021-12-13T20:00:23.947Z","steps":["trace[46480746] 'compare'  (duration: 120.1913ms)","trace[46480746] 'marshal mvccpb.KeyValue' {req_type:put; key:/registry/apiextensions.k8s.io/customresourcedefinitions/rabbitmqclusters.rabbitmq.com; req_size:199720; } (duration: 63.8761ms)"],"step_count":2}
{"level":"info","ts":"2021-12-13T20:00:23.947Z","caller":"traceutil/trace.go:171","msg":"trace[1206032293] linearizableReadLoop","detail":"{readStateIndex:81634; appliedIndex:81633; }","duration":"195.9985ms","start":"2021-12-13T20:00:23.751Z","end":"2021-12-13T20:00:23.947Z","steps":["trace[1206032293] 'read index received'  (duration: 140.3µs)","trace[1206032293] 'applied index is now lower than readState.Index'  (duration: 195.8147ms)"],"step_count":2}
{"level":"warn","ts":"2021-12-13T20:00:23.948Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"122.9446ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/csistoragecapacities/\" range_end:\"/registry/csistoragecapacities0\" count_only:true ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2021-12-13T20:00:23.948Z","caller":"traceutil/trace.go:171","msg":"trace[1531674158] range","detail":"{range_begin:/registry/csistoragecapacities/; range_end:/registry/csistoragecapacities0; response_count:0; response_revision:67390; }","duration":"123.0423ms","start":"2021-12-13T20:00:23.825Z","end":"2021-12-13T20:00:23.948Z","steps":["trace[1531674158] 'agreement among raft nodes before linearized reading'  (duration: 122.8606ms)"],"step_count":1}
{"level":"info","ts":"2021-12-13T20:00:41.639Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":66902}
{"level":"info","ts":"2021-12-13T20:00:41.747Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":66902,"took":"106.3436ms"}
{"level":"info","ts":"2021-12-13T20:05:41.305Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":67409}
{"level":"info","ts":"2021-12-13T20:05:41.396Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":67409,"took":"88.2246ms"}
{"level":"info","ts":"2021-12-13T20:10:40.972Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":67621}
{"level":"info","ts":"2021-12-13T20:10:40.974Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":67621,"took":"1.3935ms"}
{"level":"info","ts":"2021-12-13T20:15:40.634Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":67843}
{"level":"info","ts":"2021-12-13T20:15:40.645Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":67843,"took":"9.4063ms"}
{"level":"warn","ts":"2021-12-13T20:20:09.810Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"127.4352ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/services/specs/\" range_end:\"/registry/services/specs0\" count_only:true ","response":"range_response_count:0 size:8"}
{"level":"info","ts":"2021-12-13T20:20:09.780Z","caller":"traceutil/trace.go:171","msg":"trace[1666066660] linearizableReadLoop","detail":"{readStateIndex:82890; appliedIndex:82890; }","duration":"109.6169ms","start":"2021-12-13T20:20:09.645Z","end":"2021-12-13T20:20:09.755Z","steps":["trace[1666066660] 'read index received'  (duration: 109.5457ms)","trace[1666066660] 'applied index is now lower than readState.Index'  (duration: 13.6µs)"],"step_count":2}
{"level":"info","ts":"2021-12-13T20:20:09.819Z","caller":"traceutil/trace.go:171","msg":"trace[1918300282] range","detail":"{range_begin:/registry/services/specs/; range_end:/registry/services/specs0; response_count:0; response_revision:68397; }","duration":"174.3148ms","start":"2021-12-13T20:20:09.645Z","end":"2021-12-13T20:20:09.819Z","steps":["trace[1918300282] 'agreement among raft nodes before linearized reading'  (duration: 109.9593ms)"],"step_count":1}
{"level":"warn","ts":"2021-12-13T20:20:09.824Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"170.0845ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2021-12-13T20:20:09.824Z","caller":"traceutil/trace.go:171","msg":"trace[952332822] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:68397; }","duration":"170.1629ms","start":"2021-12-13T20:20:09.654Z","end":"2021-12-13T20:20:09.824Z","steps":["trace[952332822] 'agreement among raft nodes before linearized reading'  (duration: 170.0176ms)"],"step_count":1}
{"level":"warn","ts":"2021-12-13T20:20:27.096Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"211.8327ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/events/default/\" range_end:\"/registry/events/default0\" ","response":"range_response_count:239 size:176334"}
{"level":"info","ts":"2021-12-13T20:20:27.096Z","caller":"traceutil/trace.go:171","msg":"trace[1460411086] range","detail":"{range_begin:/registry/events/default/; range_end:/registry/events/default0; response_count:239; response_revision:68434; }","duration":"211.957ms","start":"2021-12-13T20:20:26.884Z","end":"2021-12-13T20:20:27.096Z","steps":["trace[1460411086] 'range keys from bolt db'  (duration: 211.4378ms)"],"step_count":1}
{"level":"info","ts":"2021-12-13T20:20:40.303Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":68128}
{"level":"info","ts":"2021-12-13T20:20:40.368Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":68128,"took":"60.4816ms"}
{"level":"info","ts":"2021-12-13T20:25:39.979Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":68491}
{"level":"info","ts":"2021-12-13T20:25:40.069Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":68491,"took":"78.4851ms"}
{"level":"info","ts":"2021-12-13T20:30:39.643Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":68777}
{"level":"info","ts":"2021-12-13T20:30:39.660Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":68777,"took":"16.7457ms"}
{"level":"info","ts":"2021-12-13T20:35:39.304Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":68989}
{"level":"info","ts":"2021-12-13T20:35:39.305Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":68989,"took":"462.9µs"}
{"level":"info","ts":"2021-12-13T20:38:42.614Z","caller":"traceutil/trace.go:171","msg":"trace[1206998088] linearizableReadLoop","detail":"{readStateIndex:84113; appliedIndex:84113; }","duration":"123.4526ms","start":"2021-12-13T20:38:42.485Z","end":"2021-12-13T20:38:42.608Z","steps":["trace[1206998088] 'read index received'  (duration: 123.3728ms)","trace[1206998088] 'applied index is now lower than readState.Index'  (duration: 45µs)"],"step_count":2}
{"level":"warn","ts":"2021-12-13T20:38:42.631Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"127.7251ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" ","response":"range_response_count:1 size:1114"}
{"level":"info","ts":"2021-12-13T20:38:42.633Z","caller":"traceutil/trace.go:171","msg":"trace[1045107774] range","detail":"{range_begin:/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath; range_end:; response_count:1; response_revision:69383; }","duration":"147.9852ms","start":"2021-12-13T20:38:42.485Z","end":"2021-12-13T20:38:42.633Z","steps":["trace[1045107774] 'agreement among raft nodes before linearized reading'  (duration: 123.6517ms)"],"step_count":1}
{"level":"warn","ts":"2021-12-13T20:38:43.627Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"105.5549ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128009631001390888 username:\"kube-apiserver-etcd-client\" auth_revision:1 > txn:<compare:<target:MOD key:\"/registry/apiextensions.k8s.io/customresourcedefinitions/rabbitmqclusters.rabbitmq.com\" mod_revision:69342 > success:<request_put:<key:\"/registry/apiextensions.k8s.io/customresourcedefinitions/rabbitmqclusters.rabbitmq.com\" value_size:199628 >> failure:<request_range:<key:\"/registry/apiextensions.k8s.io/customresourcedefinitions/rabbitmqclusters.rabbitmq.com\" > >>","response":"size:18"}
{"level":"info","ts":"2021-12-13T20:38:43.627Z","caller":"traceutil/trace.go:171","msg":"trace[2088205280] transaction","detail":"{read_only:false; response_revision:69385; number_of_response:1; }","duration":"127.8069ms","start":"2021-12-13T20:38:43.500Z","end":"2021-12-13T20:38:43.627Z","steps":["trace[2088205280] 'compare'  (duration: 102.827ms)"],"step_count":1}
{"level":"warn","ts":"2021-12-13T20:38:51.265Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"138.1938ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/events/default/\" range_end:\"/registry/events/default0\" ","response":"range_response_count:288 size:212573"}
{"level":"info","ts":"2021-12-13T20:38:51.265Z","caller":"traceutil/trace.go:171","msg":"trace[40374876] range","detail":"{range_begin:/registry/events/default/; range_end:/registry/events/default0; response_count:288; response_revision:69455; }","duration":"138.3249ms","start":"2021-12-13T20:38:51.126Z","end":"2021-12-13T20:38:51.265Z","steps":["trace[40374876] 'range keys from bolt db'  (duration: 137.4411ms)"],"step_count":1}
{"level":"info","ts":"2021-12-13T20:40:38.974Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":69208}
{"level":"info","ts":"2021-12-13T20:40:39.032Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":69208,"took":"53.4721ms"}
{"level":"info","ts":"2021-12-13T20:45:38.643Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":69550}
{"level":"info","ts":"2021-12-13T20:45:38.660Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":69550,"took":"15.3568ms"}

* 
* ==> etcd [65af892bd55f] <==
* {"level":"info","ts":"2021-12-11T05:23:49.776Z","caller":"traceutil/trace.go:171","msg":"trace[101978867] linearizableReadLoop","detail":"{readStateIndex:36026; appliedIndex:36026; }","duration":"171.2947ms","start":"2021-12-11T05:23:49.605Z","end":"2021-12-11T05:23:49.776Z","steps":["trace[101978867] 'read index received'  (duration: 170.998ms)","trace[101978867] 'applied index is now lower than readState.Index'  (duration: 197.9µs)"],"step_count":2}
{"level":"warn","ts":"2021-12-11T05:23:49.777Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"172.6874ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/apiextensions.k8s.io/customresourcedefinitions/\" range_end:\"/registry/apiextensions.k8s.io/customresourcedefinitions0\" count_only:true ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2021-12-11T05:23:49.777Z","caller":"traceutil/trace.go:171","msg":"trace[1585178874] range","detail":"{range_begin:/registry/apiextensions.k8s.io/customresourcedefinitions/; range_end:/registry/apiextensions.k8s.io/customresourcedefinitions0; response_count:0; response_revision:28170; }","duration":"172.8257ms","start":"2021-12-11T05:23:49.604Z","end":"2021-12-11T05:23:49.777Z","steps":["trace[1585178874] 'agreement among raft nodes before linearized reading'  (duration: 171.6739ms)"],"step_count":1}
{"level":"info","ts":"2021-12-11T10:33:23.259Z","caller":"traceutil/trace.go:171","msg":"trace[783855343] linearizableReadLoop","detail":"{readStateIndex:36051; appliedIndex:36051; }","duration":"338.0525ms","start":"2021-12-11T10:33:22.921Z","end":"2021-12-11T10:33:23.259Z","steps":["trace[783855343] 'read index received'  (duration: 337.8403ms)","trace[783855343] 'applied index is now lower than readState.Index'  (duration: 54.4µs)"],"step_count":2}
{"level":"warn","ts":"2021-12-11T10:33:23.261Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"315.1001ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" ","response":"range_response_count:1 size:1114"}
{"level":"info","ts":"2021-12-11T10:33:23.261Z","caller":"traceutil/trace.go:171","msg":"trace[1595016533] range","detail":"{range_begin:/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath; range_end:; response_count:1; response_revision:28189; }","duration":"315.3321ms","start":"2021-12-11T10:33:22.946Z","end":"2021-12-11T10:33:23.261Z","steps":["trace[1595016533] 'agreement among raft nodes before linearized reading'  (duration: 313.8041ms)"],"step_count":1}
{"level":"warn","ts":"2021-12-11T10:33:23.261Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"317.0035ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:6"}
{"level":"warn","ts":"2021-12-11T10:33:23.262Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2021-12-11T10:33:22.946Z","time spent":"315.4233ms","remote":"127.0.0.1:39840","response type":"/etcdserverpb.KV/Range","request count":0,"request size":67,"response count":1,"response size":1138,"request content":"key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" "}
{"level":"info","ts":"2021-12-11T10:33:23.262Z","caller":"traceutil/trace.go:171","msg":"trace[735261124] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:28189; }","duration":"317.1225ms","start":"2021-12-11T10:33:22.944Z","end":"2021-12-11T10:33:23.262Z","steps":["trace[735261124] 'agreement among raft nodes before linearized reading'  (duration: 315.5264ms)"],"step_count":1}
{"level":"warn","ts":"2021-12-11T10:33:23.262Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"317.1225ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/events/\" range_end:\"/registry/events0\" ","response":"range_response_count:56 size:38884"}
{"level":"warn","ts":"2021-12-11T10:33:23.262Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2021-12-11T10:33:22.944Z","time spent":"317.2265ms","remote":"127.0.0.1:39912","response type":"/etcdserverpb.KV/Range","request count":0,"request size":18,"response count":0,"response size":30,"request content":"key:\"/registry/health\" "}
{"level":"info","ts":"2021-12-11T10:33:23.262Z","caller":"traceutil/trace.go:171","msg":"trace[442245131] range","detail":"{range_begin:/registry/events/; range_end:/registry/events0; response_count:56; response_revision:28189; }","duration":"317.4877ms","start":"2021-12-11T10:33:22.944Z","end":"2021-12-11T10:33:23.262Z","steps":["trace[442245131] 'agreement among raft nodes before linearized reading'  (duration: 315.4645ms)"],"step_count":1}
{"level":"warn","ts":"2021-12-11T10:33:23.262Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2021-12-11T10:33:22.944Z","time spent":"317.5687ms","remote":"127.0.0.1:39972","response type":"/etcdserverpb.KV/Range","request count":0,"request size":38,"response count":56,"response size":38908,"request content":"key:\"/registry/events/\" range_end:\"/registry/events0\" "}
{"level":"warn","ts":"2021-12-11T10:33:23.262Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"341.3938ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/csinodes/\" range_end:\"/registry/csinodes0\" count_only:true ","response":"range_response_count:0 size:8"}
{"level":"info","ts":"2021-12-11T10:33:23.262Z","caller":"traceutil/trace.go:171","msg":"trace[1407089660] range","detail":"{range_begin:/registry/csinodes/; range_end:/registry/csinodes0; response_count:0; response_revision:28189; }","duration":"341.4875ms","start":"2021-12-11T10:33:22.921Z","end":"2021-12-11T10:33:23.262Z","steps":["trace[1407089660] 'agreement among raft nodes before linearized reading'  (duration: 338.4556ms)"],"step_count":1}
{"level":"warn","ts":"2021-12-11T10:33:23.263Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2021-12-11T10:33:22.921Z","time spent":"341.5813ms","remote":"127.0.0.1:39932","response type":"/etcdserverpb.KV/Range","request count":0,"request size":44,"response count":1,"response size":32,"request content":"key:\"/registry/csinodes/\" range_end:\"/registry/csinodes0\" count_only:true "}
{"level":"info","ts":"2021-12-11T10:33:34.956Z","caller":"traceutil/trace.go:171","msg":"trace[302353594] linearizableReadLoop","detail":"{readStateIndex:36060; appliedIndex:36060; }","duration":"335.996ms","start":"2021-12-11T10:33:34.620Z","end":"2021-12-11T10:33:34.956Z","steps":["trace[302353594] 'read index received'  (duration: 335.5719ms)","trace[302353594] 'applied index is now lower than readState.Index'  (duration: 341.6µs)"],"step_count":2}
{"level":"warn","ts":"2021-12-11T10:33:34.957Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"336.6504ms","expected-duration":"100ms","prefix":"read-only range ","request":"limit:1 keys_only:true ","response":"range_response_count:0 size:6"}
{"level":"warn","ts":"2021-12-11T10:33:34.957Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"333.0786ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/priorityclasses/\" range_end:\"/registry/priorityclasses0\" count_only:true ","response":"range_response_count:0 size:8"}
{"level":"info","ts":"2021-12-11T10:33:34.957Z","caller":"traceutil/trace.go:171","msg":"trace[196976559] range","detail":"{range_begin:; range_end:; response_count:0; response_revision:28196; }","duration":"336.8533ms","start":"2021-12-11T10:33:34.620Z","end":"2021-12-11T10:33:34.957Z","steps":["trace[196976559] 'agreement among raft nodes before linearized reading'  (duration: 336.4442ms)"],"step_count":1}
{"level":"warn","ts":"2021-12-11T10:33:34.957Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"225.7735ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/serviceaccounts/kube-system/kube-proxy\" ","response":"range_response_count:1 size:227"}
{"level":"info","ts":"2021-12-11T10:33:34.958Z","caller":"traceutil/trace.go:171","msg":"trace[1717597205] range","detail":"{range_begin:/registry/serviceaccounts/kube-system/kube-proxy; range_end:; response_count:1; response_revision:28196; }","duration":"225.8742ms","start":"2021-12-11T10:33:34.732Z","end":"2021-12-11T10:33:34.957Z","steps":["trace[1717597205] 'agreement among raft nodes before linearized reading'  (duration: 225.5187ms)"],"step_count":1}
{"level":"warn","ts":"2021-12-11T10:33:34.958Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"325.3646ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:6"}
{"level":"warn","ts":"2021-12-11T10:33:34.958Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"225.2461ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/serviceaccounts/kubernetes-dashboard/kubernetes-dashboard\" ","response":"range_response_count:1 size:1152"}
{"level":"info","ts":"2021-12-11T10:33:34.958Z","caller":"traceutil/trace.go:171","msg":"trace[1609335762] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:28196; }","duration":"325.4796ms","start":"2021-12-11T10:33:34.632Z","end":"2021-12-11T10:33:34.958Z","steps":["trace[1609335762] 'agreement among raft nodes before linearized reading'  (duration: 325.2113ms)"],"step_count":1}
{"level":"warn","ts":"2021-12-11T10:33:34.958Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"326.0752ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" ","response":"range_response_count:1 size:1114"}
{"level":"info","ts":"2021-12-11T10:33:34.958Z","caller":"traceutil/trace.go:171","msg":"trace[1949400615] range","detail":"{range_begin:/registry/serviceaccounts/kubernetes-dashboard/kubernetes-dashboard; range_end:; response_count:1; response_revision:28196; }","duration":"225.3711ms","start":"2021-12-11T10:33:34.732Z","end":"2021-12-11T10:33:34.958Z","steps":["trace[1949400615] 'agreement among raft nodes before linearized reading'  (duration: 225.0843ms)"],"step_count":1}
{"level":"warn","ts":"2021-12-11T10:33:34.958Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2021-12-11T10:33:34.632Z","time spent":"325.6206ms","remote":"127.0.0.1:39912","response type":"/etcdserverpb.KV/Range","request count":0,"request size":18,"response count":0,"response size":30,"request content":"key:\"/registry/health\" "}
{"level":"info","ts":"2021-12-11T10:33:34.958Z","caller":"traceutil/trace.go:171","msg":"trace[512539106] range","detail":"{range_begin:/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath; range_end:; response_count:1; response_revision:28196; }","duration":"326.2445ms","start":"2021-12-11T10:33:34.631Z","end":"2021-12-11T10:33:34.958Z","steps":["trace[512539106] 'agreement among raft nodes before linearized reading'  (duration: 325.8409ms)"],"step_count":1}
{"level":"warn","ts":"2021-12-11T10:33:34.958Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2021-12-11T10:33:34.631Z","time spent":"326.3431ms","remote":"127.0.0.1:39840","response type":"/etcdserverpb.KV/Range","request count":0,"request size":67,"response count":1,"response size":1138,"request content":"key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" "}
{"level":"info","ts":"2021-12-11T10:33:34.959Z","caller":"traceutil/trace.go:171","msg":"trace[1913658559] range","detail":"{range_begin:/registry/priorityclasses/; range_end:/registry/priorityclasses0; response_count:0; response_revision:28196; }","duration":"335.4133ms","start":"2021-12-11T10:33:34.624Z","end":"2021-12-11T10:33:34.957Z","steps":["trace[1913658559] 'agreement among raft nodes before linearized reading'  (duration: 332.9833ms)"],"step_count":1}
{"level":"warn","ts":"2021-12-11T10:33:34.959Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2021-12-11T10:33:34.624Z","time spent":"335.5857ms","remote":"127.0.0.1:39926","response type":"/etcdserverpb.KV/Range","request count":0,"request size":58,"response count":2,"response size":32,"request content":"key:\"/registry/priorityclasses/\" range_end:\"/registry/priorityclasses0\" count_only:true "}
{"level":"info","ts":"2021-12-11T14:33:23.410Z","caller":"traceutil/trace.go:171","msg":"trace[1361753477] linearizableReadLoop","detail":"{readStateIndex:36070; appliedIndex:36070; }","duration":"299.9647ms","start":"2021-12-11T14:33:23.110Z","end":"2021-12-11T14:33:23.410Z","steps":["trace[1361753477] 'read index received'  (duration: 299.9015ms)","trace[1361753477] 'applied index is now lower than readState.Index'  (duration: 30.5µs)"],"step_count":2}
{"level":"warn","ts":"2021-12-11T14:33:23.411Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"162.2215ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:6"}
{"level":"warn","ts":"2021-12-11T14:33:23.411Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"301.5712ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/flowschemas/system-node-high\" ","response":"range_response_count:1 size:1144"}
{"level":"info","ts":"2021-12-11T14:33:23.411Z","caller":"traceutil/trace.go:171","msg":"trace[1441235913] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:28204; }","duration":"162.4047ms","start":"2021-12-11T14:33:23.249Z","end":"2021-12-11T14:33:23.411Z","steps":["trace[1441235913] 'agreement among raft nodes before linearized reading'  (duration: 161.3206ms)"],"step_count":1}
{"level":"info","ts":"2021-12-11T14:33:23.411Z","caller":"traceutil/trace.go:171","msg":"trace[1894726609] range","detail":"{range_begin:/registry/flowschemas/system-node-high; range_end:; response_count:1; response_revision:28204; }","duration":"301.6987ms","start":"2021-12-11T14:33:23.110Z","end":"2021-12-11T14:33:23.411Z","steps":["trace[1894726609] 'agreement among raft nodes before linearized reading'  (duration: 300.2535ms)"],"step_count":1}
{"level":"warn","ts":"2021-12-11T14:33:23.412Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2021-12-11T14:33:23.110Z","time spent":"301.8034ms","remote":"127.0.0.1:39948","response type":"/etcdserverpb.KV/Range","request count":0,"request size":40,"response count":1,"response size":1168,"request content":"key:\"/registry/flowschemas/system-node-high\" "}
{"level":"info","ts":"2021-12-11T14:43:39.455Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":28162}
{"level":"info","ts":"2021-12-11T14:43:39.456Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":28162,"took":"507.4µs"}
{"level":"info","ts":"2021-12-11T14:48:39.119Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":28369}
{"level":"info","ts":"2021-12-11T14:48:39.119Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":28369,"took":"572.4µs"}
{"level":"info","ts":"2021-12-11T14:53:38.782Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":28580}
{"level":"info","ts":"2021-12-11T14:53:38.783Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":28580,"took":"370.6µs"}
{"level":"info","ts":"2021-12-11T14:58:38.445Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":28790}
{"level":"info","ts":"2021-12-11T14:58:38.446Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":28790,"took":"420.6µs"}
{"level":"info","ts":"2021-12-11T15:03:38.108Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":28999}
{"level":"info","ts":"2021-12-11T15:03:38.109Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":28999,"took":"501.2µs"}
{"level":"info","ts":"2021-12-11T15:05:57.636Z","caller":"traceutil/trace.go:171","msg":"trace[950676781] linearizableReadLoop","detail":"{readStateIndex:37496; appliedIndex:37496; }","duration":"323.0438ms","start":"2021-12-11T15:05:57.313Z","end":"2021-12-11T15:05:57.636Z","steps":["trace[950676781] 'read index received'  (duration: 322.9691ms)","trace[950676781] 'applied index is now lower than readState.Index'  (duration: 29.8µs)"],"step_count":2}
{"level":"warn","ts":"2021-12-11T15:05:57.638Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"320.4033ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" ","response":"range_response_count:1 size:1114"}
{"level":"warn","ts":"2021-12-11T15:05:57.638Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"324.6569ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2021-12-11T15:05:57.638Z","caller":"traceutil/trace.go:171","msg":"trace[746670962] range","detail":"{range_begin:/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath; range_end:; response_count:1; response_revision:29309; }","duration":"320.6279ms","start":"2021-12-11T15:05:57.318Z","end":"2021-12-11T15:05:57.638Z","steps":["trace[746670962] 'agreement among raft nodes before linearized reading'  (duration: 319.7329ms)"],"step_count":1}
{"level":"info","ts":"2021-12-11T15:05:57.638Z","caller":"traceutil/trace.go:171","msg":"trace[1036890537] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:29309; }","duration":"324.808ms","start":"2021-12-11T15:05:57.313Z","end":"2021-12-11T15:05:57.638Z","steps":["trace[1036890537] 'agreement among raft nodes before linearized reading'  (duration: 323.5268ms)"],"step_count":1}
{"level":"warn","ts":"2021-12-11T15:05:57.638Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2021-12-11T15:05:57.317Z","time spent":"320.8091ms","remote":"127.0.0.1:39840","response type":"/etcdserverpb.KV/Range","request count":0,"request size":67,"response count":1,"response size":1138,"request content":"key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" "}
{"level":"warn","ts":"2021-12-11T15:05:57.638Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2021-12-11T15:05:57.313Z","time spent":"325.0161ms","remote":"127.0.0.1:39912","response type":"/etcdserverpb.KV/Range","request count":0,"request size":18,"response count":0,"response size":30,"request content":"key:\"/registry/health\" "}
{"level":"info","ts":"2021-12-11T15:47:57.445Z","caller":"traceutil/trace.go:171","msg":"trace[1127439152] linearizableReadLoop","detail":"{readStateIndex:37607; appliedIndex:37605; }","duration":"144.1208ms","start":"2021-12-11T15:47:57.301Z","end":"2021-12-11T15:47:57.445Z","steps":["trace[1127439152] 'read index received'  (duration: 142.4519ms)","trace[1127439152] 'applied index is now lower than readState.Index'  (duration: 1.6416ms)"],"step_count":2}
{"level":"info","ts":"2021-12-11T15:47:57.445Z","caller":"traceutil/trace.go:171","msg":"trace[1904590672] transaction","detail":"{read_only:false; response_revision:29394; number_of_response:1; }","duration":"203.7909ms","start":"2021-12-11T15:47:57.241Z","end":"2021-12-11T15:47:57.445Z","steps":["trace[1904590672] 'process raft request'  (duration: 202.066ms)"],"step_count":1}
{"level":"info","ts":"2021-12-11T15:47:57.445Z","caller":"traceutil/trace.go:171","msg":"trace[617708697] transaction","detail":"{read_only:false; response_revision:29395; number_of_response:1; }","duration":"201.5002ms","start":"2021-12-11T15:47:57.244Z","end":"2021-12-11T15:47:57.445Z","steps":["trace[617708697] 'process raft request'  (duration: 201.0021ms)"],"step_count":1}
{"level":"warn","ts":"2021-12-11T15:47:57.445Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"144.6147ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/minions/\" range_end:\"/registry/minions0\" count_only:true ","response":"range_response_count:0 size:8"}
{"level":"info","ts":"2021-12-11T15:47:57.447Z","caller":"traceutil/trace.go:171","msg":"trace[82466385] range","detail":"{range_begin:/registry/minions/; range_end:/registry/minions0; response_count:0; response_revision:29395; }","duration":"145.6977ms","start":"2021-12-11T15:47:57.301Z","end":"2021-12-11T15:47:57.447Z","steps":["trace[82466385] 'agreement among raft nodes before linearized reading'  (duration: 144.4408ms)"],"step_count":1}

* 
* ==> kernel <==
*  20:49:32 up  8:55,  0 users,  load average: 0.64, 0.73, 0.67
Linux minikube 5.10.47-linuxkit #1 SMP Sat Jul 3 21:51:47 UTC 2021 x86_64 x86_64 x86_64 GNU/Linux
PRETTY_NAME="Ubuntu 20.04.2 LTS"

* 
* ==> kube-apiserver [379ef94b5180] <==
* I1213 20:21:59.579276       1 trace.go:205] Trace[1950420546]: "Update" url:/api/v1/namespaces/kube-system/endpoints/k8s.io-minikube-hostpath,user-agent:storage-provisioner/v0.0.0 (linux/amd64) kubernetes/$Format,audit-id:f16d8748-2be6-47f0-9809-0129be007a48,client:192.168.49.2,accept:application/json, */*,protocol:HTTP/2.0 (13-Dec-2021 20:21:58.838) (total time: 692ms):
Trace[1950420546]: ---"Conversion done" 214ms (20:21:59.053)
Trace[1950420546]: ---"Object stored in database" 477ms (20:21:59.530)
Trace[1950420546]: [692.0495ms] [692.0495ms] END
I1213 20:22:16.637450       1 trace.go:205] Trace[1994333645]: "Get" url:/api/v1/namespaces/default/pods/ordering-app-deployment-765fccdc6d-k6qgd/log,user-agent:kubectl/v1.21.5 (darwin/amd64) kubernetes/aea7bba,audit-id:d1ce57f3-510d-4e54-9a43-d1c7b40e3974,client:192.168.49.1,accept:application/json, */*,protocol:HTTP/2.0 (13-Dec-2021 20:20:26.793) (total time: 109979ms):
Trace[1994333645]: ---"Transformed response object" 109970ms (20:22:16.637)
Trace[1994333645]: [1m49.9795449s] [1m49.9795449s] END
I1213 20:22:19.685476       1 trace.go:205] Trace[409100180]: "Get" url:/api/v1/namespaces/default/pods/ordering-app-deployment-765fccdc6d-tj4t6/log,user-agent:kubectl/v1.21.5 (darwin/amd64) kubernetes/aea7bba,audit-id:cb556aac-13ff-4331-9341-166be5590d45,client:192.168.49.1,accept:application/json, */*,protocol:HTTP/2.0 (13-Dec-2021 20:20:29.492) (total time: 110328ms):
Trace[409100180]: ---"Transformed response object" 110253ms (20:22:19.685)
Trace[409100180]: [1m50.3282811s] [1m50.3282811s] END
E1213 20:22:45.700710       1 crd_finalizer.go:299] rabbitmqclusters.rabbitmq.com failed with: timed out waiting for the condition
E1213 20:26:42.619759       1 crd_finalizer.go:299] rabbitmqclusters.rabbitmq.com failed with: timed out waiting for the condition
E1213 20:31:42.219045       1 crd_finalizer.go:299] rabbitmqclusters.rabbitmq.com failed with: timed out waiting for the condition
W1213 20:32:32.922262       1 watcher.go:229] watch chan error: etcdserver: mvcc: required revision has been compacted
E1213 20:32:42.383342       1 crd_finalizer.go:299] rabbitmqclusters.rabbitmq.com failed with: timed out waiting for the condition
E1213 20:33:42.480840       1 crd_finalizer.go:299] rabbitmqclusters.rabbitmq.com failed with: timed out waiting for the condition
E1213 20:34:42.575110       1 crd_finalizer.go:299] rabbitmqclusters.rabbitmq.com failed with: timed out waiting for the condition
E1213 20:35:42.706848       1 crd_finalizer.go:299] rabbitmqclusters.rabbitmq.com failed with: timed out waiting for the condition
E1213 20:36:42.919813       1 crd_finalizer.go:299] rabbitmqclusters.rabbitmq.com failed with: timed out waiting for the condition
I1213 20:37:37.204412       1 trace.go:205] Trace[1562747947]: "Get" url:/api/v1/namespaces/default/pods/dispatch-app-deployment-6f8cb865c7-nsgf9/log,user-agent:kubectl/v1.21.5 (darwin/amd64) kubernetes/aea7bba,audit-id:3b3eefaf-1a17-45b2-944c-1219cfc56c24,client:192.168.49.1,accept:application/json, */*,protocol:HTTP/2.0 (13-Dec-2021 19:58:12.792) (total time: 2367099ms):
Trace[1562747947]: ---"Transformed response object" 2367024ms (20:37:37.203)
Trace[1562747947]: [39m27.0999118s] [39m27.0999118s] END
I1213 20:37:37.204476       1 trace.go:205] Trace[354150446]: "Get" url:/api/v1/namespaces/default/pods/ordering-app-deployment-6856b7468f-f5gnp/log,user-agent:kubectl/v1.21.5 (darwin/amd64) kubernetes/aea7bba,audit-id:4a3ff639-127e-44f0-b2f1-62735d349244,client:192.168.49.1,accept:application/json, */*,protocol:HTTP/2.0 (13-Dec-2021 20:22:15.357) (total time: 922906ms):
Trace[354150446]: ---"Transformed response object" 922891ms (20:37:37.202)
Trace[354150446]: [15m22.9067189s] [15m22.9067189s] END
I1213 20:37:37.204698       1 trace.go:205] Trace[1519124092]: "Get" url:/api/v1/namespaces/default/pods/dispatch-app-deployment-6f8cb865c7-fldhz/log,user-agent:kubectl/v1.21.5 (darwin/amd64) kubernetes/aea7bba,audit-id:f4bf0925-3e41-4572-bf7c-b0e272fccb9f,client:192.168.49.1,accept:application/json, */*,protocol:HTTP/2.0 (13-Dec-2021 19:58:15.091) (total time: 2364800ms):
Trace[1519124092]: ---"Transformed response object" 2364796ms (20:37:37.203)
Trace[1519124092]: [39m24.8009247s] [39m24.8009247s] END
I1213 20:37:37.204939       1 trace.go:205] Trace[860612780]: "Get" url:/api/v1/namespaces/default/pods/ordering-app-deployment-6856b7468f-n7kjg/log,user-agent:kubectl/v1.21.5 (darwin/amd64) kubernetes/aea7bba,audit-id:350c82e9-e295-4958-8785-c25669e65fcf,client:192.168.49.1,accept:application/json, */*,protocol:HTTP/2.0 (13-Dec-2021 20:22:18.065) (total time: 920197ms):
Trace[860612780]: ---"Transformed response object" 920132ms (20:37:37.202)
Trace[860612780]: [15m20.1974804s] [15m20.1974804s] END
E1213 20:37:43.082268       1 crd_finalizer.go:299] rabbitmqclusters.rabbitmq.com failed with: timed out waiting for the condition
E1213 20:38:43.663808       1 crd_finalizer.go:299] rabbitmqclusters.rabbitmq.com failed with: timed out waiting for the condition
E1213 20:39:44.174200       1 crd_finalizer.go:299] rabbitmqclusters.rabbitmq.com failed with: timed out waiting for the condition
E1213 20:40:26.853064       1 rest.go:582] Address {172.17.0.2  0xc00dd0d490 0xc0158a9110} isn't valid (context canceled)
E1213 20:40:26.859301       1 status.go:71] apiserver received an error that is not an metav1.Status: &errors.errorString{s:"context canceled"}: context canceled
E1213 20:40:26.859664       1 rest.go:592] Failed to find a valid address, skipping subset: &{[{172.17.0.2  0xc00dd0d490 0xc0158a9110}] [] [{ 9090 TCP <nil>}]}
E1213 20:40:44.521682       1 crd_finalizer.go:299] rabbitmqclusters.rabbitmq.com failed with: timed out waiting for the condition
E1213 20:41:44.874916       1 crd_finalizer.go:299] rabbitmqclusters.rabbitmq.com failed with: timed out waiting for the condition
E1213 20:42:45.050434       1 crd_finalizer.go:299] rabbitmqclusters.rabbitmq.com failed with: timed out waiting for the condition
E1213 20:43:45.361412       1 crd_finalizer.go:299] rabbitmqclusters.rabbitmq.com failed with: timed out waiting for the condition
E1213 20:44:45.679094       1 crd_finalizer.go:299] rabbitmqclusters.rabbitmq.com failed with: timed out waiting for the condition
E1213 20:45:45.918765       1 crd_finalizer.go:299] rabbitmqclusters.rabbitmq.com failed with: timed out waiting for the condition
E1213 20:46:02.496245       1 status.go:71] apiserver received an error that is not an metav1.Status: &errors.errorString{s:"context canceled"}: context canceled
E1213 20:46:02.496288       1 status.go:71] apiserver received an error that is not an metav1.Status: &errors.errorString{s:"context canceled"}: context canceled
E1213 20:46:46.256205       1 crd_finalizer.go:299] rabbitmqclusters.rabbitmq.com failed with: timed out waiting for the condition
I1213 20:47:38.101700       1 trace.go:205] Trace[574990068]: "Get" url:/api/v1/namespaces/default/pods/dispatch-app-deployment-5fd7f7cb68-bstk5/log,user-agent:kubectl/v1.21.5 (darwin/amd64) kubernetes/aea7bba,audit-id:b528bad4-29e5-4ed4-ad5d-c73e01bc95db,client:192.168.49.1,accept:application/json, */*,protocol:HTTP/2.0 (13-Dec-2021 20:38:52.198) (total time: 526510ms):
Trace[574990068]: ---"Transformed response object" 526504ms (20:47:38.100)
Trace[574990068]: [8m46.5102757s] [8m46.5102757s] END
I1213 20:47:38.101791       1 trace.go:205] Trace[1003548323]: "Get" url:/api/v1/namespaces/default/pods/ordering-app-deployment-6cf8b57d6f-7trhv/log,user-agent:kubectl/v1.21.5 (darwin/amd64) kubernetes/aea7bba,audit-id:283d12ed-b2bf-4eae-81a0-ca8f8f3fe776,client:192.168.49.1,accept:application/json, */*,protocol:HTTP/2.0 (13-Dec-2021 20:38:52.205) (total time: 526502ms):
Trace[1003548323]: ---"Transformed response object" 526497ms (20:47:38.099)
Trace[1003548323]: [8m46.5021265s] [8m46.5021265s] END
I1213 20:47:38.101709       1 trace.go:205] Trace[727200821]: "Get" url:/api/v1/namespaces/default/pods/dispatch-app-deployment-5fd7f7cb68-6tnsx/log,user-agent:kubectl/v1.21.5 (darwin/amd64) kubernetes/aea7bba,audit-id:6e79e858-17fb-4a8a-9f34-aeb29190f8c5,client:192.168.49.1,accept:application/json, */*,protocol:HTTP/2.0 (13-Dec-2021 20:38:52.197) (total time: 526510ms):
Trace[727200821]: ---"Transformed response object" 526504ms (20:47:38.099)
Trace[727200821]: [8m46.5105391s] [8m46.5105391s] END
I1213 20:47:38.102139       1 trace.go:205] Trace[541946866]: "Get" url:/api/v1/namespaces/default/pods/ordering-app-deployment-6cf8b57d6f-2tq6t/log,user-agent:kubectl/v1.21.5 (darwin/amd64) kubernetes/aea7bba,audit-id:2c6523b0-38ad-4ddd-b423-f9e02f3f931b,client:192.168.49.1,accept:application/json, */*,protocol:HTTP/2.0 (13-Dec-2021 20:38:52.194) (total time: 526516ms):
Trace[541946866]: ---"Transformed response object" 526509ms (20:47:38.102)
Trace[541946866]: [8m46.516455s] [8m46.516455s] END
E1213 20:47:46.520994       1 crd_finalizer.go:299] rabbitmqclusters.rabbitmq.com failed with: timed out waiting for the condition
E1213 20:48:46.929872       1 crd_finalizer.go:299] rabbitmqclusters.rabbitmq.com failed with: timed out waiting for the condition

* 
* ==> kube-apiserver [c259e0e0b11e] <==
* W1211 04:48:35.655894       1 genericapiserver.go:455] Skipping API flowcontrol.apiserver.k8s.io/v1alpha1 because it has no resources.
W1211 04:48:35.661605       1 genericapiserver.go:455] Skipping API apps/v1beta2 because it has no resources.
W1211 04:48:35.661649       1 genericapiserver.go:455] Skipping API apps/v1beta1 because it has no resources.
W1211 04:48:35.663666       1 genericapiserver.go:455] Skipping API admissionregistration.k8s.io/v1beta1 because it has no resources.
I1211 04:48:35.667675       1 plugins.go:158] Loaded 12 mutating admission controller(s) successfully in the following order: NamespaceLifecycle,LimitRanger,ServiceAccount,NodeRestriction,TaintNodesByCondition,Priority,DefaultTolerationSeconds,DefaultStorageClass,StorageObjectInUseProtection,RuntimeClass,DefaultIngressClass,MutatingAdmissionWebhook.
I1211 04:48:35.667717       1 plugins.go:161] Loaded 11 validating admission controller(s) successfully in the following order: LimitRanger,ServiceAccount,PodSecurity,Priority,PersistentVolumeClaimResize,RuntimeClass,CertificateApproval,CertificateSigning,CertificateSubjectRestriction,ValidatingAdmissionWebhook,ResourceQuota.
W1211 04:48:35.693875       1 genericapiserver.go:455] Skipping API apiregistration.k8s.io/v1beta1 because it has no resources.
I1211 04:48:37.355504       1 dynamic_cafile_content.go:155] "Starting controller" name="request-header::/var/lib/minikube/certs/front-proxy-ca.crt"
I1211 04:48:37.355623       1 dynamic_cafile_content.go:155] "Starting controller" name="client-ca-bundle::/var/lib/minikube/certs/ca.crt"
I1211 04:48:37.355698       1 dynamic_serving_content.go:129] "Starting controller" name="serving-cert::/var/lib/minikube/certs/apiserver.crt::/var/lib/minikube/certs/apiserver.key"
I1211 04:48:37.356113       1 secure_serving.go:266] Serving securely on [::]:8443
I1211 04:48:37.356315       1 customresource_discovery_controller.go:209] Starting DiscoveryController
I1211 04:48:37.356447       1 tlsconfig.go:240] "Starting DynamicServingCertificateController"
I1211 04:48:37.356455       1 nonstructuralschema_controller.go:192] Starting NonStructuralSchemaConditionController
I1211 04:48:37.356534       1 controller.go:85] Starting OpenAPI controller
I1211 04:48:37.356595       1 naming_controller.go:291] Starting NamingConditionController
I1211 04:48:37.356630       1 establishing_controller.go:76] Starting EstablishingController
I1211 04:48:37.356970       1 autoregister_controller.go:141] Starting autoregister controller
I1211 04:48:37.357070       1 cache.go:32] Waiting for caches to sync for autoregister controller
I1211 04:48:37.357193       1 crdregistration_controller.go:111] Starting crd-autoregister controller
I1211 04:48:37.357389       1 shared_informer.go:240] Waiting for caches to sync for crd-autoregister
I1211 04:48:37.356992       1 cluster_authentication_trust_controller.go:440] Starting cluster_authentication_trust_controller controller
I1211 04:48:37.357581       1 shared_informer.go:240] Waiting for caches to sync for cluster_authentication_trust_controller
I1211 04:48:37.357812       1 controller.go:83] Starting OpenAPI AggregationController
I1211 04:48:37.357911       1 dynamic_cafile_content.go:155] "Starting controller" name="client-ca-bundle::/var/lib/minikube/certs/ca.crt"
I1211 04:48:37.358066       1 dynamic_cafile_content.go:155] "Starting controller" name="request-header::/var/lib/minikube/certs/front-proxy-ca.crt"
I1211 04:48:37.358663       1 apiservice_controller.go:97] Starting APIServiceRegistrationController
I1211 04:48:37.358706       1 cache.go:32] Waiting for caches to sync for APIServiceRegistrationController controller
I1211 04:48:37.359511       1 apf_controller.go:312] Starting API Priority and Fairness config controller
I1211 04:48:37.359536       1 apiapproval_controller.go:186] Starting KubernetesAPIApprovalPolicyConformantConditionController
I1211 04:48:37.359580       1 crd_finalizer.go:266] Starting CRDFinalizer
I1211 04:48:37.356315       1 dynamic_serving_content.go:129] "Starting controller" name="aggregator-proxy-cert::/var/lib/minikube/certs/front-proxy-client.crt::/var/lib/minikube/certs/front-proxy-client.key"
I1211 04:48:37.359686       1 available_controller.go:491] Starting AvailableConditionController
I1211 04:48:37.359706       1 cache.go:32] Waiting for caches to sync for AvailableConditionController controller
I1211 04:48:37.523106       1 cache.go:39] Caches are synced for APIServiceRegistrationController controller
E1211 04:48:37.532934       1 controller.go:152] Unable to remove old endpoints from kubernetes service: no master IPs were listed in storage, refusing to erase all endpoints for the kubernetes service
I1211 04:48:37.551571       1 shared_informer.go:247] Caches are synced for node_authorizer 
I1211 04:48:37.558065       1 shared_informer.go:247] Caches are synced for cluster_authentication_trust_controller 
I1211 04:48:37.558138       1 cache.go:39] Caches are synced for autoregister controller
I1211 04:48:37.558321       1 shared_informer.go:247] Caches are synced for crd-autoregister 
I1211 04:48:37.608642       1 apf_controller.go:317] Running API Priority and Fairness config worker
I1211 04:48:37.608649       1 cache.go:39] Caches are synced for AvailableConditionController controller
I1211 04:48:37.620104       1 controller.go:611] quota admission added evaluator for: leases.coordination.k8s.io
I1211 04:48:38.356923       1 controller.go:132] OpenAPI AggregationController: action for item : Nothing (removed from the queue).
I1211 04:48:38.356989       1 controller.go:132] OpenAPI AggregationController: action for item k8s_internal_local_delegation_chain_0000000000: Nothing (removed from the queue).
I1211 04:48:38.366494       1 storage_scheduling.go:148] all system priority classes are created successfully or already exist.
I1211 04:48:41.316076       1 controller.go:611] quota admission added evaluator for: serviceaccounts
I1211 04:48:41.346174       1 controller.go:611] quota admission added evaluator for: deployments.apps
I1211 04:48:41.635147       1 controller.go:611] quota admission added evaluator for: daemonsets.apps
I1211 04:48:41.749171       1 controller.go:611] quota admission added evaluator for: roles.rbac.authorization.k8s.io
I1211 04:48:41.825817       1 controller.go:611] quota admission added evaluator for: rolebindings.rbac.authorization.k8s.io
I1211 04:48:43.533542       1 controller.go:611] quota admission added evaluator for: events.events.k8s.io
I1211 04:48:44.221844       1 controller.go:611] quota admission added evaluator for: endpoints
I1211 04:48:51.112405       1 controller.go:611] quota admission added evaluator for: endpointslices.discovery.k8s.io
W1211 05:03:42.073881       1 watcher.go:229] watch chan error: etcdserver: mvcc: required revision has been compacted
W1211 05:17:04.156910       1 watcher.go:229] watch chan error: etcdserver: mvcc: required revision has been compacted
W1211 10:33:20.510535       1 watcher.go:229] watch chan error: etcdserver: mvcc: required revision has been compacted
E1211 10:33:30.973968       1 authentication.go:63] "Unable to authenticate the request" err="[invalid bearer token, Token has expired.]"
E1211 10:33:31.114771       1 authentication.go:63] "Unable to authenticate the request" err="[invalid bearer token, Token has expired.]"
W1211 14:55:19.819033       1 watcher.go:229] watch chan error: etcdserver: mvcc: required revision has been compacted

* 
* ==> kube-controller-manager [45cfd0fc9b57] <==
* I1213 19:59:47.902205       1 event.go:291] "Event occurred" object="default/ordering-app-deployment-7f966c6c86" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: ordering-app-deployment-7f966c6c86-vbff4"
I1213 19:59:49.160996       1 event.go:291] "Event occurred" object="default/ordering-app-deployment" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled down replica set ordering-app-deployment-5cb45ccfdd to 1"
I1213 19:59:49.179825       1 event.go:291] "Event occurred" object="default/ordering-app-deployment-5cb45ccfdd" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulDelete" message="Deleted pod: ordering-app-deployment-5cb45ccfdd-lm2sr"
I1213 19:59:49.200713       1 event.go:291] "Event occurred" object="default/ordering-app-deployment" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set ordering-app-deployment-7f966c6c86 to 2"
I1213 19:59:49.271766       1 event.go:291] "Event occurred" object="default/ordering-app-deployment-7f966c6c86" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: ordering-app-deployment-7f966c6c86-cqr9p"
I1213 19:59:51.685749       1 event.go:291] "Event occurred" object="default/ordering-app-deployment" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled down replica set ordering-app-deployment-5cb45ccfdd to 0"
I1213 19:59:51.765750       1 event.go:291] "Event occurred" object="default/ordering-app-deployment-5cb45ccfdd" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulDelete" message="Deleted pod: ordering-app-deployment-5cb45ccfdd-z5bfm"
I1213 20:00:18.040436       1 event.go:291] "Event occurred" object="default/ordering-app-deployment" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set ordering-app-deployment-697d664747 to 1"
I1213 20:00:18.075068       1 event.go:291] "Event occurred" object="default/ordering-app-deployment-697d664747" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: ordering-app-deployment-697d664747-wvv68"
I1213 20:00:20.046221       1 event.go:291] "Event occurred" object="default/ordering-app-deployment" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled down replica set ordering-app-deployment-7f966c6c86 to 1"
I1213 20:00:20.143102       1 event.go:291] "Event occurred" object="default/ordering-app-deployment-7f966c6c86" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulDelete" message="Deleted pod: ordering-app-deployment-7f966c6c86-vbff4"
I1213 20:00:20.256847       1 event.go:291] "Event occurred" object="default/ordering-app-deployment" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set ordering-app-deployment-697d664747 to 2"
I1213 20:00:20.346003       1 event.go:291] "Event occurred" object="default/ordering-app-deployment-697d664747" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: ordering-app-deployment-697d664747-87c2m"
I1213 20:00:23.239403       1 event.go:291] "Event occurred" object="default/ordering-app-deployment" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled down replica set ordering-app-deployment-7f966c6c86 to 0"
I1213 20:00:23.260998       1 event.go:291] "Event occurred" object="default/ordering-app-deployment-7f966c6c86" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulDelete" message="Deleted pod: ordering-app-deployment-7f966c6c86-cqr9p"
W1213 20:00:26.155020       1 endpointslice_controller.go:306] Error syncing endpoint slices for service "default/ordering-app-service", retrying. Error: EndpointSlice informer cache is out of date
I1213 20:12:46.254319       1 event.go:291] "Event occurred" object="default/ordering-app-deployment" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set ordering-app-deployment-69544c556d to 1"
I1213 20:12:46.265353       1 event.go:291] "Event occurred" object="default/ordering-app-deployment-69544c556d" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: ordering-app-deployment-69544c556d-8x6jg"
I1213 20:12:48.522092       1 event.go:291] "Event occurred" object="default/ordering-app-deployment" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled down replica set ordering-app-deployment-697d664747 to 1"
I1213 20:12:48.535625       1 event.go:291] "Event occurred" object="default/ordering-app-deployment-697d664747" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulDelete" message="Deleted pod: ordering-app-deployment-697d664747-wvv68"
I1213 20:12:48.579747       1 event.go:291] "Event occurred" object="default/ordering-app-deployment" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set ordering-app-deployment-69544c556d to 2"
I1213 20:12:48.596223       1 event.go:291] "Event occurred" object="default/ordering-app-deployment-69544c556d" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: ordering-app-deployment-69544c556d-z8zgg"
I1213 20:12:50.684612       1 event.go:291] "Event occurred" object="default/ordering-app-deployment" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled down replica set ordering-app-deployment-697d664747 to 0"
I1213 20:12:50.701458       1 event.go:291] "Event occurred" object="default/ordering-app-deployment-697d664747" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulDelete" message="Deleted pod: ordering-app-deployment-697d664747-87c2m"
I1213 20:18:42.451355       1 event.go:291] "Event occurred" object="default/ordering-app-deployment" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set ordering-app-deployment-697d664747 to 1"
I1213 20:18:42.464262       1 event.go:291] "Event occurred" object="default/ordering-app-deployment-697d664747" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: ordering-app-deployment-697d664747-5grhb"
I1213 20:18:44.391311       1 event.go:291] "Event occurred" object="default/ordering-app-deployment" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled down replica set ordering-app-deployment-69544c556d to 1"
I1213 20:18:44.465838       1 event.go:291] "Event occurred" object="default/ordering-app-deployment-69544c556d" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulDelete" message="Deleted pod: ordering-app-deployment-69544c556d-z8zgg"
I1213 20:18:44.469565       1 event.go:291] "Event occurred" object="default/ordering-app-deployment" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set ordering-app-deployment-697d664747 to 2"
I1213 20:18:44.481820       1 event.go:291] "Event occurred" object="default/ordering-app-deployment-697d664747" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: ordering-app-deployment-697d664747-fcstf"
I1213 20:18:47.192028       1 event.go:291] "Event occurred" object="default/ordering-app-deployment" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled down replica set ordering-app-deployment-69544c556d to 0"
I1213 20:18:47.283949       1 event.go:291] "Event occurred" object="default/ordering-app-deployment-69544c556d" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulDelete" message="Deleted pod: ordering-app-deployment-69544c556d-8x6jg"
I1213 20:20:25.693588       1 event.go:291] "Event occurred" object="default/ordering-app-deployment" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set ordering-app-deployment-765fccdc6d to 1"
I1213 20:20:25.718338       1 event.go:291] "Event occurred" object="default/ordering-app-deployment-765fccdc6d" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: ordering-app-deployment-765fccdc6d-k6qgd"
I1213 20:20:26.705658       1 event.go:291] "Event occurred" object="default/ordering-app-deployment" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled down replica set ordering-app-deployment-697d664747 to 1"
I1213 20:20:26.770872       1 event.go:291] "Event occurred" object="default/ordering-app-deployment-697d664747" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulDelete" message="Deleted pod: ordering-app-deployment-697d664747-5grhb"
I1213 20:20:26.941994       1 event.go:291] "Event occurred" object="default/ordering-app-deployment" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set ordering-app-deployment-765fccdc6d to 2"
I1213 20:20:26.976655       1 event.go:291] "Event occurred" object="default/ordering-app-deployment-765fccdc6d" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: ordering-app-deployment-765fccdc6d-tj4t6"
I1213 20:20:29.490686       1 event.go:291] "Event occurred" object="default/ordering-app-deployment" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled down replica set ordering-app-deployment-697d664747 to 0"
I1213 20:20:29.565114       1 event.go:291] "Event occurred" object="default/ordering-app-deployment-697d664747" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulDelete" message="Deleted pod: ordering-app-deployment-697d664747-fcstf"
I1213 20:22:14.032106       1 event.go:291] "Event occurred" object="default/ordering-app-deployment" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set ordering-app-deployment-6856b7468f to 1"
I1213 20:22:14.056214       1 event.go:291] "Event occurred" object="default/ordering-app-deployment-6856b7468f" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: ordering-app-deployment-6856b7468f-f5gnp"
I1213 20:22:15.286057       1 event.go:291] "Event occurred" object="default/ordering-app-deployment" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled down replica set ordering-app-deployment-765fccdc6d to 1"
I1213 20:22:15.330179       1 event.go:291] "Event occurred" object="default/ordering-app-deployment-765fccdc6d" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulDelete" message="Deleted pod: ordering-app-deployment-765fccdc6d-k6qgd"
I1213 20:22:15.365117       1 event.go:291] "Event occurred" object="default/ordering-app-deployment" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set ordering-app-deployment-6856b7468f to 2"
I1213 20:22:15.372723       1 event.go:291] "Event occurred" object="default/ordering-app-deployment-6856b7468f" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: ordering-app-deployment-6856b7468f-n7kjg"
I1213 20:22:17.961342       1 event.go:291] "Event occurred" object="default/ordering-app-deployment" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled down replica set ordering-app-deployment-765fccdc6d to 0"
I1213 20:22:18.049869       1 event.go:291] "Event occurred" object="default/ordering-app-deployment-765fccdc6d" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulDelete" message="Deleted pod: ordering-app-deployment-765fccdc6d-tj4t6"
I1213 20:38:48.015290       1 event.go:291] "Event occurred" object="default/ordering-app-deployment" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set ordering-app-deployment-6cf8b57d6f to 2"
I1213 20:38:48.015362       1 event.go:291] "Event occurred" object="default/dispatch-app-deployment" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set dispatch-app-deployment-5fd7f7cb68 to 2"
I1213 20:38:48.053653       1 event.go:291] "Event occurred" object="default/dispatch-app-deployment-5fd7f7cb68" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: dispatch-app-deployment-5fd7f7cb68-6tnsx"
I1213 20:38:48.055605       1 event.go:291] "Event occurred" object="default/ordering-app-deployment-6cf8b57d6f" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: ordering-app-deployment-6cf8b57d6f-7trhv"
I1213 20:38:48.100961       1 event.go:291] "Event occurred" object="default/dispatch-app-deployment-5fd7f7cb68" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: dispatch-app-deployment-5fd7f7cb68-bstk5"
I1213 20:38:48.103116       1 event.go:291] "Event occurred" object="default/ordering-app-deployment-6cf8b57d6f" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: ordering-app-deployment-6cf8b57d6f-2tq6t"
I1213 20:47:43.951378       1 event.go:291] "Event occurred" object="default/ordering-app-deployment" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set ordering-app-deployment-8b649d968 to 2"
I1213 20:47:43.959527       1 event.go:291] "Event occurred" object="default/ordering-app-deployment-8b649d968" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: ordering-app-deployment-8b649d968-cwj29"
I1213 20:47:43.966465       1 event.go:291] "Event occurred" object="default/ordering-app-deployment-8b649d968" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: ordering-app-deployment-8b649d968-chv45"
I1213 20:47:44.012249       1 event.go:291] "Event occurred" object="default/dispatch-app-deployment" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set dispatch-app-deployment-7db8d9494d to 2"
I1213 20:47:44.019163       1 event.go:291] "Event occurred" object="default/dispatch-app-deployment-7db8d9494d" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: dispatch-app-deployment-7db8d9494d-crznk"
I1213 20:47:44.099559       1 event.go:291] "Event occurred" object="default/dispatch-app-deployment-7db8d9494d" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: dispatch-app-deployment-7db8d9494d-hr7gj"

* 
* ==> kube-controller-manager [d3ae499e7f5f] <==
* I1211 04:48:50.862800       1 disruption.go:363] Starting disruption controller
I1211 04:48:50.862837       1 shared_informer.go:240] Waiting for caches to sync for disruption
I1211 04:48:50.864958       1 controllermanager.go:577] Started "csrcleaner"
I1211 04:48:50.865183       1 cleaner.go:82] Starting CSR cleaner controller
I1211 04:48:50.868752       1 controllermanager.go:577] Started "bootstrapsigner"
I1211 04:48:50.868978       1 shared_informer.go:240] Waiting for caches to sync for resource quota
I1211 04:48:50.869602       1 shared_informer.go:240] Waiting for caches to sync for bootstrap_signer
I1211 04:48:50.927916       1 shared_informer.go:247] Caches are synced for certificate-csrapproving 
I1211 04:48:50.932101       1 shared_informer.go:247] Caches are synced for namespace 
W1211 04:48:50.931413       1 actual_state_of_world.go:534] Failed to update statusUpdateNeeded field in actual state of world: Failed to set statusUpdateNeeded to needed true, because nodeName="minikube" does not exist
I1211 04:48:50.933425       1 shared_informer.go:247] Caches are synced for certificate-csrsigning-kubelet-client 
I1211 04:48:50.933566       1 shared_informer.go:247] Caches are synced for certificate-csrsigning-kube-apiserver-client 
I1211 04:48:50.933625       1 shared_informer.go:247] Caches are synced for certificate-csrsigning-kubelet-serving 
I1211 04:48:50.933950       1 shared_informer.go:247] Caches are synced for TTL 
I1211 04:48:50.934877       1 shared_informer.go:247] Caches are synced for certificate-csrsigning-legacy-unknown 
I1211 04:48:50.939851       1 shared_informer.go:247] Caches are synced for PV protection 
I1211 04:48:50.942181       1 shared_informer.go:247] Caches are synced for HPA 
I1211 04:48:50.942505       1 shared_informer.go:247] Caches are synced for TTL after finished 
I1211 04:48:50.943300       1 shared_informer.go:247] Caches are synced for endpoint_slice_mirroring 
I1211 04:48:50.942870       1 shared_informer.go:247] Caches are synced for taint 
I1211 04:48:50.944657       1 node_lifecycle_controller.go:1398] Initializing eviction metric for zone: 
I1211 04:48:50.945500       1 taint_manager.go:187] "Starting NoExecuteTaintManager"
W1211 04:48:50.946114       1 node_lifecycle_controller.go:1013] Missing timestamp for Node minikube. Assuming now as a timestamp.
I1211 04:48:50.946449       1 shared_informer.go:247] Caches are synced for ReplicationController 
I1211 04:48:50.946468       1 shared_informer.go:247] Caches are synced for expand 
I1211 04:48:50.946554       1 shared_informer.go:247] Caches are synced for endpoint 
I1211 04:48:50.946671       1 shared_informer.go:247] Caches are synced for ReplicaSet 
I1211 04:48:50.946584       1 shared_informer.go:247] Caches are synced for ClusterRoleAggregator 
I1211 04:48:50.948292       1 node_lifecycle_controller.go:1214] Controller detected that zone  is now in state Normal.
I1211 04:48:50.954040       1 shared_informer.go:247] Caches are synced for node 
I1211 04:48:50.954095       1 range_allocator.go:172] Starting range CIDR allocator
I1211 04:48:50.954107       1 shared_informer.go:240] Waiting for caches to sync for cidrallocator
I1211 04:48:50.954126       1 shared_informer.go:247] Caches are synced for cidrallocator 
I1211 04:48:51.011636       1 shared_informer.go:247] Caches are synced for disruption 
I1211 04:48:51.011867       1 disruption.go:371] Sending events to api server.
I1211 04:48:51.014854       1 shared_informer.go:247] Caches are synced for job 
I1211 04:48:51.015761       1 shared_informer.go:247] Caches are synced for crt configmap 
I1211 04:48:51.015796       1 shared_informer.go:247] Caches are synced for service account 
I1211 04:48:51.016121       1 shared_informer.go:247] Caches are synced for bootstrap_signer 
I1211 04:48:51.016216       1 event.go:291] "Event occurred" object="minikube" kind="Node" apiVersion="v1" type="Normal" reason="RegisteredNode" message="Node minikube event: Registered Node minikube in Controller"
I1211 04:48:51.021621       1 shared_informer.go:247] Caches are synced for cronjob 
I1211 04:48:51.031473       1 shared_informer.go:247] Caches are synced for ephemeral 
I1211 04:48:51.031541       1 shared_informer.go:247] Caches are synced for GC 
I1211 04:48:51.031796       1 shared_informer.go:247] Caches are synced for deployment 
I1211 04:48:51.032230       1 shared_informer.go:247] Caches are synced for persistent volume 
I1211 04:48:51.039383       1 shared_informer.go:247] Caches are synced for endpoint_slice 
I1211 04:48:51.041866       1 shared_informer.go:247] Caches are synced for PVC protection 
I1211 04:48:51.042702       1 shared_informer.go:240] Waiting for caches to sync for garbage collector
I1211 04:48:51.043145       1 shared_informer.go:247] Caches are synced for daemon sets 
I1211 04:48:51.123412       1 shared_informer.go:247] Caches are synced for stateful set 
I1211 04:48:51.139519       1 shared_informer.go:247] Caches are synced for attach detach 
I1211 04:48:51.170396       1 shared_informer.go:247] Caches are synced for resource quota 
I1211 04:48:51.221077       1 shared_informer.go:247] Caches are synced for resource quota 
I1211 04:48:51.623409       1 shared_informer.go:247] Caches are synced for garbage collector 
I1211 04:48:51.623488       1 garbagecollector.go:151] Garbage collector: all resource monitors have synced. Proceeding to collect garbage
I1211 04:48:51.642993       1 shared_informer.go:247] Caches are synced for garbage collector 
E1211 10:33:30.975255       1 resource_quota_controller.go:413] failed to discover resources: Unauthorized
W1211 10:33:31.115866       1 garbagecollector.go:705] failed to discover preferred resources: Unauthorized
W1211 15:48:23.463305       1 garbagecollector.go:705] failed to discover preferred resources: Unauthorized
E1211 15:48:23.463332       1 resource_quota_controller.go:413] failed to discover resources: Unauthorized

* 
* ==> kube-proxy [685c256f4a4c] <==
* I1211 04:48:43.117118       1 node.go:172] Successfully retrieved node IP: 192.168.49.2
I1211 04:48:43.117178       1 server_others.go:140] Detected node IP 192.168.49.2
W1211 04:48:43.117232       1 server_others.go:565] Unknown proxy mode "", assuming iptables proxy
I1211 04:48:43.426614       1 server_others.go:206] kube-proxy running in dual-stack mode, IPv4-primary
I1211 04:48:43.426677       1 server_others.go:212] Using iptables Proxier.
I1211 04:48:43.426703       1 server_others.go:219] creating dualStackProxier for iptables.
W1211 04:48:43.426743       1 server_others.go:495] detect-local-mode set to ClusterCIDR, but no IPv6 cluster CIDR defined, , defaulting to no-op detect-local for IPv6
I1211 04:48:43.432833       1 server.go:649] Version: v1.22.3
I1211 04:48:43.439688       1 config.go:224] Starting endpoint slice config controller
I1211 04:48:43.439736       1 shared_informer.go:240] Waiting for caches to sync for endpoint slice config
I1211 04:48:43.440664       1 config.go:315] Starting service config controller
I1211 04:48:43.440704       1 shared_informer.go:240] Waiting for caches to sync for service config
I1211 04:48:43.539816       1 shared_informer.go:247] Caches are synced for endpoint slice config 
I1211 04:48:43.543380       1 shared_informer.go:247] Caches are synced for service config 

* 
* ==> kube-proxy [f266c250cfba] <==
* I1213 04:17:55.125762       1 node.go:172] Successfully retrieved node IP: 192.168.49.2
I1213 04:17:55.125854       1 server_others.go:140] Detected node IP 192.168.49.2
W1213 04:17:55.125901       1 server_others.go:565] Unknown proxy mode "", assuming iptables proxy
I1213 04:17:55.322076       1 server_others.go:206] kube-proxy running in dual-stack mode, IPv4-primary
I1213 04:17:55.322113       1 server_others.go:212] Using iptables Proxier.
I1213 04:17:55.322122       1 server_others.go:219] creating dualStackProxier for iptables.
W1213 04:17:55.322129       1 server_others.go:495] detect-local-mode set to ClusterCIDR, but no IPv6 cluster CIDR defined, , defaulting to no-op detect-local for IPv6
I1213 04:17:55.322856       1 server.go:649] Version: v1.22.3
I1213 04:17:55.325051       1 config.go:315] Starting service config controller
I1213 04:17:55.325090       1 shared_informer.go:240] Waiting for caches to sync for service config
I1213 04:17:55.325562       1 config.go:224] Starting endpoint slice config controller
I1213 04:17:55.325686       1 shared_informer.go:240] Waiting for caches to sync for endpoint slice config
I1213 04:17:55.425397       1 shared_informer.go:247] Caches are synced for service config 
I1213 04:17:55.426009       1 shared_informer.go:247] Caches are synced for endpoint slice config 

* 
* ==> kube-scheduler [c3d76ba5da76] <==
* I1211 04:48:33.143936       1 serving.go:347] Generated self-signed cert in-memory
W1211 04:48:37.429384       1 requestheader_controller.go:193] Unable to get configmap/extension-apiserver-authentication in kube-system.  Usually fixed by 'kubectl create rolebinding -n kube-system ROLEBINDING_NAME --role=extension-apiserver-authentication-reader --serviceaccount=YOUR_NS:YOUR_SA'
W1211 04:48:37.432305       1 authentication.go:345] Error looking up in-cluster authentication configuration: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot get resource "configmaps" in API group "" in the namespace "kube-system"
W1211 04:48:37.432381       1 authentication.go:346] Continuing without authentication configuration. This may treat all requests as anonymous.
W1211 04:48:37.432406       1 authentication.go:347] To require authentication configuration lookup to succeed, set --authentication-tolerate-lookup-failure=false
I1211 04:48:37.610630       1 secure_serving.go:200] Serving securely on 127.0.0.1:10259
I1211 04:48:37.614743       1 configmap_cafile_content.go:201] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I1211 04:48:37.614794       1 shared_informer.go:240] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I1211 04:48:37.614846       1 tlsconfig.go:240] "Starting DynamicServingCertificateController"
I1211 04:48:37.715131       1 shared_informer.go:247] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file 

* 
* ==> kube-scheduler [f6976956c4e4] <==
* I1213 04:17:47.933096       1 serving.go:347] Generated self-signed cert in-memory
I1213 04:17:50.909556       1 secure_serving.go:200] Serving securely on 127.0.0.1:10259
I1213 04:17:50.909828       1 configmap_cafile_content.go:201] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I1213 04:17:50.909874       1 tlsconfig.go:240] "Starting DynamicServingCertificateController"
I1213 04:17:50.910288       1 configmap_cafile_content.go:201] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file"
I1213 04:17:50.910616       1 shared_informer.go:240] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I1213 04:17:50.910626       1 shared_informer.go:240] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file
I1213 04:17:50.910741       1 requestheader_controller.go:169] Starting RequestHeaderAuthRequestController
I1213 04:17:50.910756       1 shared_informer.go:240] Waiting for caches to sync for RequestHeaderAuthRequestController
I1213 04:17:51.011100       1 shared_informer.go:247] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file 
I1213 04:17:51.011243       1 shared_informer.go:247] Caches are synced for RequestHeaderAuthRequestController 
I1213 04:17:51.011526       1 shared_informer.go:247] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file 

* 
* ==> kubelet <==
* -- Logs begin at Mon 2021-12-13 04:17:31 UTC, end at Mon 2021-12-13 20:49:33 UTC. --
Dec 13 20:47:40 minikube kubelet[1053]: I1213 20:47:40.800352    1053 reconciler.go:319] "Volume detached for volume \"kube-api-access-6p9ww\" (UniqueName: \"kubernetes.io/projected/ee479966-be8a-4fc3-bf36-3f8f78bae620-kube-api-access-6p9ww\") on node \"minikube\" DevicePath \"\""
Dec 13 20:47:40 minikube kubelet[1053]: I1213 20:47:40.800381    1053 reconciler.go:319] "Volume detached for volume \"kube-api-access-mzj2x\" (UniqueName: \"kubernetes.io/projected/9c723004-4347-402d-9e7a-54f0fdc9098d-kube-api-access-mzj2x\") on node \"minikube\" DevicePath \"\""
Dec 13 20:47:40 minikube kubelet[1053]: I1213 20:47:40.800402    1053 reconciler.go:319] "Volume detached for volume \"kube-api-access-skp8v\" (UniqueName: \"kubernetes.io/projected/58d741a8-fe2a-4461-8931-01f7628a7965-kube-api-access-skp8v\") on node \"minikube\" DevicePath \"\""
Dec 13 20:47:41 minikube kubelet[1053]: I1213 20:47:41.733509    1053 scope.go:110] "RemoveContainer" containerID="5a22a05fd17ea7bf568c420cc2a986ac869897fc97e4c192787b1b3989704603"
Dec 13 20:47:41 minikube kubelet[1053]: I1213 20:47:41.759369    1053 scope.go:110] "RemoveContainer" containerID="3ca021eac49ffda642e7f0f8ae009d9b2c75e0177816e7e83dd54e11bd498e14"
Dec 13 20:47:42 minikube kubelet[1053]: I1213 20:47:42.440630    1053 kubelet_volumes.go:160] "Cleaned up orphaned pod volumes dir" podUID=046c2ebe-514b-448e-9a2c-8cc4f3186c89 path="/var/lib/kubelet/pods/046c2ebe-514b-448e-9a2c-8cc4f3186c89/volumes"
Dec 13 20:47:42 minikube kubelet[1053]: I1213 20:47:42.441049    1053 kubelet_volumes.go:160] "Cleaned up orphaned pod volumes dir" podUID=58d741a8-fe2a-4461-8931-01f7628a7965 path="/var/lib/kubelet/pods/58d741a8-fe2a-4461-8931-01f7628a7965/volumes"
Dec 13 20:47:42 minikube kubelet[1053]: I1213 20:47:42.441450    1053 kubelet_volumes.go:160] "Cleaned up orphaned pod volumes dir" podUID=9c723004-4347-402d-9e7a-54f0fdc9098d path="/var/lib/kubelet/pods/9c723004-4347-402d-9e7a-54f0fdc9098d/volumes"
Dec 13 20:47:42 minikube kubelet[1053]: I1213 20:47:42.441815    1053 kubelet_volumes.go:160] "Cleaned up orphaned pod volumes dir" podUID=ee479966-be8a-4fc3-bf36-3f8f78bae620 path="/var/lib/kubelet/pods/ee479966-be8a-4fc3-bf36-3f8f78bae620/volumes"
Dec 13 20:47:43 minikube kubelet[1053]: I1213 20:47:43.967076    1053 topology_manager.go:200] "Topology Admit Handler"
Dec 13 20:47:43 minikube kubelet[1053]: I1213 20:47:43.998063    1053 topology_manager.go:200] "Topology Admit Handler"
Dec 13 20:47:44 minikube kubelet[1053]: I1213 20:47:44.099255    1053 topology_manager.go:200] "Topology Admit Handler"
Dec 13 20:47:44 minikube kubelet[1053]: I1213 20:47:44.110285    1053 topology_manager.go:200] "Topology Admit Handler"
Dec 13 20:47:44 minikube kubelet[1053]: I1213 20:47:44.124791    1053 reconciler.go:224] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-hvwjq\" (UniqueName: \"kubernetes.io/projected/4181bd96-1f81-4b24-83d8-ba3e41378ee2-kube-api-access-hvwjq\") pod \"ordering-app-deployment-8b649d968-cwj29\" (UID: \"4181bd96-1f81-4b24-83d8-ba3e41378ee2\") "
Dec 13 20:47:44 minikube kubelet[1053]: I1213 20:47:44.124986    1053 reconciler.go:224] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-zd5dk\" (UniqueName: \"kubernetes.io/projected/a2c85642-3a26-41e5-9a86-aa1ec0f28f16-kube-api-access-zd5dk\") pod \"ordering-app-deployment-8b649d968-chv45\" (UID: \"a2c85642-3a26-41e5-9a86-aa1ec0f28f16\") "
Dec 13 20:47:44 minikube kubelet[1053]: I1213 20:47:44.225447    1053 reconciler.go:224] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-2mr4s\" (UniqueName: \"kubernetes.io/projected/f5385ac5-9abc-4254-873f-80eac7becd18-kube-api-access-2mr4s\") pod \"dispatch-app-deployment-7db8d9494d-hr7gj\" (UID: \"f5385ac5-9abc-4254-873f-80eac7becd18\") "
Dec 13 20:47:44 minikube kubelet[1053]: I1213 20:47:44.225536    1053 reconciler.go:224] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-sw4st\" (UniqueName: \"kubernetes.io/projected/16204863-c2ea-4363-afe3-374c9b61d584-kube-api-access-sw4st\") pod \"dispatch-app-deployment-7db8d9494d-crznk\" (UID: \"16204863-c2ea-4363-afe3-374c9b61d584\") "
Dec 13 20:47:45 minikube kubelet[1053]: I1213 20:47:45.228161    1053 docker_sandbox.go:401] "Failed to read pod IP from plugin/docker" err="Couldn't find network status for default/dispatch-app-deployment-7db8d9494d-hr7gj through plugin: invalid network status for"
Dec 13 20:47:45 minikube kubelet[1053]: I1213 20:47:45.315132    1053 docker_sandbox.go:401] "Failed to read pod IP from plugin/docker" err="Couldn't find network status for default/ordering-app-deployment-8b649d968-chv45 through plugin: invalid network status for"
Dec 13 20:47:45 minikube kubelet[1053]: I1213 20:47:45.320200    1053 docker_sandbox.go:401] "Failed to read pod IP from plugin/docker" err="Couldn't find network status for default/ordering-app-deployment-8b649d968-chv45 through plugin: invalid network status for"
Dec 13 20:47:45 minikube kubelet[1053]: I1213 20:47:45.325570    1053 pod_container_deletor.go:79] "Container not found in pod's containers" containerID="367830a572ea2631899fea10d4d5a6df3d2f5b257355a162d1022687edd3ef82"
Dec 13 20:47:45 minikube kubelet[1053]: I1213 20:47:45.403270    1053 docker_sandbox.go:401] "Failed to read pod IP from plugin/docker" err="Couldn't find network status for default/dispatch-app-deployment-7db8d9494d-hr7gj through plugin: invalid network status for"
Dec 13 20:47:45 minikube kubelet[1053]: I1213 20:47:45.408132    1053 pod_container_deletor.go:79] "Container not found in pod's containers" containerID="0840fc01c0d5dbb0ec9a2b01ec74d001fdd04bf752567400aca2e71eda975bfd"
Dec 13 20:47:45 minikube kubelet[1053]: I1213 20:47:45.524252    1053 docker_sandbox.go:401] "Failed to read pod IP from plugin/docker" err="Couldn't find network status for default/ordering-app-deployment-8b649d968-cwj29 through plugin: invalid network status for"
Dec 13 20:47:45 minikube kubelet[1053]: I1213 20:47:45.715131    1053 pod_container_deletor.go:79] "Container not found in pod's containers" containerID="1536ee5fe38bb720a0b6004216aaa7487bf201c54640a78ab5ca0e8999dbde9d"
Dec 13 20:47:45 minikube kubelet[1053]: I1213 20:47:45.715633    1053 docker_sandbox.go:401] "Failed to read pod IP from plugin/docker" err="Couldn't find network status for default/dispatch-app-deployment-7db8d9494d-crznk through plugin: invalid network status for"
Dec 13 20:47:45 minikube kubelet[1053]: I1213 20:47:45.718788    1053 docker_sandbox.go:401] "Failed to read pod IP from plugin/docker" err="Couldn't find network status for default/ordering-app-deployment-8b649d968-cwj29 through plugin: invalid network status for"
Dec 13 20:47:46 minikube kubelet[1053]: I1213 20:47:46.107010    1053 pod_container_deletor.go:79] "Container not found in pod's containers" containerID="e2229818f8a4b17f0b8cb12f9dbeec1674e9b235ce7bf2f39db4287d7c1b4dd3"
Dec 13 20:47:46 minikube kubelet[1053]: E1213 20:47:46.333938    1053 cadvisor_stats_provider.go:147] "Unable to fetch pod log stats" err="open /var/log/pods/default_dispatch-app-deployment-5fd7f7cb68-6tnsx_58d741a8-fe2a-4461-8931-01f7628a7965: no such file or directory" pod="default/dispatch-app-deployment-5fd7f7cb68-6tnsx"
Dec 13 20:47:46 minikube kubelet[1053]: E1213 20:47:46.334055    1053 cadvisor_stats_provider.go:147] "Unable to fetch pod log stats" err="open /var/log/pods/default_dispatch-app-deployment-5fd7f7cb68-bstk5_9c723004-4347-402d-9e7a-54f0fdc9098d: no such file or directory" pod="default/dispatch-app-deployment-5fd7f7cb68-bstk5"
Dec 13 20:47:46 minikube kubelet[1053]: E1213 20:47:46.334178    1053 cadvisor_stats_provider.go:147] "Unable to fetch pod log stats" err="open /var/log/pods/default_ordering-app-deployment-6cf8b57d6f-2tq6t_046c2ebe-514b-448e-9a2c-8cc4f3186c89: no such file or directory" pod="default/ordering-app-deployment-6cf8b57d6f-2tq6t"
Dec 13 20:47:46 minikube kubelet[1053]: E1213 20:47:46.334363    1053 cadvisor_stats_provider.go:147] "Unable to fetch pod log stats" err="open /var/log/pods/default_ordering-app-deployment-6cf8b57d6f-7trhv_ee479966-be8a-4fc3-bf36-3f8f78bae620: no such file or directory" pod="default/ordering-app-deployment-6cf8b57d6f-7trhv"
Dec 13 20:47:47 minikube kubelet[1053]: I1213 20:47:47.125677    1053 docker_sandbox.go:401] "Failed to read pod IP from plugin/docker" err="Couldn't find network status for default/ordering-app-deployment-8b649d968-cwj29 through plugin: invalid network status for"
Dec 13 20:47:47 minikube kubelet[1053]: I1213 20:47:47.205809    1053 docker_sandbox.go:401] "Failed to read pod IP from plugin/docker" err="Couldn't find network status for default/ordering-app-deployment-8b649d968-chv45 through plugin: invalid network status for"
Dec 13 20:47:47 minikube kubelet[1053]: I1213 20:47:47.291271    1053 docker_sandbox.go:401] "Failed to read pod IP from plugin/docker" err="Couldn't find network status for default/dispatch-app-deployment-7db8d9494d-hr7gj through plugin: invalid network status for"
Dec 13 20:47:47 minikube kubelet[1053]: I1213 20:47:47.314664    1053 docker_sandbox.go:401] "Failed to read pod IP from plugin/docker" err="Couldn't find network status for default/dispatch-app-deployment-7db8d9494d-crznk through plugin: invalid network status for"
Dec 13 20:47:56 minikube kubelet[1053]: E1213 20:47:56.823080    1053 cadvisor_stats_provider.go:147] "Unable to fetch pod log stats" err="open /var/log/pods/default_dispatch-app-deployment-5fd7f7cb68-6tnsx_58d741a8-fe2a-4461-8931-01f7628a7965: no such file or directory" pod="default/dispatch-app-deployment-5fd7f7cb68-6tnsx"
Dec 13 20:47:56 minikube kubelet[1053]: E1213 20:47:56.828274    1053 cadvisor_stats_provider.go:147] "Unable to fetch pod log stats" err="open /var/log/pods/default_dispatch-app-deployment-5fd7f7cb68-bstk5_9c723004-4347-402d-9e7a-54f0fdc9098d: no such file or directory" pod="default/dispatch-app-deployment-5fd7f7cb68-bstk5"
Dec 13 20:47:56 minikube kubelet[1053]: E1213 20:47:56.828501    1053 cadvisor_stats_provider.go:147] "Unable to fetch pod log stats" err="open /var/log/pods/default_ordering-app-deployment-6cf8b57d6f-2tq6t_046c2ebe-514b-448e-9a2c-8cc4f3186c89: no such file or directory" pod="default/ordering-app-deployment-6cf8b57d6f-2tq6t"
Dec 13 20:47:56 minikube kubelet[1053]: E1213 20:47:56.829018    1053 cadvisor_stats_provider.go:147] "Unable to fetch pod log stats" err="open /var/log/pods/default_ordering-app-deployment-6cf8b57d6f-7trhv_ee479966-be8a-4fc3-bf36-3f8f78bae620: no such file or directory" pod="default/ordering-app-deployment-6cf8b57d6f-7trhv"
Dec 13 20:48:06 minikube kubelet[1053]: E1213 20:48:06.837303    1053 cadvisor_stats_provider.go:147] "Unable to fetch pod log stats" err="open /var/log/pods/default_ordering-app-deployment-6cf8b57d6f-2tq6t_046c2ebe-514b-448e-9a2c-8cc4f3186c89: no such file or directory" pod="default/ordering-app-deployment-6cf8b57d6f-2tq6t"
Dec 13 20:48:06 minikube kubelet[1053]: E1213 20:48:06.837385    1053 cadvisor_stats_provider.go:147] "Unable to fetch pod log stats" err="open /var/log/pods/default_dispatch-app-deployment-5fd7f7cb68-bstk5_9c723004-4347-402d-9e7a-54f0fdc9098d: no such file or directory" pod="default/dispatch-app-deployment-5fd7f7cb68-bstk5"
Dec 13 20:48:06 minikube kubelet[1053]: E1213 20:48:06.837839    1053 cadvisor_stats_provider.go:147] "Unable to fetch pod log stats" err="open /var/log/pods/default_dispatch-app-deployment-5fd7f7cb68-6tnsx_58d741a8-fe2a-4461-8931-01f7628a7965: no such file or directory" pod="default/dispatch-app-deployment-5fd7f7cb68-6tnsx"
Dec 13 20:48:06 minikube kubelet[1053]: E1213 20:48:06.837887    1053 cadvisor_stats_provider.go:147] "Unable to fetch pod log stats" err="open /var/log/pods/default_ordering-app-deployment-6cf8b57d6f-7trhv_ee479966-be8a-4fc3-bf36-3f8f78bae620: no such file or directory" pod="default/ordering-app-deployment-6cf8b57d6f-7trhv"
Dec 13 20:48:16 minikube kubelet[1053]: E1213 20:48:16.875087    1053 cadvisor_stats_provider.go:147] "Unable to fetch pod log stats" err="open /var/log/pods/default_ordering-app-deployment-6cf8b57d6f-7trhv_ee479966-be8a-4fc3-bf36-3f8f78bae620: no such file or directory" pod="default/ordering-app-deployment-6cf8b57d6f-7trhv"
Dec 13 20:48:16 minikube kubelet[1053]: E1213 20:48:16.875240    1053 cadvisor_stats_provider.go:147] "Unable to fetch pod log stats" err="open /var/log/pods/default_dispatch-app-deployment-5fd7f7cb68-bstk5_9c723004-4347-402d-9e7a-54f0fdc9098d: no such file or directory" pod="default/dispatch-app-deployment-5fd7f7cb68-bstk5"
Dec 13 20:48:16 minikube kubelet[1053]: E1213 20:48:16.875281    1053 cadvisor_stats_provider.go:147] "Unable to fetch pod log stats" err="open /var/log/pods/default_dispatch-app-deployment-5fd7f7cb68-6tnsx_58d741a8-fe2a-4461-8931-01f7628a7965: no such file or directory" pod="default/dispatch-app-deployment-5fd7f7cb68-6tnsx"
Dec 13 20:48:16 minikube kubelet[1053]: E1213 20:48:16.875322    1053 cadvisor_stats_provider.go:147] "Unable to fetch pod log stats" err="open /var/log/pods/default_ordering-app-deployment-6cf8b57d6f-2tq6t_046c2ebe-514b-448e-9a2c-8cc4f3186c89: no such file or directory" pod="default/ordering-app-deployment-6cf8b57d6f-2tq6t"
Dec 13 20:48:26 minikube kubelet[1053]: E1213 20:48:26.918926    1053 cadvisor_stats_provider.go:147] "Unable to fetch pod log stats" err="open /var/log/pods/default_dispatch-app-deployment-5fd7f7cb68-6tnsx_58d741a8-fe2a-4461-8931-01f7628a7965: no such file or directory" pod="default/dispatch-app-deployment-5fd7f7cb68-6tnsx"
Dec 13 20:48:26 minikube kubelet[1053]: E1213 20:48:26.919005    1053 cadvisor_stats_provider.go:147] "Unable to fetch pod log stats" err="open /var/log/pods/default_ordering-app-deployment-6cf8b57d6f-2tq6t_046c2ebe-514b-448e-9a2c-8cc4f3186c89: no such file or directory" pod="default/ordering-app-deployment-6cf8b57d6f-2tq6t"
Dec 13 20:48:26 minikube kubelet[1053]: E1213 20:48:26.919146    1053 cadvisor_stats_provider.go:147] "Unable to fetch pod log stats" err="open /var/log/pods/default_dispatch-app-deployment-5fd7f7cb68-bstk5_9c723004-4347-402d-9e7a-54f0fdc9098d: no such file or directory" pod="default/dispatch-app-deployment-5fd7f7cb68-bstk5"
Dec 13 20:48:26 minikube kubelet[1053]: E1213 20:48:26.919559    1053 cadvisor_stats_provider.go:147] "Unable to fetch pod log stats" err="open /var/log/pods/default_ordering-app-deployment-6cf8b57d6f-7trhv_ee479966-be8a-4fc3-bf36-3f8f78bae620: no such file or directory" pod="default/ordering-app-deployment-6cf8b57d6f-7trhv"
Dec 13 20:48:34 minikube kubelet[1053]: E1213 20:48:34.308225    1053 fsHandler.go:114] failed to collect filesystem stats - rootDiskErr: could not stat "/var/lib/docker/overlay2/82540ea1e1d6034464cb8fb36e5b056c2af9062334bb14a491fed443ade74ea2/diff" to get inode usage: stat /var/lib/docker/overlay2/82540ea1e1d6034464cb8fb36e5b056c2af9062334bb14a491fed443ade74ea2/diff: no such file or directory, extraDiskErr: could not stat "/var/lib/docker/containers/5a22a05fd17ea7bf568c420cc2a986ac869897fc97e4c192787b1b3989704603" to get inode usage: stat /var/lib/docker/containers/5a22a05fd17ea7bf568c420cc2a986ac869897fc97e4c192787b1b3989704603: no such file or directory
Dec 13 20:48:34 minikube kubelet[1053]: E1213 20:48:34.308562    1053 fsHandler.go:114] failed to collect filesystem stats - rootDiskErr: could not stat "/var/lib/docker/overlay2/35497b0ae699397d8631dc5d02dd53c24a214381eb097f0855c7d1536418eade/diff" to get inode usage: stat /var/lib/docker/overlay2/35497b0ae699397d8631dc5d02dd53c24a214381eb097f0855c7d1536418eade/diff: no such file or directory, extraDiskErr: could not stat "/var/lib/docker/containers/3ca021eac49ffda642e7f0f8ae009d9b2c75e0177816e7e83dd54e11bd498e14" to get inode usage: stat /var/lib/docker/containers/3ca021eac49ffda642e7f0f8ae009d9b2c75e0177816e7e83dd54e11bd498e14: no such file or directory
Dec 13 20:48:34 minikube kubelet[1053]: E1213 20:48:34.312856    1053 fsHandler.go:114] failed to collect filesystem stats - rootDiskErr: could not stat "/var/lib/docker/overlay2/a4b5ea78a7cf3ef117ddfe2a01d7c19adc2397ecbc9abc6cb35d6bf2fd11792a/diff" to get inode usage: stat /var/lib/docker/overlay2/a4b5ea78a7cf3ef117ddfe2a01d7c19adc2397ecbc9abc6cb35d6bf2fd11792a/diff: no such file or directory, extraDiskErr: could not stat "/var/lib/docker/containers/36a334f57fd1c499f78d89d1a3bbbba00f89f7126609c63e919f7656c5299944" to get inode usage: stat /var/lib/docker/containers/36a334f57fd1c499f78d89d1a3bbbba00f89f7126609c63e919f7656c5299944: no such file or directory
Dec 13 20:48:34 minikube kubelet[1053]: E1213 20:48:34.313538    1053 fsHandler.go:114] failed to collect filesystem stats - rootDiskErr: could not stat "/var/lib/docker/overlay2/65347800920e688931f4e93e6731dda1488ffc79020e8366fdfb7da00b5c4dce/diff" to get inode usage: stat /var/lib/docker/overlay2/65347800920e688931f4e93e6731dda1488ffc79020e8366fdfb7da00b5c4dce/diff: no such file or directory, extraDiskErr: could not stat "/var/lib/docker/containers/5ac4d792efeed31e6418c70bfc8ac09923268b1f252e52b770da9af83d6e44f1" to get inode usage: stat /var/lib/docker/containers/5ac4d792efeed31e6418c70bfc8ac09923268b1f252e52b770da9af83d6e44f1: no such file or directory
Dec 13 20:48:34 minikube kubelet[1053]: E1213 20:48:34.320634    1053 fsHandler.go:114] failed to collect filesystem stats - rootDiskErr: could not stat "/var/lib/docker/overlay2/7574d018f290990148e8f03e765ce53c2f313ca1f9e5fe607870f88fb3e9902f/diff" to get inode usage: stat /var/lib/docker/overlay2/7574d018f290990148e8f03e765ce53c2f313ca1f9e5fe607870f88fb3e9902f/diff: no such file or directory, extraDiskErr: could not stat "/var/lib/docker/containers/faf498aac96f5a5507af9d52b3d31402dbc04d820c0b7abaeb2e859ddf5414a8" to get inode usage: stat /var/lib/docker/containers/faf498aac96f5a5507af9d52b3d31402dbc04d820c0b7abaeb2e859ddf5414a8: no such file or directory
Dec 13 20:48:34 minikube kubelet[1053]: E1213 20:48:34.327810    1053 fsHandler.go:114] failed to collect filesystem stats - rootDiskErr: could not stat "/var/lib/docker/overlay2/2c7026dbecde6d9d2d4418326295debccba61f50d1fad5ae3a67af7cd220072f/diff" to get inode usage: stat /var/lib/docker/overlay2/2c7026dbecde6d9d2d4418326295debccba61f50d1fad5ae3a67af7cd220072f/diff: no such file or directory, extraDiskErr: could not stat "/var/lib/docker/containers/4f38dc38e9dfd3a2b5bef8ed1e97f57fb00cfbed9585247ae3ab3728e90f6a82" to get inode usage: stat /var/lib/docker/containers/4f38dc38e9dfd3a2b5bef8ed1e97f57fb00cfbed9585247ae3ab3728e90f6a82: no such file or directory
Dec 13 20:48:34 minikube kubelet[1053]: E1213 20:48:34.329553    1053 fsHandler.go:114] failed to collect filesystem stats - rootDiskErr: could not stat "/var/lib/docker/overlay2/a5091c264742436c6ac17e80e9bbb635fc9ef0fb9701be291813c55f49c6831f/diff" to get inode usage: stat /var/lib/docker/overlay2/a5091c264742436c6ac17e80e9bbb635fc9ef0fb9701be291813c55f49c6831f/diff: no such file or directory, extraDiskErr: could not stat "/var/lib/docker/containers/ade6542813efb5932b5ebe622e0b29e159c3e56fd03888d1bbe2358010ae9e8e" to get inode usage: stat /var/lib/docker/containers/ade6542813efb5932b5ebe622e0b29e159c3e56fd03888d1bbe2358010ae9e8e: no such file or directory
Dec 13 20:48:34 minikube kubelet[1053]: E1213 20:48:34.331865    1053 fsHandler.go:114] failed to collect filesystem stats - rootDiskErr: could not stat "/var/lib/docker/overlay2/e25e05cb1cc2abe25b1fda5de3bb987b359dbef6e0c6e70661c52e6e3dc04fd6/diff" to get inode usage: stat /var/lib/docker/overlay2/e25e05cb1cc2abe25b1fda5de3bb987b359dbef6e0c6e70661c52e6e3dc04fd6/diff: no such file or directory, extraDiskErr: could not stat "/var/lib/docker/containers/56370076ef31a55c5a1a511a5141699cc7a0a89a0f7203e4596a48804708dcc9" to get inode usage: stat /var/lib/docker/containers/56370076ef31a55c5a1a511a5141699cc7a0a89a0f7203e4596a48804708dcc9: no such file or directory

* 
* ==> kubernetes-dashboard [8b8e280b404e] <==
* 2021/12/13 20:49:29 Skipping metric because of error: Metric label not set.
2021/12/13 20:49:29 Skipping metric because of error: Metric label not set.
2021/12/13 20:49:29 Skipping metric because of error: Metric label not set.
2021/12/13 20:49:29 Skipping metric because of error: Metric label not set.
2021/12/13 20:49:29 Skipping metric because of error: Metric label not set.
2021/12/13 20:49:29 Skipping metric because of error: Metric label not set.
2021/12/13 20:49:29 Skipping metric because of error: Metric label not set.
2021/12/13 20:49:29 Skipping metric because of error: Metric label not set.
2021/12/13 20:49:29 Skipping metric because of error: Metric label not set.
2021/12/13 20:49:29 Skipping metric because of error: Metric label not set.
2021/12/13 20:49:29 Skipping metric because of error: Metric label not set.
2021/12/13 20:49:29 Skipping metric because of error: Metric label not set.
2021/12/13 20:49:29 Skipping metric because of error: Metric label not set.
2021/12/13 20:49:29 Skipping metric because of error: Metric label not set.
2021/12/13 20:49:29 [2021-12-13T20:49:29Z] Outcoming response to 127.0.0.1 with 200 status code
2021/12/13 20:49:29 received 0 resources from sidecar instead of 2
2021/12/13 20:49:29 received 0 resources from sidecar instead of 7
2021/12/13 20:49:29 received 0 resources from sidecar instead of 5
2021/12/13 20:49:29 received 0 resources from sidecar instead of 2
2021/12/13 20:49:29 received 0 resources from sidecar instead of 5
2021/12/13 20:49:29 received 0 resources from sidecar instead of 7
2021/12/13 20:49:29 Getting pod metrics
2021/12/13 20:49:29 received 0 resources from sidecar instead of 7
2021/12/13 20:49:29 received 0 resources from sidecar instead of 5
2021/12/13 20:49:29 received 0 resources from sidecar instead of 2
2021/12/13 20:49:29 received 0 resources from sidecar instead of 5
2021/12/13 20:49:29 received 0 resources from sidecar instead of 7
2021/12/13 20:49:29 received 0 resources from sidecar instead of 2
2021/12/13 20:49:29 Skipping metric because of error: Metric label not set.
2021/12/13 20:49:29 Skipping metric because of error: Metric label not set.
2021/12/13 20:49:29 Skipping metric because of error: Metric label not set.
2021/12/13 20:49:29 Skipping metric because of error: Metric label not set.
2021/12/13 20:49:29 Skipping metric because of error: Metric label not set.
2021/12/13 20:49:29 Skipping metric because of error: Metric label not set.
2021/12/13 20:49:29 Skipping metric because of error: Metric label not set.
2021/12/13 20:49:29 Skipping metric because of error: Metric label not set.
2021/12/13 20:49:29 Skipping metric because of error: Metric label not set.
2021/12/13 20:49:29 Skipping metric because of error: Metric label not set.
2021/12/13 20:49:29 Skipping metric because of error: Metric label not set.
2021/12/13 20:49:29 Skipping metric because of error: Metric label not set.
2021/12/13 20:49:29 Skipping metric because of error: Metric label not set.
2021/12/13 20:49:29 Skipping metric because of error: Metric label not set.
2021/12/13 20:49:29 Skipping metric because of error: Metric label not set.
2021/12/13 20:49:29 Skipping metric because of error: Metric label not set.
2021/12/13 20:49:29 Skipping metric because of error: Metric label not set.
2021/12/13 20:49:29 Skipping metric because of error: Metric label not set.
2021/12/13 20:49:29 Skipping metric because of error: Metric label not set.
2021/12/13 20:49:29 Skipping metric because of error: Metric label not set.
2021/12/13 20:49:29 Skipping metric because of error: Metric label not set.
2021/12/13 20:49:29 Skipping metric because of error: Metric label not set.
2021/12/13 20:49:29 Skipping metric because of error: Metric label not set.
2021/12/13 20:49:29 Skipping metric because of error: Metric label not set.
2021/12/13 20:49:29 Skipping metric because of error: Metric label not set.
2021/12/13 20:49:29 Skipping metric because of error: Metric label not set.
2021/12/13 20:49:29 Skipping metric because of error: Metric label not set.
2021/12/13 20:49:29 Skipping metric because of error: Metric label not set.
2021/12/13 20:49:29 [2021-12-13T20:49:29Z] Outcoming response to 127.0.0.1 with 200 status code
2021/12/13 20:49:32 [2021-12-13T20:49:32Z] Incoming HTTP/1.1 GET /api/v1/namespace request from 127.0.0.1: 
2021/12/13 20:49:32 Getting list of namespaces
2021/12/13 20:49:32 [2021-12-13T20:49:32Z] Outcoming response to 127.0.0.1 with 200 status code

* 
* ==> storage-provisioner [e36baf943382] <==
* I1213 15:41:28.040779       1 storage_provisioner.go:98] Deleting volume &PersistentVolume{ObjectMeta:{pvc-af289560-4ccc-4a25-8cfc-d27bf109886e    e96bec5a-95cc-4e1c-a690-15b1a1763c9a 40537 0 2021-12-13 15:40:33 +0000 UTC <nil> <nil> map[] map[hostPathProvisionerIdentity:d73bd919-49a2-4fcc-b0e4-a8529c45f82f pv.kubernetes.io/provisioned-by:k8s.io/minikube-hostpath] [] [kubernetes.io/pv-protection]  [{kube-controller-manager Update v1 2021-12-13 15:40:33 +0000 UTC FieldsV1 {"f:status":{"f:phase":{}}}} {storage-provisioner Update v1 2021-12-13 15:40:33 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:hostPathProvisionerIdentity":{},"f:pv.kubernetes.io/provisioned-by":{}}},"f:spec":{"f:accessModes":{},"f:capacity":{".":{},"f:storage":{}},"f:claimRef":{},"f:hostPath":{".":{},"f:path":{},"f:type":{}},"f:persistentVolumeReclaimPolicy":{},"f:storageClassName":{},"f:volumeMode":{}}}}]},Spec:PersistentVolumeSpec{Capacity:ResourceList{storage: {{10737418240 0} {<nil>} 10Gi BinarySI},},PersistentVolumeSource:PersistentVolumeSource{GCEPersistentDisk:nil,AWSElasticBlockStore:nil,HostPath:&HostPathVolumeSource{Path:/tmp/hostpath-provisioner/default/persistence-hello-world-server-0,Type:*,},Glusterfs:nil,NFS:nil,RBD:nil,ISCSI:nil,Cinder:nil,CephFS:nil,FC:nil,Flocker:nil,FlexVolume:nil,AzureFile:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Local:nil,StorageOS:nil,CSI:nil,},AccessModes:[ReadWriteOnce],ClaimRef:&ObjectReference{Kind:PersistentVolumeClaim,Namespace:default,Name:persistence-hello-world-server-0,UID:af289560-4ccc-4a25-8cfc-d27bf109886e,APIVersion:v1,ResourceVersion:40351,FieldPath:,},PersistentVolumeReclaimPolicy:Delete,StorageClassName:standard,MountOptions:[],VolumeMode:*Filesystem,NodeAffinity:nil,},Status:PersistentVolumeStatus{Phase:Released,Message:,Reason:,},}
I1213 15:41:28.041338       1 controller.go:1487] delete "pvc-af289560-4ccc-4a25-8cfc-d27bf109886e": volume deleted
I1213 15:41:28.047227       1 controller.go:1537] delete "pvc-af289560-4ccc-4a25-8cfc-d27bf109886e": persistentvolume deleted
I1213 15:41:28.047273       1 controller.go:1542] delete "pvc-af289560-4ccc-4a25-8cfc-d27bf109886e": succeeded
I1213 15:46:15.869159       1 controller.go:1332] provision "default/persistence-rabbitmqcluster-server-0" class "standard": started
I1213 15:46:15.874312       1 storage_provisioner.go:61] Provisioning volume {&StorageClass{ObjectMeta:{standard    8bf73280-609b-4939-a151-ec863f9c159d 368 0 2021-12-10 14:03:30 +0000 UTC <nil> <nil> map[addonmanager.kubernetes.io/mode:EnsureExists] map[kubectl.kubernetes.io/last-applied-configuration:{"apiVersion":"storage.k8s.io/v1","kind":"StorageClass","metadata":{"annotations":{"storageclass.kubernetes.io/is-default-class":"true"},"labels":{"addonmanager.kubernetes.io/mode":"EnsureExists"},"name":"standard"},"provisioner":"k8s.io/minikube-hostpath"}
 storageclass.kubernetes.io/is-default-class:true] [] []  [{kubectl-client-side-apply Update storage.k8s.io/v1 2021-12-10 14:03:30 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:kubectl.kubernetes.io/last-applied-configuration":{},"f:storageclass.kubernetes.io/is-default-class":{}},"f:labels":{".":{},"f:addonmanager.kubernetes.io/mode":{}}},"f:provisioner":{},"f:reclaimPolicy":{},"f:volumeBindingMode":{}}}]},Provisioner:k8s.io/minikube-hostpath,Parameters:map[string]string{},ReclaimPolicy:*Delete,MountOptions:[],AllowVolumeExpansion:nil,VolumeBindingMode:*Immediate,AllowedTopologies:[]TopologySelectorTerm{},} pvc-39cc81d4-1f87-481e-9283-d2dfdb9c1bcb &PersistentVolumeClaim{ObjectMeta:{persistence-rabbitmqcluster-server-0  default  39cc81d4-1f87-481e-9283-d2dfdb9c1bcb 41092 0 2021-12-13 15:46:15 +0000 UTC <nil> <nil> map[app.kubernetes.io/component:rabbitmq app.kubernetes.io/name:rabbitmqcluster app.kubernetes.io/part-of:rabbitmq] map[volume.beta.kubernetes.io/storage-provisioner:k8s.io/minikube-hostpath] [{rabbitmq.com/v1beta1 RabbitmqCluster rabbitmqcluster 5163c18b-b71e-4ca4-acc4-8aee45783a92 0xc000465d0b 0xc000465d0c}] [kubernetes.io/pvc-protection]  [{kube-controller-manager Update v1 2021-12-13 15:46:15 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:volume.beta.kubernetes.io/storage-provisioner":{}},"f:labels":{".":{},"f:app.kubernetes.io/component":{},"f:app.kubernetes.io/name":{},"f:app.kubernetes.io/part-of":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5163c18b-b71e-4ca4-acc4-8aee45783a92\"}":{}}},"f:spec":{"f:accessModes":{},"f:resources":{"f:requests":{".":{},"f:storage":{}}},"f:volumeMode":{}}}}]},Spec:PersistentVolumeClaimSpec{AccessModes:[ReadWriteOnce],Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{storage: {{10737418240 0} {<nil>} 10Gi BinarySI},},},VolumeName:,Selector:nil,StorageClassName:*standard,VolumeMode:*Filesystem,DataSource:nil,},Status:PersistentVolumeClaimStatus{Phase:Pending,AccessModes:[],Capacity:ResourceList{},Conditions:[]PersistentVolumeClaimCondition{},},} nil} to /tmp/hostpath-provisioner/default/persistence-rabbitmqcluster-server-0
I1213 15:46:15.876059       1 event.go:282] Event(v1.ObjectReference{Kind:"PersistentVolumeClaim", Namespace:"default", Name:"persistence-rabbitmqcluster-server-0", UID:"39cc81d4-1f87-481e-9283-d2dfdb9c1bcb", APIVersion:"v1", ResourceVersion:"41092", FieldPath:""}): type: 'Normal' reason: 'Provisioning' External provisioner is provisioning volume for claim "default/persistence-rabbitmqcluster-server-0"
I1213 15:46:15.878074       1 controller.go:1439] provision "default/persistence-rabbitmqcluster-server-0" class "standard": volume "pvc-39cc81d4-1f87-481e-9283-d2dfdb9c1bcb" provisioned
I1213 15:46:15.878433       1 controller.go:1456] provision "default/persistence-rabbitmqcluster-server-0" class "standard": succeeded
I1213 15:46:15.878737       1 volume_store.go:212] Trying to save persistentvolume "pvc-39cc81d4-1f87-481e-9283-d2dfdb9c1bcb"
I1213 15:46:15.955952       1 volume_store.go:219] persistentvolume "pvc-39cc81d4-1f87-481e-9283-d2dfdb9c1bcb" saved
I1213 15:46:15.956707       1 event.go:282] Event(v1.ObjectReference{Kind:"PersistentVolumeClaim", Namespace:"default", Name:"persistence-rabbitmqcluster-server-0", UID:"39cc81d4-1f87-481e-9283-d2dfdb9c1bcb", APIVersion:"v1", ResourceVersion:"41092", FieldPath:""}): type: 'Normal' reason: 'ProvisioningSucceeded' Successfully provisioned volume pvc-39cc81d4-1f87-481e-9283-d2dfdb9c1bcb
I1213 15:49:14.065772       1 controller.go:1332] provision "default/persistence-hello-world-server-0" class "standard": started
I1213 15:49:14.068795       1 storage_provisioner.go:61] Provisioning volume {&StorageClass{ObjectMeta:{standard    8bf73280-609b-4939-a151-ec863f9c159d 368 0 2021-12-10 14:03:30 +0000 UTC <nil> <nil> map[addonmanager.kubernetes.io/mode:EnsureExists] map[kubectl.kubernetes.io/last-applied-configuration:{"apiVersion":"storage.k8s.io/v1","kind":"StorageClass","metadata":{"annotations":{"storageclass.kubernetes.io/is-default-class":"true"},"labels":{"addonmanager.kubernetes.io/mode":"EnsureExists"},"name":"standard"},"provisioner":"k8s.io/minikube-hostpath"}
 storageclass.kubernetes.io/is-default-class:true] [] []  [{kubectl-client-side-apply Update storage.k8s.io/v1 2021-12-10 14:03:30 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:kubectl.kubernetes.io/last-applied-configuration":{},"f:storageclass.kubernetes.io/is-default-class":{}},"f:labels":{".":{},"f:addonmanager.kubernetes.io/mode":{}}},"f:provisioner":{},"f:reclaimPolicy":{},"f:volumeBindingMode":{}}}]},Provisioner:k8s.io/minikube-hostpath,Parameters:map[string]string{},ReclaimPolicy:*Delete,MountOptions:[],AllowVolumeExpansion:nil,VolumeBindingMode:*Immediate,AllowedTopologies:[]TopologySelectorTerm{},} pvc-b3dc4dfd-4739-49fa-94ee-b26e804d4d4f &PersistentVolumeClaim{ObjectMeta:{persistence-hello-world-server-0  default  b3dc4dfd-4739-49fa-94ee-b26e804d4d4f 41494 0 2021-12-13 15:49:14 +0000 UTC <nil> <nil> map[app.kubernetes.io/component:rabbitmq app.kubernetes.io/name:hello-world app.kubernetes.io/part-of:rabbitmq] map[volume.beta.kubernetes.io/storage-provisioner:k8s.io/minikube-hostpath] [{rabbitmq.com/v1beta1 RabbitmqCluster hello-world 113cdbaa-bc26-4945-a85a-c3926cbcb098 0xc000708b7b 0xc000708b7c}] [kubernetes.io/pvc-protection]  [{kube-controller-manager Update v1 2021-12-13 15:49:14 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:volume.beta.kubernetes.io/storage-provisioner":{}},"f:labels":{".":{},"f:app.kubernetes.io/component":{},"f:app.kubernetes.io/name":{},"f:app.kubernetes.io/part-of":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"113cdbaa-bc26-4945-a85a-c3926cbcb098\"}":{}}},"f:spec":{"f:accessModes":{},"f:resources":{"f:requests":{".":{},"f:storage":{}}},"f:volumeMode":{}}}}]},Spec:PersistentVolumeClaimSpec{AccessModes:[ReadWriteOnce],Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{storage: {{10737418240 0} {<nil>} 10Gi BinarySI},},},VolumeName:,Selector:nil,StorageClassName:*standard,VolumeMode:*Filesystem,DataSource:nil,},Status:PersistentVolumeClaimStatus{Phase:Pending,AccessModes:[],Capacity:ResourceList{},Conditions:[]PersistentVolumeClaimCondition{},},} nil} to /tmp/hostpath-provisioner/default/persistence-hello-world-server-0
I1213 15:49:14.069051       1 controller.go:1439] provision "default/persistence-hello-world-server-0" class "standard": volume "pvc-b3dc4dfd-4739-49fa-94ee-b26e804d4d4f" provisioned
I1213 15:49:14.069095       1 controller.go:1456] provision "default/persistence-hello-world-server-0" class "standard": succeeded
I1213 15:49:14.069107       1 volume_store.go:212] Trying to save persistentvolume "pvc-b3dc4dfd-4739-49fa-94ee-b26e804d4d4f"
I1213 15:49:14.069665       1 event.go:282] Event(v1.ObjectReference{Kind:"PersistentVolumeClaim", Namespace:"default", Name:"persistence-hello-world-server-0", UID:"b3dc4dfd-4739-49fa-94ee-b26e804d4d4f", APIVersion:"v1", ResourceVersion:"41494", FieldPath:""}): type: 'Normal' reason: 'Provisioning' External provisioner is provisioning volume for claim "default/persistence-hello-world-server-0"
I1213 15:49:14.144011       1 volume_store.go:219] persistentvolume "pvc-b3dc4dfd-4739-49fa-94ee-b26e804d4d4f" saved
I1213 15:49:14.144159       1 event.go:282] Event(v1.ObjectReference{Kind:"PersistentVolumeClaim", Namespace:"default", Name:"persistence-hello-world-server-0", UID:"b3dc4dfd-4739-49fa-94ee-b26e804d4d4f", APIVersion:"v1", ResourceVersion:"41494", FieldPath:""}): type: 'Normal' reason: 'ProvisioningSucceeded' Successfully provisioned volume pvc-b3dc4dfd-4739-49fa-94ee-b26e804d4d4f
I1213 15:49:20.917305       1 controller.go:1472] delete "pvc-756944d4-effd-4913-9b20-11225c7801f1": started
I1213 15:49:20.917351       1 storage_provisioner.go:98] Deleting volume &PersistentVolume{ObjectMeta:{pvc-756944d4-effd-4913-9b20-11225c7801f1    35573de4-21ed-4aba-bcfb-02eac3dd2fba 41540 0 2021-12-13 15:40:43 +0000 UTC <nil> <nil> map[] map[hostPathProvisionerIdentity:d73bd919-49a2-4fcc-b0e4-a8529c45f82f pv.kubernetes.io/provisioned-by:k8s.io/minikube-hostpath] [] [kubernetes.io/pv-protection]  [{kube-controller-manager Update v1 2021-12-13 15:40:43 +0000 UTC FieldsV1 {"f:status":{"f:phase":{}}}} {storage-provisioner Update v1 2021-12-13 15:40:43 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:hostPathProvisionerIdentity":{},"f:pv.kubernetes.io/provisioned-by":{}}},"f:spec":{"f:accessModes":{},"f:capacity":{".":{},"f:storage":{}},"f:claimRef":{},"f:hostPath":{".":{},"f:path":{},"f:type":{}},"f:persistentVolumeReclaimPolicy":{},"f:storageClassName":{},"f:volumeMode":{}}}}]},Spec:PersistentVolumeSpec{Capacity:ResourceList{storage: {{10737418240 0} {<nil>} 10Gi BinarySI},},PersistentVolumeSource:PersistentVolumeSource{GCEPersistentDisk:nil,AWSElasticBlockStore:nil,HostPath:&HostPathVolumeSource{Path:/tmp/hostpath-provisioner/default/persistence-resource-limits-server-0,Type:*,},Glusterfs:nil,NFS:nil,RBD:nil,ISCSI:nil,Cinder:nil,CephFS:nil,FC:nil,Flocker:nil,FlexVolume:nil,AzureFile:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Local:nil,StorageOS:nil,CSI:nil,},AccessModes:[ReadWriteOnce],ClaimRef:&ObjectReference{Kind:PersistentVolumeClaim,Namespace:default,Name:persistence-resource-limits-server-0,UID:756944d4-effd-4913-9b20-11225c7801f1,APIVersion:v1,ResourceVersion:40414,FieldPath:,},PersistentVolumeReclaimPolicy:Delete,StorageClassName:standard,MountOptions:[],VolumeMode:*Filesystem,NodeAffinity:nil,},Status:PersistentVolumeStatus{Phase:Released,Message:,Reason:,},}
I1213 15:49:20.928903       1 controller.go:1487] delete "pvc-756944d4-effd-4913-9b20-11225c7801f1": volume deleted
I1213 15:49:21.008290       1 controller.go:1537] delete "pvc-756944d4-effd-4913-9b20-11225c7801f1": persistentvolume deleted
I1213 15:49:21.008356       1 controller.go:1542] delete "pvc-756944d4-effd-4913-9b20-11225c7801f1": succeeded
I1213 15:49:21.016323       1 controller.go:1332] provision "default/persistence-resource-limits-server-0" class "standard": started
I1213 15:49:21.016792       1 storage_provisioner.go:61] Provisioning volume {&StorageClass{ObjectMeta:{standard    8bf73280-609b-4939-a151-ec863f9c159d 368 0 2021-12-10 14:03:30 +0000 UTC <nil> <nil> map[addonmanager.kubernetes.io/mode:EnsureExists] map[kubectl.kubernetes.io/last-applied-configuration:{"apiVersion":"storage.k8s.io/v1","kind":"StorageClass","metadata":{"annotations":{"storageclass.kubernetes.io/is-default-class":"true"},"labels":{"addonmanager.kubernetes.io/mode":"EnsureExists"},"name":"standard"},"provisioner":"k8s.io/minikube-hostpath"}
 storageclass.kubernetes.io/is-default-class:true] [] []  [{kubectl-client-side-apply Update storage.k8s.io/v1 2021-12-10 14:03:30 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:kubectl.kubernetes.io/last-applied-configuration":{},"f:storageclass.kubernetes.io/is-default-class":{}},"f:labels":{".":{},"f:addonmanager.kubernetes.io/mode":{}}},"f:provisioner":{},"f:reclaimPolicy":{},"f:volumeBindingMode":{}}}]},Provisioner:k8s.io/minikube-hostpath,Parameters:map[string]string{},ReclaimPolicy:*Delete,MountOptions:[],AllowVolumeExpansion:nil,VolumeBindingMode:*Immediate,AllowedTopologies:[]TopologySelectorTerm{},} pvc-9b49e0e5-be5b-47b2-8968-1a86087ad26c &PersistentVolumeClaim{ObjectMeta:{persistence-resource-limits-server-0  default  9b49e0e5-be5b-47b2-8968-1a86087ad26c 41542 0 2021-12-13 15:49:20 +0000 UTC <nil> <nil> map[app.kubernetes.io/component:rabbitmq app.kubernetes.io/name:resource-limits app.kubernetes.io/part-of:rabbitmq] map[volume.beta.kubernetes.io/storage-provisioner:k8s.io/minikube-hostpath] [{rabbitmq.com/v1beta1 RabbitmqCluster resource-limits 2d2e1625-d0d3-4f15-92b3-c9cc3e3a0f8f 0xc00049c69b 0xc00049c69c}] [kubernetes.io/pvc-protection]  [{kube-controller-manager Update v1 2021-12-13 15:49:20 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:volume.beta.kubernetes.io/storage-provisioner":{}},"f:labels":{".":{},"f:app.kubernetes.io/component":{},"f:app.kubernetes.io/name":{},"f:app.kubernetes.io/part-of":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2d2e1625-d0d3-4f15-92b3-c9cc3e3a0f8f\"}":{}}},"f:spec":{"f:accessModes":{},"f:resources":{"f:requests":{".":{},"f:storage":{}}},"f:volumeMode":{}}}}]},Spec:PersistentVolumeClaimSpec{AccessModes:[ReadWriteOnce],Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{storage: {{10737418240 0} {<nil>} 10Gi BinarySI},},},VolumeName:,Selector:nil,StorageClassName:*standard,VolumeMode:*Filesystem,DataSource:nil,},Status:PersistentVolumeClaimStatus{Phase:Pending,AccessModes:[],Capacity:ResourceList{},Conditions:[]PersistentVolumeClaimCondition{},},} nil} to /tmp/hostpath-provisioner/default/persistence-resource-limits-server-0
I1213 15:49:21.017759       1 controller.go:1439] provision "default/persistence-resource-limits-server-0" class "standard": volume "pvc-9b49e0e5-be5b-47b2-8968-1a86087ad26c" provisioned
I1213 15:49:21.017973       1 controller.go:1456] provision "default/persistence-resource-limits-server-0" class "standard": succeeded
I1213 15:49:21.018400       1 volume_store.go:212] Trying to save persistentvolume "pvc-9b49e0e5-be5b-47b2-8968-1a86087ad26c"
I1213 15:49:21.017985       1 event.go:282] Event(v1.ObjectReference{Kind:"PersistentVolumeClaim", Namespace:"default", Name:"persistence-resource-limits-server-0", UID:"9b49e0e5-be5b-47b2-8968-1a86087ad26c", APIVersion:"v1", ResourceVersion:"41542", FieldPath:""}): type: 'Normal' reason: 'Provisioning' External provisioner is provisioning volume for claim "default/persistence-resource-limits-server-0"
I1213 15:49:21.029476       1 volume_store.go:219] persistentvolume "pvc-9b49e0e5-be5b-47b2-8968-1a86087ad26c" saved
I1213 15:49:21.029850       1 event.go:282] Event(v1.ObjectReference{Kind:"PersistentVolumeClaim", Namespace:"default", Name:"persistence-resource-limits-server-0", UID:"9b49e0e5-be5b-47b2-8968-1a86087ad26c", APIVersion:"v1", ResourceVersion:"41542", FieldPath:""}): type: 'Normal' reason: 'ProvisioningSucceeded' Successfully provisioned volume pvc-9b49e0e5-be5b-47b2-8968-1a86087ad26c
I1213 15:51:24.072732       1 controller.go:1472] delete "pvc-b3dc4dfd-4739-49fa-94ee-b26e804d4d4f": started
I1213 15:51:24.072785       1 storage_provisioner.go:98] Deleting volume &PersistentVolume{ObjectMeta:{pvc-b3dc4dfd-4739-49fa-94ee-b26e804d4d4f    5a56dab5-58ba-433d-aee9-1683dd377fcb 41823 0 2021-12-13 15:49:14 +0000 UTC <nil> <nil> map[] map[hostPathProvisionerIdentity:d73bd919-49a2-4fcc-b0e4-a8529c45f82f pv.kubernetes.io/provisioned-by:k8s.io/minikube-hostpath] [] [kubernetes.io/pv-protection]  [{kube-controller-manager Update v1 2021-12-13 15:49:14 +0000 UTC FieldsV1 {"f:status":{"f:phase":{}}}} {storage-provisioner Update v1 2021-12-13 15:49:14 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:hostPathProvisionerIdentity":{},"f:pv.kubernetes.io/provisioned-by":{}}},"f:spec":{"f:accessModes":{},"f:capacity":{".":{},"f:storage":{}},"f:claimRef":{},"f:hostPath":{".":{},"f:path":{},"f:type":{}},"f:persistentVolumeReclaimPolicy":{},"f:storageClassName":{},"f:volumeMode":{}}}}]},Spec:PersistentVolumeSpec{Capacity:ResourceList{storage: {{10737418240 0} {<nil>} 10Gi BinarySI},},PersistentVolumeSource:PersistentVolumeSource{GCEPersistentDisk:nil,AWSElasticBlockStore:nil,HostPath:&HostPathVolumeSource{Path:/tmp/hostpath-provisioner/default/persistence-hello-world-server-0,Type:*,},Glusterfs:nil,NFS:nil,RBD:nil,ISCSI:nil,Cinder:nil,CephFS:nil,FC:nil,Flocker:nil,FlexVolume:nil,AzureFile:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Local:nil,StorageOS:nil,CSI:nil,},AccessModes:[ReadWriteOnce],ClaimRef:&ObjectReference{Kind:PersistentVolumeClaim,Namespace:default,Name:persistence-hello-world-server-0,UID:b3dc4dfd-4739-49fa-94ee-b26e804d4d4f,APIVersion:v1,ResourceVersion:41494,FieldPath:,},PersistentVolumeReclaimPolicy:Delete,StorageClassName:standard,MountOptions:[],VolumeMode:*Filesystem,NodeAffinity:nil,},Status:PersistentVolumeStatus{Phase:Released,Message:,Reason:,},}
I1213 15:51:24.077915       1 controller.go:1487] delete "pvc-b3dc4dfd-4739-49fa-94ee-b26e804d4d4f": volume deleted
I1213 15:51:24.086955       1 controller.go:1537] delete "pvc-b3dc4dfd-4739-49fa-94ee-b26e804d4d4f": persistentvolume deleted
I1213 15:51:24.087000       1 controller.go:1542] delete "pvc-b3dc4dfd-4739-49fa-94ee-b26e804d4d4f": succeeded
I1213 15:51:38.168536       1 controller.go:1472] delete "pvc-9b49e0e5-be5b-47b2-8968-1a86087ad26c": started
I1213 15:51:38.168599       1 storage_provisioner.go:98] Deleting volume &PersistentVolume{ObjectMeta:{pvc-9b49e0e5-be5b-47b2-8968-1a86087ad26c    71223e44-cdce-477e-9e97-8cc70d482ad7 41876 0 2021-12-13 15:49:21 +0000 UTC <nil> <nil> map[] map[hostPathProvisionerIdentity:d73bd919-49a2-4fcc-b0e4-a8529c45f82f pv.kubernetes.io/provisioned-by:k8s.io/minikube-hostpath] [] [kubernetes.io/pv-protection]  [{kube-controller-manager Update v1 2021-12-13 15:49:21 +0000 UTC FieldsV1 {"f:status":{"f:phase":{}}}} {storage-provisioner Update v1 2021-12-13 15:49:21 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:hostPathProvisionerIdentity":{},"f:pv.kubernetes.io/provisioned-by":{}}},"f:spec":{"f:accessModes":{},"f:capacity":{".":{},"f:storage":{}},"f:claimRef":{},"f:hostPath":{".":{},"f:path":{},"f:type":{}},"f:persistentVolumeReclaimPolicy":{},"f:storageClassName":{},"f:volumeMode":{}}}}]},Spec:PersistentVolumeSpec{Capacity:ResourceList{storage: {{10737418240 0} {<nil>} 10Gi BinarySI},},PersistentVolumeSource:PersistentVolumeSource{GCEPersistentDisk:nil,AWSElasticBlockStore:nil,HostPath:&HostPathVolumeSource{Path:/tmp/hostpath-provisioner/default/persistence-resource-limits-server-0,Type:*,},Glusterfs:nil,NFS:nil,RBD:nil,ISCSI:nil,Cinder:nil,CephFS:nil,FC:nil,Flocker:nil,FlexVolume:nil,AzureFile:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Local:nil,StorageOS:nil,CSI:nil,},AccessModes:[ReadWriteOnce],ClaimRef:&ObjectReference{Kind:PersistentVolumeClaim,Namespace:default,Name:persistence-resource-limits-server-0,UID:9b49e0e5-be5b-47b2-8968-1a86087ad26c,APIVersion:v1,ResourceVersion:41542,FieldPath:,},PersistentVolumeReclaimPolicy:Delete,StorageClassName:standard,MountOptions:[],VolumeMode:*Filesystem,NodeAffinity:nil,},Status:PersistentVolumeStatus{Phase:Released,Message:,Reason:,},}
I1213 15:51:38.169101       1 controller.go:1487] delete "pvc-9b49e0e5-be5b-47b2-8968-1a86087ad26c": volume deleted
I1213 15:51:38.176127       1 controller.go:1537] delete "pvc-9b49e0e5-be5b-47b2-8968-1a86087ad26c": persistentvolume deleted
I1213 15:51:38.176175       1 controller.go:1542] delete "pvc-9b49e0e5-be5b-47b2-8968-1a86087ad26c": succeeded
I1213 16:02:30.960972       1 controller.go:1472] delete "pvc-39cc81d4-1f87-481e-9283-d2dfdb9c1bcb": started
I1213 16:02:30.961011       1 storage_provisioner.go:98] Deleting volume &PersistentVolume{ObjectMeta:{pvc-39cc81d4-1f87-481e-9283-d2dfdb9c1bcb    b7dc9b01-ca9c-4c06-8941-28f5a7ba747b 43025 0 2021-12-13 15:46:15 +0000 UTC <nil> <nil> map[] map[hostPathProvisionerIdentity:d73bd919-49a2-4fcc-b0e4-a8529c45f82f pv.kubernetes.io/provisioned-by:k8s.io/minikube-hostpath] [] [kubernetes.io/pv-protection]  [{kube-controller-manager Update v1 2021-12-13 15:46:15 +0000 UTC FieldsV1 {"f:status":{"f:phase":{}}}} {storage-provisioner Update v1 2021-12-13 15:46:15 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:hostPathProvisionerIdentity":{},"f:pv.kubernetes.io/provisioned-by":{}}},"f:spec":{"f:accessModes":{},"f:capacity":{".":{},"f:storage":{}},"f:claimRef":{},"f:hostPath":{".":{},"f:path":{},"f:type":{}},"f:persistentVolumeReclaimPolicy":{},"f:storageClassName":{},"f:volumeMode":{}}}}]},Spec:PersistentVolumeSpec{Capacity:ResourceList{storage: {{10737418240 0} {<nil>} 10Gi BinarySI},},PersistentVolumeSource:PersistentVolumeSource{GCEPersistentDisk:nil,AWSElasticBlockStore:nil,HostPath:&HostPathVolumeSource{Path:/tmp/hostpath-provisioner/default/persistence-rabbitmqcluster-server-0,Type:*,},Glusterfs:nil,NFS:nil,RBD:nil,ISCSI:nil,Cinder:nil,CephFS:nil,FC:nil,Flocker:nil,FlexVolume:nil,AzureFile:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Local:nil,StorageOS:nil,CSI:nil,},AccessModes:[ReadWriteOnce],ClaimRef:&ObjectReference{Kind:PersistentVolumeClaim,Namespace:default,Name:persistence-rabbitmqcluster-server-0,UID:39cc81d4-1f87-481e-9283-d2dfdb9c1bcb,APIVersion:v1,ResourceVersion:41092,FieldPath:,},PersistentVolumeReclaimPolicy:Delete,StorageClassName:standard,MountOptions:[],VolumeMode:*Filesystem,NodeAffinity:nil,},Status:PersistentVolumeStatus{Phase:Released,Message:,Reason:,},}
I1213 16:02:30.972116       1 controller.go:1487] delete "pvc-39cc81d4-1f87-481e-9283-d2dfdb9c1bcb": volume deleted
I1213 16:02:30.977929       1 controller.go:1537] delete "pvc-39cc81d4-1f87-481e-9283-d2dfdb9c1bcb": persistentvolume deleted
I1213 16:02:30.978301       1 controller.go:1542] delete "pvc-39cc81d4-1f87-481e-9283-d2dfdb9c1bcb": succeeded
I1213 16:02:41.848666       1 controller.go:1332] provision "default/persistence-hello-world-rabbitmq-server-0" class "standard": started
I1213 16:02:41.848761       1 storage_provisioner.go:61] Provisioning volume {&StorageClass{ObjectMeta:{standard    8bf73280-609b-4939-a151-ec863f9c159d 368 0 2021-12-10 14:03:30 +0000 UTC <nil> <nil> map[addonmanager.kubernetes.io/mode:EnsureExists] map[kubectl.kubernetes.io/last-applied-configuration:{"apiVersion":"storage.k8s.io/v1","kind":"StorageClass","metadata":{"annotations":{"storageclass.kubernetes.io/is-default-class":"true"},"labels":{"addonmanager.kubernetes.io/mode":"EnsureExists"},"name":"standard"},"provisioner":"k8s.io/minikube-hostpath"}
 storageclass.kubernetes.io/is-default-class:true] [] []  [{kubectl-client-side-apply Update storage.k8s.io/v1 2021-12-10 14:03:30 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:kubectl.kubernetes.io/last-applied-configuration":{},"f:storageclass.kubernetes.io/is-default-class":{}},"f:labels":{".":{},"f:addonmanager.kubernetes.io/mode":{}}},"f:provisioner":{},"f:reclaimPolicy":{},"f:volumeBindingMode":{}}}]},Provisioner:k8s.io/minikube-hostpath,Parameters:map[string]string{},ReclaimPolicy:*Delete,MountOptions:[],AllowVolumeExpansion:nil,VolumeBindingMode:*Immediate,AllowedTopologies:[]TopologySelectorTerm{},} pvc-154a1d67-8da9-4f57-a38e-6afaa18ef432 &PersistentVolumeClaim{ObjectMeta:{persistence-hello-world-rabbitmq-server-0  default  154a1d67-8da9-4f57-a38e-6afaa18ef432 43083 0 2021-12-13 16:02:41 +0000 UTC <nil> <nil> map[app.kubernetes.io/component:rabbitmq app.kubernetes.io/name:hello-world-rabbitmq app.kubernetes.io/part-of:rabbitmq] map[volume.beta.kubernetes.io/storage-provisioner:k8s.io/minikube-hostpath] [{rabbitmq.com/v1beta1 RabbitmqCluster hello-world-rabbitmq 078e6dcb-e5c8-4da4-8e33-2f5cd10d50ba 0xc0005f3d1b 0xc0005f3d1c}] [kubernetes.io/pvc-protection]  [{kube-controller-manager Update v1 2021-12-13 16:02:41 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:volume.beta.kubernetes.io/storage-provisioner":{}},"f:labels":{".":{},"f:app.kubernetes.io/component":{},"f:app.kubernetes.io/name":{},"f:app.kubernetes.io/part-of":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"078e6dcb-e5c8-4da4-8e33-2f5cd10d50ba\"}":{}}},"f:spec":{"f:accessModes":{},"f:resources":{"f:requests":{".":{},"f:storage":{}}},"f:volumeMode":{}}}}]},Spec:PersistentVolumeClaimSpec{AccessModes:[ReadWriteOnce],Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{storage: {{10737418240 0} {<nil>} 10Gi BinarySI},},},VolumeName:,Selector:nil,StorageClassName:*standard,VolumeMode:*Filesystem,DataSource:nil,},Status:PersistentVolumeClaimStatus{Phase:Pending,AccessModes:[],Capacity:ResourceList{},Conditions:[]PersistentVolumeClaimCondition{},},} nil} to /tmp/hostpath-provisioner/default/persistence-hello-world-rabbitmq-server-0
I1213 16:02:41.848952       1 controller.go:1439] provision "default/persistence-hello-world-rabbitmq-server-0" class "standard": volume "pvc-154a1d67-8da9-4f57-a38e-6afaa18ef432" provisioned
I1213 16:02:41.848971       1 controller.go:1456] provision "default/persistence-hello-world-rabbitmq-server-0" class "standard": succeeded
I1213 16:02:41.848987       1 volume_store.go:212] Trying to save persistentvolume "pvc-154a1d67-8da9-4f57-a38e-6afaa18ef432"
I1213 16:02:41.853999       1 event.go:282] Event(v1.ObjectReference{Kind:"PersistentVolumeClaim", Namespace:"default", Name:"persistence-hello-world-rabbitmq-server-0", UID:"154a1d67-8da9-4f57-a38e-6afaa18ef432", APIVersion:"v1", ResourceVersion:"43083", FieldPath:""}): type: 'Normal' reason: 'Provisioning' External provisioner is provisioning volume for claim "default/persistence-hello-world-rabbitmq-server-0"
I1213 16:02:41.928781       1 volume_store.go:219] persistentvolume "pvc-154a1d67-8da9-4f57-a38e-6afaa18ef432" saved
I1213 16:02:41.928946       1 event.go:282] Event(v1.ObjectReference{Kind:"PersistentVolumeClaim", Namespace:"default", Name:"persistence-hello-world-rabbitmq-server-0", UID:"154a1d67-8da9-4f57-a38e-6afaa18ef432", APIVersion:"v1", ResourceVersion:"43083", FieldPath:""}): type: 'Normal' reason: 'ProvisioningSucceeded' Successfully provisioned volume pvc-154a1d67-8da9-4f57-a38e-6afaa18ef432

* 
* ==> storage-provisioner [f9e0f7c806e1] <==
* I1213 04:17:54.012976       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
F1213 04:18:24.012012       1 main.go:39] error getting server version: Get "https://10.96.0.1:443/version?timeout=32s": dial tcp 10.96.0.1:443: i/o timeout

